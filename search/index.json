[{"content":"在项目管理中，准确估算时间和资源是确保项目成功的关键因素。关键路径法（Critical Path Method, CPM）和浮动时间计算是两个强大的工具，能帮助项目经理有效规划和控制项目进度。本文将深入探讨这两个概念，解释它们的计算方法及其在项目管理中的重要性。\n# 关键路径法：ES、EF、LS、LF的计算 关键路径法是一种网络计划技术，用于确定项目中最长的活动序列，即关键路径。它涉及四个关键时间点的计算：最早开始时间（ES）、最早完成时间（EF）、最晚开始时间（LS）和最晚完成时间（LF）。\n# 前推法（计算ES和EF） ES (最早开始时间):\n对于起始活动：ES = 项目开始时间（通常为0） 对于其他活动：ES = 所有紧前活动中最大的EF值 EF (最早完成时间): EF = ES + 活动持续时间\n# 后推法（计算LS和LF） LF (最晚完成时间):\n对于终止活动：LF = 项目的计划完成时间 对于其他活动：LF = 所有紧后活动中最小的LS值 LS (最晚开始时间): LS = LF - 活动持续时间\n# 浮动时间的计算及其重要性 浮动时间表示一个活动可以推迟的时间而不影响整个项目的完成时间。理解和管理浮动时间对于优化项目进度和资源分配至关重要。\n# 总浮动时间 (Total Float, TF) 总浮动时间是一个活动可以推迟的最大时间，而不影响整个项目的完成日期。\n计算方法： TF = LS - ES 或 TF = LF - EF\n# 自由浮动时间 (Free Float, FF) 自由浮动时间是一个活动可以推迟的时间，而不影响任何后续活动的最早开始时间。\n计算方法： FF = 紧后活动的最早开始时间 - 当前活动的最早完成时间 或 FF = MIN(紧后活动的ES) - EF\n# 应用这些概念的步骤 从项目开始到结束，使用前推法计算每个活动的ES和EF。 从项目结束到开始，使用后推法计算每个活动的LF和LS。 计算总浮动时间和自由浮动时间。 识别关键路径：总浮动时间为0的活动序列即为关键路径。 # 这些概念的重要性 识别关键活动和非关键活动 优化资源分配 评估项目风险 制定应急计划 通过掌握关键路径法和浮动时间计算，项目经理可以：\n准确预测项目完成时间 识别需要特别关注的关键活动 合理分配资源，优化项目进度 及时发现潜在的进度问题并采取相应措施 虽然现代项目管理软件可以自动执行这些计算，但深入理解这些概念的原理对于做出明智的项目决策至关重要。通过熟练运用这些工具，项目经理可以更好地控制项目进度，提高项目成功的可能性。\n","date":"2024-07-30T14:16:16+08:00","permalink":"https://blog.happyhacker.fun/p/%E6%8F%A1%E9%A1%B9%E7%9B%AE%E7%AE%A1%E7%90%86%E7%9A%84%E5%85%B3%E9%94%AE%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3%E5%85%B3%E9%94%AE%E8%B7%AF%E5%BE%84%E6%B3%95%E5%92%8C%E6%B5%AE%E5%8A%A8%E6%97%B6%E9%97%B4%E8%AE%A1%E7%AE%97/","title":"握项目管理的关键：深入理解关键路径法和浮动时间计算"},{"content":"正态分布（也称为高斯分布）是统计学中非常重要的一种概率分布，其特征是呈钟形曲线。正态分布的形状由其均值（μ）和标准差（σ）决定。以下是正态分布和标准差之间的关系：\n# 1. 定义： 均值（μ）：正态分布的中心，表示数据的平均值。 标准差（σ）：表示数据点与均值之间的离散程度或波动性。标准差越大，数据的分布越宽，曲线越平坦；标准差越小，数据分布越窄，曲线越陡。 # 2. 正态分布的概率密度函数： 正态分布的概率密度函数（PDF）可以表示为： \\[ f(x) = \\frac{1}{\\sigma \\sqrt{2\\pi}} e^{-\\frac{(x - \\mu)^2}{2\\sigma^2}} \\] 其中：\n\\( f(x) \\) 是在某个特定值 \\( x \\) 处的概率密度。 \\( e \\) 是自然对数的底数。 \\( \\pi \\) 是圆周率。 # 3. 标准差与曲线形状的关系： 68-95-99.7规则：在正态分布中，大约68%的数据位于均值±1个标准差（μ±σ）之间，约95%的数据位于均值±2个标准差（μ±2σ）之间，约99.7%的数据位于均值±3个标准差（μ±3σ）之间。这一性质使得标准差在理解数据分布的范围和集中程度上极为重要。 所以当知道一个概率之后，想推算如果想达到某一个概率需要把期望移动到哪个区间，就是基于这个原理。\n# 4. 应用： 标准差在许多统计分析中扮演着重要角色，尤其是在进行假设检验、控制图和其他数据分析方法时。它帮助我们理解数据的可靠性和变异性。\n# 总结： 正态分布的形状和特性直接与标准差相关。标准差不仅影响分布的宽度和高度，还在数据分析和解释中起着关键作用。了解两者之间的关系，有助于我们更好地分析和理解数据。\n","date":"2024-07-29T18:58:40+08:00","permalink":"https://blog.happyhacker.fun/p/%E6%AD%A3%E6%80%81%E5%88%86%E5%B8%83%E5%92%8C%E6%A0%87%E5%87%86%E5%B7%AE/","title":"正态分布和标准差"},{"content":"在Go语言中，类型断言(type assertion)和类型选择(type switch)是用于检查和操作空接口类型interface{}或any（自Go 1.18开始引入的别名）的两个重要特性。\n你的代码片段中使用了类型选择(type switch)，下面我们深入解释一下它的工作原理。\n# 类型选择的工作原理 类型选择用于检测并处理接口值的动态类型。类型选择的语法如下：\n1 2 3 4 5 6 7 8 9 switch v := x.(type) { case T1: // v 是 T1 类型并且持有 x 的值 case T2: // v 是 T2 类型并且持有 x 的值 // ... default: // x 不符合任何已列出的类型 } # 代码解析 你的代码如下：\n1 2 3 4 5 6 7 8 var structure any switch structure := structure.(type) { case map[string]any: fmt.Printf(\u0026#34;%v\u0026#34;, structure) case []any: fmt.Printf(\u0026#34;%v\u0026#34;, structure) } 这里的类型选择的具体步骤如下：\n类型断言： structure.(type)用于检测structure的动态类型。在类型选择的上下文中，structure.(type)是一个特殊的语法，用于获取接口的具体类型。\n类型选择的语法： 类型选择用于根据接口的动态类型来执行不同的代码。具体来说，structure := structure.(type)会对structure进行类型断言，并将其结果赋值给新的变量structure（在每个case块中，structure的类型和值依据具体的类型断言而变化）。\n类型匹配：\n如果structure的动态类型是map[string]any，则执行第一个case块，且此时structure的类型就是map[string]any。 如果structure的动态类型是[]any，则执行第二个case块，且此时structure的类型就是[]any。 # 为什么既是类型，又是值？ 在类型选择的每个case块中，structure被重新赋值为具体类型，并且持有与其动态类型匹配的值。因此，在每个case块中：\n变量structure不仅仅是类型，还代表了实际的值。 例如，在case map[string]any:这个块中，structure的类型是map[string]any，且其值是原始structure变量中存储的map[string]any类型的值。 # 总结 类型选择的语法和行为可能看起来有些复杂，但实际上它非常直观地将类型断言和分支控制结合在一起：\nstructure.(type)用于检查接口的动态类型。 在每个case块中，structure不仅持有确定的类型，还持有接口的实际值。 这使得你可以针对不同的动态类型执行不同的处理逻辑，而无需显式地进行多次类型断言。 这样写法简洁且易读，避免了繁冗的类型断言和错误处理逻辑。\n","date":"2024-07-25T14:32:18+08:00","permalink":"https://blog.happyhacker.fun/p/go%E8%AF%AD%E8%A8%80%E7%9A%84%E7%B1%BB%E5%9E%8B%E9%80%89%E6%8B%A9/","title":"Go语言的类型选择"},{"content":" # Why? The first programming language I\u0026rsquo;ve learnt is PHP and I love it today though I don\u0026rsquo;t write it so frequently.\nThe most charming part of PHP is the design of array. It combines the concept of dict list in Python, someone would say it\u0026rsquo;s not so clear but a man who has writen PHP for a long time would say that\u0026rsquo;s a fucking good design.\nRecently I\u0026rsquo;ve fallen in love with Golang because of its simplicity and readability. But there would be some awful times when it comes to retriet a value from a map. This project is designed for this.\n# Usage 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 package main import ( \u0026#34;encoding/json\u0026#34; \u0026#34;fmt\u0026#34; array \u0026#34;github.com/lovelock/garray\u0026#34; ) func main() { data := `{ \u0026#34;name\u0026#34;: \u0026#34;John Doe\u0026#34;, \u0026#34;age\u0026#34;: 30, \u0026#34;email\u0026#34;: \u0026#34;john.doe@example.com\u0026#34;, \u0026#34;isActive\u0026#34;: true, \u0026#34;address\u0026#34;: { \u0026#34;street\u0026#34;: \u0026#34;123 Main St\u0026#34;, \u0026#34;city\u0026#34;: \u0026#34;Anytown\u0026#34;, \u0026#34;state\u0026#34;: \u0026#34;CA\u0026#34;, \u0026#34;postalCode\u0026#34;: \u0026#34;12345\u0026#34; }, \u0026#34;phoneNumbers\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;home\u0026#34;, \u0026#34;number\u0026#34;: \u0026#34;555-555-5555\u0026#34; }, { \u0026#34;type\u0026#34;: \u0026#34;work\u0026#34;, \u0026#34;number\u0026#34;: \u0026#34;555-555-5556\u0026#34; } ], \u0026#34;projects\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;Project Alpha\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;completed\u0026#34;, \u0026#34;tasks\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;Task 1\u0026#34;, \u0026#34;dueDate\u0026#34;: \u0026#34;2023-10-01\u0026#34;, \u0026#34;completed\u0026#34;: true }, { \u0026#34;name\u0026#34;: \u0026#34;Task 2\u0026#34;, \u0026#34;dueDate\u0026#34;: \u0026#34;2023-10-15\u0026#34;, \u0026#34;completed\u0026#34;: false } ] }, { \u0026#34;name\u0026#34;: \u0026#34;Project Beta\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;in progress\u0026#34;, \u0026#34;tasks\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;Task 3\u0026#34;, \u0026#34;dueDate\u0026#34;: \u0026#34;2023-11-01\u0026#34;, \u0026#34;completed\u0026#34;: false }, { \u0026#34;name\u0026#34;: \u0026#34;Task 4\u0026#34;, \u0026#34;dueDate\u0026#34;: \u0026#34;2023-12-01\u0026#34;, \u0026#34;completed\u0026#34;: false } ] } ], \u0026#34;preferences\u0026#34;: { \u0026#34;contactMethod\u0026#34;: \u0026#34;email\u0026#34;, \u0026#34;newsletterSubscribed\u0026#34;: true, \u0026#34;languages\u0026#34;: [\u0026#34;English\u0026#34;, \u0026#34;Spanish\u0026#34;, \u0026#34;German\u0026#34;] } }` var jsonMap map[string]any json.Unmarshal([]byte(data), \u0026amp;jsonMap) completed, err := array.Get(jsonMap, \u0026#34;projects\u0026#34;, \u0026#34;1\u0026#34;, \u0026#34;tasks\u0026#34;, \u0026#34;1\u0026#34;, \u0026#34;completed\u0026#34;) if err != nil { fmt.Println(\u0026#34;statusOfTheSecondTaskOfTheFirstProject: \u0026#34;, completed) } } # What problems does it solve Imagine you\u0026rsquo;ve got a complex structure like this\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 { \u0026#34;name\u0026#34;: \u0026#34;John Doe\u0026#34;, \u0026#34;age\u0026#34;: 30, \u0026#34;email\u0026#34;: \u0026#34;john.doe@example.com\u0026#34;, \u0026#34;isActive\u0026#34;: true, \u0026#34;address\u0026#34;: { \u0026#34;street\u0026#34;: \u0026#34;123 Main St\u0026#34;, \u0026#34;city\u0026#34;: \u0026#34;Anytown\u0026#34;, \u0026#34;state\u0026#34;: \u0026#34;CA\u0026#34;, \u0026#34;postalCode\u0026#34;: \u0026#34;12345\u0026#34; }, \u0026#34;phoneNumbers\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;home\u0026#34;, \u0026#34;number\u0026#34;: \u0026#34;555-555-5555\u0026#34; }, { \u0026#34;type\u0026#34;: \u0026#34;work\u0026#34;, \u0026#34;number\u0026#34;: \u0026#34;555-555-5556\u0026#34; } ], \u0026#34;projects\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;Project Alpha\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;completed\u0026#34;, \u0026#34;tasks\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;Task 1\u0026#34;, \u0026#34;dueDate\u0026#34;: \u0026#34;2023-10-01\u0026#34;, \u0026#34;completed\u0026#34;: true }, { \u0026#34;name\u0026#34;: \u0026#34;Task 2\u0026#34;, \u0026#34;dueDate\u0026#34;: \u0026#34;2023-10-15\u0026#34;, \u0026#34;completed\u0026#34;: false } ] }, { \u0026#34;name\u0026#34;: \u0026#34;Project Beta\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;in progress\u0026#34;, \u0026#34;tasks\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;Task 3\u0026#34;, \u0026#34;dueDate\u0026#34;: \u0026#34;2023-11-01\u0026#34;, \u0026#34;completed\u0026#34;: false }, { \u0026#34;name\u0026#34;: \u0026#34;Task 4\u0026#34;, \u0026#34;dueDate\u0026#34;: \u0026#34;2023-12-01\u0026#34;, \u0026#34;completed\u0026#34;: false } ] } ], \u0026#34;preferences\u0026#34;: { \u0026#34;contactMethod\u0026#34;: \u0026#34;email\u0026#34;, \u0026#34;newsletterSubscribed\u0026#34;: true, \u0026#34;languages\u0026#34;: [\u0026#34;English\u0026#34;, \u0026#34;Spanish\u0026#34;, \u0026#34;German\u0026#34;] } } How can you check if the first projects second task is completed?\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 package main import ( \u0026#34;encoding/json\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; ) // 定义JSON结构体 type Address struct { Street string `json:\u0026#34;street\u0026#34;` City string `json:\u0026#34;city\u0026#34;` State string `json:\u0026#34;state\u0026#34;` PostalCode string `json:\u0026#34;postalCode\u0026#34;` } type PhoneNumber struct { Type string `json:\u0026#34;type\u0026#34;` Number string `json:\u0026#34;number\u0026#34;` } type Task struct { Name string `json:\u0026#34;name\u0026#34;` DueDate string `json:\u0026#34;dueDate\u0026#34;` Completed bool `json:\u0026#34;completed\u0026#34;` } type Project struct { Name string `json:\u0026#34;name\u0026#34;` Status string `json:\u0026#34;status\u0026#34;` Tasks []Task `json:\u0026#34;tasks\u0026#34;` } type Preferences struct { ContactMethod string `json:\u0026#34;contactMethod\u0026#34;` NewsletterSubscribed bool `json:\u0026#34;newsletterSubscribed\u0026#34;` Languages []string `json:\u0026#34;languages\u0026#34;` } type Person struct { Name string `json:\u0026#34;name\u0026#34;` Age int `json:\u0026#34;age\u0026#34;` Email string `json:\u0026#34;email\u0026#34;` IsActive bool `json:\u0026#34;isActive\u0026#34;` Address Address `json:\u0026#34;address\u0026#34;` PhoneNumbers []PhoneNumber `json:\u0026#34;phoneNumbers\u0026#34;` Projects []Project `json:\u0026#34;projects\u0026#34;` Preferences Preferences `json:\u0026#34;preferences\u0026#34;` } func main() { // JSON数据 data := `{ \u0026#34;name\u0026#34;: \u0026#34;John Doe\u0026#34;, \u0026#34;age\u0026#34;: 30, \u0026#34;email\u0026#34;: \u0026#34;john.doe@example.com\u0026#34;, \u0026#34;isActive\u0026#34;: true, \u0026#34;address\u0026#34;: { \u0026#34;street\u0026#34;: \u0026#34;123 Main St\u0026#34;, \u0026#34;city\u0026#34;: \u0026#34;Anytown\u0026#34;, \u0026#34;state\u0026#34;: \u0026#34;CA\u0026#34;, \u0026#34;postalCode\u0026#34;: \u0026#34;12345\u0026#34; }, \u0026#34;phoneNumbers\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;home\u0026#34;, \u0026#34;number\u0026#34;: \u0026#34;555-555-5555\u0026#34; }, { \u0026#34;type\u0026#34;: \u0026#34;work\u0026#34;, \u0026#34;number\u0026#34;: \u0026#34;555-555-5556\u0026#34; } ], \u0026#34;projects\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;Project Alpha\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;completed\u0026#34;, \u0026#34;tasks\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;Task 1\u0026#34;, \u0026#34;dueDate\u0026#34;: \u0026#34;2023-10-01\u0026#34;, \u0026#34;completed\u0026#34;: true }, { \u0026#34;name\u0026#34;: \u0026#34;Task 2\u0026#34;, \u0026#34;dueDate\u0026#34;: \u0026#34;2023-10-15\u0026#34;, \u0026#34;completed\u0026#34;: false } ] }, { \u0026#34;name\u0026#34;: \u0026#34;Project Beta\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;in progress\u0026#34;, \u0026#34;tasks\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;Task 3\u0026#34;, \u0026#34;dueDate\u0026#34;: \u0026#34;2023-11-01\u0026#34;, \u0026#34;completed\u0026#34;: false }, { \u0026#34;name\u0026#34;: \u0026#34;Task 4\u0026#34;, \u0026#34;dueDate\u0026#34;: \u0026#34;2023-12-01\u0026#34;, \u0026#34;completed\u0026#34;: false } ] } ], \u0026#34;preferences\u0026#34;: { \u0026#34;contactMethod\u0026#34;: \u0026#34;email\u0026#34;, \u0026#34;newsletterSubscribed\u0026#34;: true, \u0026#34;languages\u0026#34;: [\u0026#34;English\u0026#34;, \u0026#34;Spanish\u0026#34;, \u0026#34;German\u0026#34;] } }` var person Person // 解析JSON数据 err := json.Unmarshal([]byte(data), \u0026amp;person) if err != nil { log.Fatalf(\u0026#34;Error parsing JSON: %v\u0026#34;, err) } // 获取第一个项目的第二个任务是否完成 if len(person.Projects) \u0026gt; 0 \u0026amp;\u0026amp; len(person.Projects[0].Tasks) \u0026gt; 1 { isCompleted := person.Projects[0].Tasks[1].Completed fmt.Printf(\u0026#34;The second task of the first project is completed: %v\\n\u0026#34;, isCompleted) } else { fmt.Println(\u0026#34;The required task does not exist.\u0026#34;) } } Too complicated? What if you do not want define the structs in advance?\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 package main import ( \u0026#34;encoding/json\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; ) func main() { // JSON数据 data := `{ ... }` // 定义一个用来存储JSON数据的map var result map[string]interface{} // 解析JSON数据到map err := json.Unmarshal([]byte(data), \u0026amp;result) if err != nil { log.Fatalf(\u0026#34;Error parsing JSON: %v\u0026#34;, err) } // 获取第一个项目的第二个任务是否完成 projects, ok := result[\u0026#34;projects\u0026#34;].([]interface{}) if !ok || len(projects) == 0 { log.Fatalf(\u0026#34;No projects found or format is incorrect\u0026#34;) } firstProject, ok := projects[0].(map[string]interface{}) if !ok { log.Fatalf(\u0026#34;First project format is incorrect\u0026#34;) } tasks, ok := firstProject[\u0026#34;tasks\u0026#34;].([]interface{}) if !ok || len(tasks) \u0026lt; 2 { log.Fatalf(\u0026#34;No tasks found or insufficient tasks in the first project\u0026#34;) } secondTask, ok := tasks[1].(map[string]interface{}) if !ok { log.Fatalf(\u0026#34;Second task format is incorrect\u0026#34;) } completed, ok := secondTask[\u0026#34;completed\u0026#34;].(bool) if !ok { log.Fatalf(\u0026#34;Completed field is missing or not a boolean\u0026#34;) } fmt.Printf(\u0026#34;The second task of the first project is completed: %v\\n\u0026#34;, completed) } Holy shit! Too many template code. Now do you miss the PHP way?\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 \u0026lt;?php $jsonStr = \u0026lt;\u0026lt;\u0026lt;EOF { \u0026#34;name\u0026#34;: \u0026#34;John Doe\u0026#34;, \u0026#34;age\u0026#34;: 30, \u0026#34;email\u0026#34;: \u0026#34;john.doe@example.com\u0026#34;, \u0026#34;isActive\u0026#34;: true, \u0026#34;address\u0026#34;: { \u0026#34;street\u0026#34;: \u0026#34;123 Main St\u0026#34;, \u0026#34;city\u0026#34;: \u0026#34;Anytown\u0026#34;, \u0026#34;state\u0026#34;: \u0026#34;CA\u0026#34;, \u0026#34;postalCode\u0026#34;: \u0026#34;12345\u0026#34; }, \u0026#34;phoneNumbers\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;home\u0026#34;, \u0026#34;number\u0026#34;: \u0026#34;555-555-5555\u0026#34; }, { \u0026#34;type\u0026#34;: \u0026#34;work\u0026#34;, \u0026#34;number\u0026#34;: \u0026#34;555-555-5556\u0026#34; } ], \u0026#34;projects\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;Project Alpha\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;completed\u0026#34;, \u0026#34;tasks\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;Task 1\u0026#34;, \u0026#34;dueDate\u0026#34;: \u0026#34;2023-10-01\u0026#34;, \u0026#34;completed\u0026#34;: true }, { \u0026#34;name\u0026#34;: \u0026#34;Task 2\u0026#34;, \u0026#34;dueDate\u0026#34;: \u0026#34;2023-10-15\u0026#34;, \u0026#34;completed\u0026#34;: false } ] }, { \u0026#34;name\u0026#34;: \u0026#34;Project Beta\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;in progress\u0026#34;, \u0026#34;tasks\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;Task 3\u0026#34;, \u0026#34;dueDate\u0026#34;: \u0026#34;2023-11-01\u0026#34;, \u0026#34;completed\u0026#34;: false }, { \u0026#34;name\u0026#34;: \u0026#34;Task 4\u0026#34;, \u0026#34;dueDate\u0026#34;: \u0026#34;2023-12-01\u0026#34;, \u0026#34;completed\u0026#34;: false } ] } ], \u0026#34;preferences\u0026#34;: { \u0026#34;contactMethod\u0026#34;: \u0026#34;email\u0026#34;, \u0026#34;newsletterSubscribed\u0026#34;: true, \u0026#34;languages\u0026#34;: [\u0026#34;English\u0026#34;, \u0026#34;Spanish\u0026#34;, \u0026#34;German\u0026#34;] } } EOF; $json = json_decode($jsonStr, true); $statusOfTheSecondTaskOfTheFirstProject = $json[\u0026#39;projects\u0026#39;][1][\u0026#39;completed\u0026#39;] ?? false; var_dump($statusOfTheSecondTaskOfTheFirstProject); var_dump(isset($json[\u0026#39;projects\u0026#39;][2][\u0026#39;completed\u0026#39;])); In PHP you don\u0026rsquo;t have to check the validatbility of every level and you can use isset to check only the target key.\nIn golang operators are not allowed to be overrided, so we can use variant variables.\n# Examples See the *_test.go files.\n","date":"2024-07-25T14:12:47+08:00","permalink":"https://blog.happyhacker.fun/p/%E5%83%8Fphp%E9%A3%8E%E6%A0%BC%E4%B8%80%E6%A0%B7%E7%9A%84go-array/","title":"像PHP风格一样的Go Array"},{"content":"前面我写了一篇关于实现一个像PHP风格一样的Go语言map/array/slice解析的文章，在写这篇文章的过程中我还发现了一些有意思的事情，这里记录两件。\n# 判断一个变量的类型 比如一个变量structure的类型可能是map[string]any或者是[]any，正常我们的想法肯定是\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 package mai import \u0026#34;fmt\u0026#34; func main() { var structure any switch structure.(type) { case map[string]any: s := structure.(map[string]any) fmt.Printf(\u0026#34;%v\u0026#34;, s) case []any: s := structure.([]any) fmt.Printf(\u0026#34;%v\u0026#34;, s) } } 我当时写完这段代码的第一反应就是为什么还要在里面再转换一次呢？外面都已经判断过是这样的了，肯定里面要按判断的这个类型去用啊，不过也没有多想，这个时候看到了VSCode有提示\n这个意思就是可以改写成\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 package main import \u0026#34;fmt\u0026#34; func main() { var structure any switch structure := structure.(type) { case map[string]any: fmt.Printf(\u0026#34;%v\u0026#34;, structure) case []any: fmt.Printf(\u0026#34;%v\u0026#34;, structure) } } 这个写法简直完美了，简洁又优雅，虽然感觉有点奇怪。具体的解释可以看Go语言的类型选择。\n补充：刚才验证了下，如果想直接获取一个变量的类型，正常的做法是reflect.TypeOf(structure)，而不是structure.(type)，后者就是只能在switch/case语句中使用的，所以它就是这么一个结构，无论如何，这确实是一个非常好的设计。\n# 使用VSCode粘贴json内容时自动生成structs 在写Garray的过程中，我发现把那段用于测试的json粘贴到Go的代码中时，它会自动转换成struct，而且是会将整个json中用到的所有struct都能定义出来，刚开始我还以为是我复制错了，直到反复确认才确定了确实是VSCode的功能，至于是哪个插件做到的我就不知道了。比如粘贴这样一段json到*.go的代码中时，粘贴进去的将是这样的一组类型定义。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 { \u0026#34;name\u0026#34;: \u0026#34;John Doe\u0026#34;, \u0026#34;age\u0026#34;: 30, \u0026#34;email\u0026#34;: \u0026#34;john.doe@example.com\u0026#34;, \u0026#34;isActive\u0026#34;: true, \u0026#34;address\u0026#34;: { \u0026#34;street\u0026#34;: \u0026#34;123 Main St\u0026#34;, \u0026#34;city\u0026#34;: \u0026#34;Anytown\u0026#34;, \u0026#34;state\u0026#34;: \u0026#34;CA\u0026#34;, \u0026#34;postalCode\u0026#34;: \u0026#34;12345\u0026#34; }, \u0026#34;phoneNumbers\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;home\u0026#34;, \u0026#34;number\u0026#34;: \u0026#34;555-555-5555\u0026#34; }, { \u0026#34;type\u0026#34;: \u0026#34;work\u0026#34;, \u0026#34;number\u0026#34;: \u0026#34;555-555-5556\u0026#34; } ], \u0026#34;projects\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;Project Alpha\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;completed\u0026#34;, \u0026#34;tasks\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;Task 1\u0026#34;, \u0026#34;dueDate\u0026#34;: \u0026#34;2023-10-01\u0026#34;, \u0026#34;completed\u0026#34;: true }, { \u0026#34;name\u0026#34;: \u0026#34;Task 2\u0026#34;, \u0026#34;dueDate\u0026#34;: \u0026#34;2023-10-15\u0026#34;, \u0026#34;completed\u0026#34;: false } ] }, { \u0026#34;name\u0026#34;: \u0026#34;Project Beta\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;in progress\u0026#34;, \u0026#34;tasks\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;Task 3\u0026#34;, \u0026#34;dueDate\u0026#34;: \u0026#34;2023-11-01\u0026#34;, \u0026#34;completed\u0026#34;: false }, { \u0026#34;name\u0026#34;: \u0026#34;Task 4\u0026#34;, \u0026#34;dueDate\u0026#34;: \u0026#34;2023-12-01\u0026#34;, \u0026#34;completed\u0026#34;: false } ] } ], \u0026#34;preferences\u0026#34;: { \u0026#34;contactMethod\u0026#34;: \u0026#34;email\u0026#34;, \u0026#34;newsletterSubscribed\u0026#34;: true, \u0026#34;languages\u0026#34;: [\u0026#34;English\u0026#34;, \u0026#34;Spanish\u0026#34;, \u0026#34;German\u0026#34;] } } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 type AutoGenerated struct { Name string `json:\u0026#34;name\u0026#34;` Age int `json:\u0026#34;age\u0026#34;` Email string `json:\u0026#34;email\u0026#34;` IsActive bool `json:\u0026#34;isActive\u0026#34;` Address Address `json:\u0026#34;address\u0026#34;` PhoneNumbers []PhoneNumbers `json:\u0026#34;phoneNumbers\u0026#34;` Projects []Projects `json:\u0026#34;projects\u0026#34;` Preferences Preferences `json:\u0026#34;preferences\u0026#34;` } type Address struct { Street string `json:\u0026#34;street\u0026#34;` City string `json:\u0026#34;city\u0026#34;` State string `json:\u0026#34;state\u0026#34;` PostalCode string `json:\u0026#34;postalCode\u0026#34;` } type PhoneNumbers struct { Type string `json:\u0026#34;type\u0026#34;` Number string `json:\u0026#34;number\u0026#34;` } type Tasks struct { Name string `json:\u0026#34;name\u0026#34;` DueDate string `json:\u0026#34;dueDate\u0026#34;` Completed bool `json:\u0026#34;completed\u0026#34;` } type Projects struct { Name string `json:\u0026#34;name\u0026#34;` Status string `json:\u0026#34;status\u0026#34;` Tasks []Tasks `json:\u0026#34;tasks\u0026#34;` } type Preferences struct { ContactMethod string `json:\u0026#34;contactMethod\u0026#34;` NewsletterSubscribed bool `json:\u0026#34;newsletterSubscribed\u0026#34;` Languages []string `json:\u0026#34;languages\u0026#34;` } 这简直太妙了。\n","date":"2024-07-25T14:12:15+08:00","permalink":"https://blog.happyhacker.fun/p/%E4%BD%BF%E7%94%A8vscode%E5%BC%80%E5%8F%91go%E7%9A%84%E5%A5%87%E5%A6%99%E4%BD%93%E9%AA%8C-part1/","title":"使用VScode开发Go的奇妙体验-part1"},{"content":"Apache Commons 是一个由Apache软件基金会维护的Java库的集合，旨在提供一系列可重用的开源Java组件。这些库通常解决了广泛的编程问题，使得Java开发者可以更加高效地编码，避免重复造轮子。Apache Commons 系列包括许多不同的组件，每个组件都专注于解冒特定的问题域。下面是一些比较常用的Apache Commons组件的简介：\nCommons Lang：\ncommons-lang 提供了一些核心的Java库的补充，这些库对于日常的Java开发非常有用。它包括工具类来处理字符串操作、数值计算、并发、反射以及更多。 1 2 3 4 5 \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.commons\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;commons-lang3\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;3.14.0\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; Commons IO：\ncommons-io 提供了一系列实用工具，以简化文件、流和文件系统的操作。这些功能包括文件复制、文件过滤、文件监视以及对输入/输出流的操作。 Commons Collections：\ncommons-collections 提供了扩展和优化的集合类，这些集合类在Java标准库中未被包含。它包括新类型的集合，如双向映射、多值映射、有序集合等。 Commons Codec：\ncommons-codec 包括对常见的编解码算法的实现，如Base64、Hex、Phonetic 和 MD5。这些工具类帮助开发者在Java应用中轻松地实现数据编解码。 1 2 3 4 5 \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;commons-codec\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;commons-codec\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.17.1\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; Commons Net：\ncommons-net 包含了许多用于开发网络应用的类，支持许多网络协议，包括FTP、SMTP、Telnet和NNTP。它为这些协议的客户端实现提供了一个框架。 Commons Math：\ncommons-math 是一个涵盖数学和统计组件的库，提供了工具类和方法，用于数学运算、统计分析和数值计算。 Commons Configuration：\ncommons-configuration 提供了各种格式的配置文件（如XML、JSON、Properties文件）的读写支持，以及配置信息的管理。 Commons DBUtils：\ncommons-dbutils 是一个小型的，用来简化JDBC的库，它封装了JDBC的操作，减少常见的数据库编程任务的代码量。 这些组件通常是通过 Maven 或 Gradle 等构建工具引入到项目中的。Apache Commons 库的主要优点是它的稳定性、广泛的测试和活跃的社区支持。使用这些库可以显著提高开发效率并增强应用程序的稳定性和功能。\n","date":"2024-07-19T14:43:20+08:00","permalink":"https://blog.happyhacker.fun/p/apache-commons-%E7%B3%BB%E5%88%97%E5%BA%93%E7%AE%80%E4%BB%8B/","title":"Apache Commons 系列库简介"},{"content":" 这是GPT给的答案，看起来很不错，但实际上shardingsphere问题也很多，而且API变更非常频繁。\n在Spring Boot中实现多数据库配置、数据库分片（Sharding）、以及主从自动切换，可以依赖多种技术和框架组合。一个较为流行且被广泛认为是最优方案之一的组合是使用Spring Boot结合MyBatis或JPA，以及集成ShardingSphere作为数据库中间件。Apache ShardingSphere提供了透明化的数据分片、读写分离、分布式事务等能力。\n# 步骤概览 添加依赖: 在Spring Boot项目的pom.xml中引入ShardingSphere的依赖。\n配置数据源: 在application.yml或application.properties文件中配置ShardingSphere以实现多数据库集成、分片以及读写分离等。\n实现业务逻辑: 使用Spring Data JPA或MyBatis等进行数据访问。\n# 1. 添加依赖 以使用Spring Boot 2.x版本和ShardingSphere 5.x版本为例，首先在pom.xml中添加必要的依赖：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 \u0026lt;!-- Spring Boot的启动器依赖 --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;!-- ShardingSphere的数据分片和读写分离依赖 --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.shardingsphere\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;shardingsphere-jdbc-core-spring-boot-starter\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;5.x.x\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;!-- MyBatis或Spring Data JPA依赖（基于选择） --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-mybatis\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; # 2. 配置数据源 接下来，在application.yml中配置ShardingSphere以实现数据源的分片和读写分离：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 spring: shardingsphere: datasource: names: ds0,ds1 # 数据源名称 ds0: # 第一个数据源配置 type: com.zaxxer.hikari.HikariDataSource driver-class-name: com.mysql.cj.jdbc.Driver jdbc-url: jdbc:mysql://localhost:3306/ds0?serverTimezone=UTC\u0026amp;useSSL=false username: root password: password ds1: # 第二个数据源配置 type: com.zaxxer.hikari.HikariDataSource driver-class-name: com.mysql.cj.jdbc.Driver jdbc-url: jdbc:mysql://localhost:3306/ds1?serverTimezone=UTC\u0026amp;useSSL=false username: root password: password rules: sharding: # 分片策略配置 tables: your_table: actual-data-nodes: ds$-\u0026gt;{0..1}.your_table table-strategy: inline: sharding-column: id algorithm-expression: your_table$-\u0026gt;{id % 2} binding-tables: your_table readwrite-splitting: # 读写分离配置 dataSources: ds0replica: primary-data-source-name: ds0 replica-data-source-names: ds0replica0,ds0replica1 load-balancer-name: round_robin loadBalancers: round_robin: type: ROUND_ROBIN # 3. 实现业务逻辑 接下来，你可以像平常一样使用MyBatis或JPA来实现你的业务逻辑。ShardingSphere会根据你的配置自动进行数据分片和读写分离。\n# 注意事项 版本兼容性: 确保你所使用的Spring Boot、ShardingSphere以及任何ORM框架（如MyBatis或JPA）的版本是相互兼容的。 性能测试: 在生产环境部署前，进行全面的性能测试是非常必要的，确保配置的分片和读写分离策略符合你的性能需求。 事务管理: 如果你的应用需要处理跨多个数据库实例的事务，需要仔细考虑ShardingSphere提供的分布式事务解决方案。 通过上述步骤，你可以在Spring Boot应用中实现高效的多数据库配置、数据分片以及主从自动切换，以满足复杂的业务需求。\n","date":"2024-07-19T10:56:38+08:00","permalink":"https://blog.happyhacker.fun/p/springboot%E5%BA%94%E7%94%A8%E7%9A%84sharding%E6%96%B9%E6%A1%88/","title":"SpringBoot应用的Sharding方案"},{"content":"之前体验过一把IDEA的远程开发，只能说能看出来IntelliJ想把功能做到很完善，也确实做到了，但由于占用资源太多，即使远程使用的是配置相对非常高的服务器，占用10G+的内存也不是一个好的选择吧，尝试了几次之后就被惊人的延迟和频繁丢失的输入给劝退了。\n听说VSCode的远程开发效果非常不错，所以想尝试一下。只需要给服务器开一个专用的SSH端口，把用密码登录的选项禁用掉，就相对安全了。只要有了SSH通道，整个配置就已经完成了，对，就是这么简单。\n以RockyLinux8的容器为例。\n# 在容器中安装并启动SSH服务 1 2 dnf install -y ssh-server /usr/sbin/ssh-server 这时不出意外会报错，因为配置文件中指定的key文件不存在，执行以下三条命令之后就好了。\n1 2 3 ssh-keygen -t rsa -f /etc/ssh/ssh_host_rsa_key ssh-keygen -t ecdsa -f /etc/ssh/ssh_host_ecdsa_key ssh-keygen -t ed25519 -f /etc/ssh/ssh_host_ed25519_key # 创建一个开发专用的账号，赋予一个登录的shell，指定Home目录，并赋予相应的权限。 1 2 3 4 5 useradd dev usermod -s /bin/bash dev usermod -aG wheel dev usermod -d /data1/code dev chown -R dev. /data1/code 好了，现在/data1/code已经是dev账户的家目录了。\n现在设置登录相关的信息。随便设置一个密码，其实可以尽量复杂一些。\n1 passwd dev #接下来要输入密码 这时候尝试从远程登录，可能会这个错误\n1 System is booting up. Unprivileged users are not permitted to log in yet. Please come back later. For technical details, see pam_nologin(8). 你知道，系统并没有正在启动，这明显是一个误报。针对不同的系统可能不同，对于Rocky Linux 8而言，需要删除/run/nologin文件，再尝试登录就可以了。\n删除的这个文件中的内容恰恰就是上面的报错信息，意外吧。\n# 修改sshd配置文件 密码登录既不方便也不安全，需要先用ssh-copy-id来实现证书登录，再从服务端关闭密码登录功能。\n1 ssh-copy-id dev@remote-host -p{newPort} 这就可以了。然后在服务端的/etc/ssh/sshd_config中找到\n1 PasswordAuthentication yes 把yes改成no，重启ssh-server，就会发现已经无法用密码登录了，也就安全了。\n搞定了。\n# 总结 最近使用VSCode比较多，我发现其实绝大多数情况下使用VSCode就已经够了，完全不需要JetBrains家的产品了，甚至JB家引以为傲的Java开发好像也没有那么大的优势了，jdt-lsp的很多提示已经比IDEA更友好了，更容易在代码上线运行之前就发现问题。举一个例子\n1 2 3 4 5 @Builder public class Data { private Integer age = 0; } 上面的代码非常具有迷惑性，因为实际运行时你会发现，如果没有执行.age()方法，这个值将是null，而不是0，这在VSCode中就会提示如果不使用@Builder.Default，则代码中指定的这个=0的值将直接被抛弃。这非常有用。\n","date":"2024-07-17T14:53:16+08:00","permalink":"https://blog.happyhacker.fun/p/vscode%E8%BF%9C%E7%A8%8B%E5%BC%80%E5%8F%91%E5%88%9D%E4%BD%93%E9%AA%8C/","title":"VSCode远程开发初体验"},{"content":" # topic相关 # 创建topic 1 kafka-topics --create --bootstrap-server localhost:9092 --replication-factor 1 --partitions 1 --topic my-topic # topic列表 1 kafka-topics --list --bootstrap-server localhost:9092 # 修改topic 1 kafka-topics --alter --bootstrap-server localhost:9092 --topic my-topic --partitions 2 注意：并不支持修改\u0026ndash;replication-factor，而只能通过迁移topic来实现\n# 删除topic 1 kafka-topics --delete --bootstrap-server localhost:9092 --topic my-topic # 查看topic的详细配置 1 kafka-topics --describe --bootstrap-server localhost:9092 --topic my-topic # 生产 1 kafka-console-producer --bootstrap-server localhost:9092 --topic my-topic # 消费 1 kafka-console-consumer --bootstrap-server localhost:9092 --topic my-topic --from-beginning # 消费者组 # 列出所有的消费者组 1 kafka-consumer-groups --bootstrap-server localhost:9092 --list # 查看指定消费者组的偏移量 1 kafka-consumer-groups --bootstrap-server localhost:9092 --describe --group my-consumer-group 注意：这两个命令都不支持指定topic\n# 总结 以上基本就是在命令行需要用到的所有命令了。\n","date":"2024-07-14T10:32:58+08:00","permalink":"https://blog.happyhacker.fun/p/homebrew-kafka-commands/","title":"Homebrew Kafka Commands"},{"content":"PyInstaller 和 Nuitka 在将 Python 应用程序打包成独立的可执行文件时，可能会导致启动时间变慢。以下是一些可能导致启动时间变慢的原因以及相应的解决方法：\n# 1. 文件大小和加载时间 原因：打包后的可执行文件通常较大，包含所有依赖项和 Python 解释器。较大的文件可能需要更长的时间来加载和初始化。\n解决方法：\n使用 --onefile 选项将所有内容打包成一个单独的可执行文件，但这可能会增加加载时间。 尝试使用 --onedir 选项将所有内容打包成一个目录，这样可以减少单个文件的加载时间。 # 2. 代码优化 原因：打包后的可执行文件可能包含一些不必要的代码或模块，这些代码在启动时会被加载和初始化。\n解决方法：\n优化代码，移除不必要的导入和初始化操作。 使用 if __name__ == \u0026quot;__main__\u0026quot;: 确保只在主程序中执行必要的代码。 # 3. 冻结时间（Freeze Time） 原因：Nuitka 在编译过程中会将 Python 代码转换为 C++ 代码，并进行优化。这个过程可能会导致启动时间变慢。\n解决方法：\n使用 Nuitka 的 --standalone 选项生成独立的可执行文件，但可能会增加启动时间。 尝试使用 Nuitka 的 --onefile 选项将所有内容打包成一个单独的可执行文件，但这可能会进一步增加启动时间。 # 4. 动态链接库（DLL）加载 原因：打包后的可执行文件可能需要加载多个动态链接库（DLL），这可能会增加启动时间。\n解决方法：\n确保所有依赖项的 DLL 文件都正确包含在打包文件中。 使用 patchelf 工具（Linux）或 rcedit 工具（Windows）调整可执行文件的依赖项。 # 5. 调试信息 原因：打包过程中可能包含调试信息，这会增加文件大小并影响启动时间。\n解决方法：\n使用 --strip 选项（Linux）或 /DEBUG:NONE 选项（Windows）移除调试信息。 # 6. 缓存和优化 原因：首次运行打包后的可执行文件时，可能需要进行一些初始化和优化操作。\n解决方法：\n确保在打包过程中启用所有优化选项，例如 Nuitka 的 --lto 选项。 多次运行可执行文件，以利用缓存和优化。 # 总结 PyInstaller 和 Nuitka 打包后的可执行文件启动时间变慢可能是由于文件大小、代码优化、冻结时间、动态链接库加载、调试信息以及缓存和优化等因素导致的。通过优化打包选项、代码和依赖项，可以减少启动时间并提高性能。\n","date":"2024-07-09T14:49:27+08:00","image":"https://blog.happyhacker.fun/images/covers/python-growing.webp","permalink":"https://blog.happyhacker.fun/p/%E5%8F%AF%E6%89%A7%E8%A1%8C%E6%96%87%E4%BB%B6%E5%90%AF%E5%8A%A8%E6%85%A2%E7%9A%84%E5%8E%9F%E5%9B%A0%E5%88%86%E6%9E%90/","title":"可执行文件启动慢的原因分析"},{"content":"有几种方法可以将 Python 解释器和您的应用程序一起打包，以创建一个完全独立的可执行文件。以下是几种常见的方法：\n# 1. PyInstaller PyInstaller 是一个流行的工具，可以将 Python 应用程序及其所有依赖项打包成一个独立的可执行文件，适用于 Windows、Mac 和 Linux。\n# 安装 PyInstaller 1 pip install pyinstaller # 使用 PyInstaller 1 pyinstaller --onefile your_script.py --onefile 选项将所有内容打包成一个单独的可执行文件。\n如果没有特别的要求，目前来看PyInstaller还是第一位的选择。\n如果要将一些二进制文件比如ffmpeg也打包进来，可以使用 --add-binary来进行添加，具体参数是这样 --add-binary='/path/to/ffmpeg:.，其中 /path/to/ffmpeg是二进制文件的实际路径，而后面的 .表示要打包到当前路径下，如果就多个就跟多个 --add-binary。\n完整的命令是这样的\n1 pyinstaller --onefile --add-binary=\u0026#39;/opt/homebrew/Cellar/ffmpeg/7.0.1/bin/ffmpeg:.\u0026#39; --add-binary=\u0026#39;/opt/homebrew/Cellar/ffmpeg/7.0.1/bin/ffprobe:.\u0026#39; hello.py 执行完之后会在当前路径下生成一个 hello.spec，就是把上面指定的这些东西保存到配置文件了，后面如果你再需要添加就直接往配置文件里添加即可。然后直接执行 pyinstaller hello.spec，就可以使用里面的所有配置了。\n# 2. Nuitka Nuitka 是一个 Python 到 C++ 的编译器，可以将 Python 代码编译成高度优化的可执行文件。\n# 安装 Nuitka 1 pip install nuitka # 使用 Nuitka 1 nuitka --standalone --onefile your_script.py --standalone 选项生成一个独立的可执行文件，--onefile 选项将其打包成一个文件。\n# 3. PyOxidizer PyOxidizer 是一个工具，可以将 Python 应用程序及其所有依赖项打包成一个独立的可执行文件，支持 Windows、Mac 和 Linux。\n# 安装 PyOxidizer 1 cargo install pyoxidizer # 使用 PyOxidizer 1 pyoxidizer build --release 这将生成一个独立的可执行文件。\n实际上pyoxidizer需要一个还挺复杂的配置文件，详情还得查看对应的官方文档。不过我发现pyinstaller和nuitka已经可以满足需求了，就没有继续研究这个了。\n# 4. shiv 与 Docker 虽然 shiv 本身不包含 Python 解释器，但您可以结合 Docker 来创建一个包含所有依赖项的容器镜像。\n# 创建 Dockerfile 1 2 3 4 5 6 7 8 9 FROM python:3.8-slim COPY . /app WORKDIR /app RUN pip install shiv RUN shiv -o my_app.pyz -c my_app . CMD [\u0026#34;python\u0026#34;, \u0026#34;my_app.pyz\u0026#34;] # 构建 Docker 镜像 1 docker build -t my_app_image . # 运行 Docker 容器 1 docker run -it my_app_image 这种方法将 Python 解释器和应用程序及其依赖项打包在一个 Docker 容器中。\n# 总结 以上方法都可以将 Python 解释器和应用程序一起打包，创建一个完全独立的可执行文件。选择哪种方法取决于您的具体需求和目标平台。\n但是测试发现PyInstaller和Nuitka打包成的可执行文件，在启动时都有一个明显的延迟，这里是解释\n","date":"2024-07-09T11:31:01+08:00","image":"https://blog.happyhacker.fun/images/covers/python-growing.webp","permalink":"https://blog.happyhacker.fun/p/%E5%88%9B%E5%BB%BA%E4%B8%80%E4%B8%AA%E5%AE%8C%E6%95%B4%E7%9A%84%E5%8F%AF%E7%A7%BB%E6%A4%8D%E7%9A%84python%E8%BF%90%E8%A1%8C%E7%8E%AF%E5%A2%83/","title":"创建一个完整的可移植的Python运行环境"},{"content":"如果shiv不能把解释器打包进去，不就丧失了可移植性吗？如果只能在当前机器上移动，这还有什么实际的应用场景呢？\n确实，shiv 本身并不包含 Python 解释器，这可能会影响其可移植性。然而，shiv 的设计目标和应用场景有其特定的考虑和优势：\n# shiv 的设计目标和优势 轻量级和快速部署：shiv 生成的 .pyz 文件非常轻量级，因为它不包含 Python 解释器。这使得文件传输和部署速度更快。 依赖系统 Python：shiv 依赖于目标系统上已安装的 Python 解释器。这意味着您可以利用系统上已有的 Python 环境，而不需要额外打包解释器。 简化依赖管理：shiv 可以与 pip 和 requirements.txt 文件结合使用，简化依赖项的管理和打包过程。 # 实际应用场景 尽管 shiv 不包含 Python 解释器，但它仍然在以下场景中非常有用：\n内部工具和脚本：在企业内部或团队内部，假设所有目标机器都安装了兼容的 Python 环境，shiv 可以用于快速部署和共享内部工具和脚本。 持续集成和部署（CI/CD）：在 CI/CD 流程中，假设构建和部署环境都安装了兼容的 Python 环境，shiv 可以用于自动化构建和部署过程。 开发和测试环境：在开发和测试阶段，假设开发和测试机器都安装了兼容的 Python 环境，shiv 可以用于快速分发和测试代码。 临时任务和脚本：对于一些临时任务和脚本，如果目标机器已经安装了兼容的 Python 环境，shiv 可以用于快速执行这些任务。 # 结合其他工具增强可移植性 虽然 shiv 本身不包含 Python 解释器，但您可以结合其他工具来增强其可移植性：\nDocker：结合 Docker 容器化技术，您可以将 shiv 生成的 .pyz 文件与所需的 Python 解释器一起打包成一个 Docker 镜像。这样，您可以在任何支持 Docker 的环境中运行应用程序。\n1 2 3 4 5 6 7 8 9 FROM python:3.8-slim COPY . /app WORKDIR /app RUN pip install shiv RUN shiv -o my_app.pyz -c my_app . CMD [\u0026#34;python\u0026#34;, \u0026#34;my_app.pyz\u0026#34;] PyInstaller 或 Nuitka：如果您需要完全独立的可执行文件，可以使用 PyInstaller 或 Nuitka 将 Python 解释器和应用程序一起打包。\n# 总结 shiv 的设计和应用场景主要集中在轻量级和快速部署上，依赖于目标系统上已安装的 Python 环境。虽然这可能会影响其可移植性，但结合 Docker 或其他打包工具，您可以增强其可移植性，使其适用于更广泛的场景。\n","date":"2024-07-09T11:18:50+08:00","image":"https://blog.happyhacker.fun/images/covers/python-growing.webp","permalink":"https://blog.happyhacker.fun/p/shiv%E8%A7%A3%E5%86%B3%E4%BA%86%E4%BB%80%E4%B9%88%E9%97%AE%E9%A2%98/","title":"shiv解决了什么问题"},{"content":"使用 pip install dist/my_simple_package-0.1.0-py3-none-any.whl 命令安装包后，该包会被安装到你的Python环境中。具体的安装位置取决于你使用的Python环境类型和配置。\n# 安装位置 系统全局环境：\n如果使用系统Python并且没有激活任何虚拟环境，包通常安装在Python的 site-packages 目录中。例如：\n1 /usr/local/lib/python3.9/site-packages/my_simple_package 你可以通过以下命令查看 site-packages 目录的位置：\n1 2 import site print(site.getsitepackages()) 虚拟环境：\n如果使用的是虚拟环境，包会安装在虚拟环境的 site-packages 目录中。例如：\n1 /path/to/venv/lib/python3.9/site-packages/my_simple_package 你可以通过以下命令查看虚拟环境的 site-packages 目录位置：\n1 2 import site print(site.getsitepackages()) 用户目录：\n如果使用 --user 选项进行安装，包会被安装到用户目录下的 site-packages 目录。例如：\n1 ~/.local/lib/python3.9/site-packages/my_simple_package # 引用安装的包 一旦包被安装到Python环境中，你可以像引用任何其他标准库或已安装的第三方包一样引用它。假设包名是 my_simple_package，可以在Python脚本或交互式环境中使用以下代码引用该包：\n1 2 3 4 import my_simple_package.simple_module as sm print(sm.hello()) # Output: Hello, World! # 检查包是否成功安装 你可以通过以下几种方法检查包是否成功安装：\n使用 pip list 查看已安装的包：\n1 pip list | grep my_simple_package 使用 pip show 获取包的详细信息：\n1 pip show my_simple_package 在Python环境中尝试导入包：\n1 2 import my_simple_package print(my_simple_package) # 示例 假设你已经创建并安装了一个名为 my_simple_package 的包，以下是一个完整的示例流程：\n创建并安装包：\n1 2 python setup.py sdist bdist_wheel pip install dist/my_simple_package-0.1.0-py3-none-any.whl 引用包：\n1 2 3 4 import my_simple_package.simple_module as sm print(sm.hello()) # Output: Hello, World! 通过这些步骤，你可以成功安装和引用一个本地创建并通过 pip 安装的Python包。\n","date":"2024-07-08T15:07:16+08:00","image":"https://blog.happyhacker.fun/images/covers/python-growing.webp","permalink":"https://blog.happyhacker.fun/p/pip%E5%AE%89%E8%A3%85%E7%9A%84%E5%8C%85%E5%9C%A8%E5%93%AA%E9%87%8C/","title":"pip安装的包在哪里？"},{"content":"要创建一个最简单的 Python 包来演示包的作用，你只需创建一个包含必要文件和目录结构的项目。这个示例包不会有实际的功能，但它会展示如何构建和打包一个 Python 项目。\n# 目录结构 1 2 3 4 5 6 7 my_simple_package/ ├── my_simple_package/ │ ├── __init__.py │ └── simple_module.py ├── README.md ├── setup.py └── pyproject.toml # 详细步骤 创建包目录\n首先创建一个名为 my_simple_package 的目录，这是你的项目根目录。\n创建包的子目录\n在项目根目录下，再创建一个同名的子目录 my_simple_package，这是实际的包目录。\n创建 __init__.py 文件\n在 my_simple_package 目录下创建一个空的 __init__.py 文件，这个文件告诉 Python 这个目录是一个包。\n1 # my_simple_package/__init__.py 创建一个简单的模块\n在 my_simple_package 目录下创建一个名为 simple_module.py 的文件，并在其中添加一些简单的代码。\n1 2 3 4 # my_simple_package/simple_module.py def hello(): return \u0026#34;Hello, World!\u0026#34; 创建 README 文件\n在项目根目录下创建一个 README.md 文件，为包提供一些基本的描述。\n1 2 3 # My Simple Package This is a simple package to demonstrate how to create a Python package. 创建 setup.py 文件\n在项目根目录下创建一个 setup.py 文件，用于设置包的元数据和安装信息。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 # setup.py from setuptools import setup, find_packages setup( name=\u0026#34;my_simple_package\u0026#34;, version=\u0026#34;0.1.0\u0026#34;, packages=find_packages(), install_requires=[], # List your package dependencies here author=\u0026#34;Your Name\u0026#34;, author_email=\u0026#34;your.email@example.com\u0026#34;, description=\u0026#34;A simple package to demonstrate packaging in Python\u0026#34;, long_description=open(\u0026#39;README.md\u0026#39;).read(), long_description_content_type=\u0026#34;text/markdown\u0026#34;, url=\u0026#34;http://example.com/my_simple_package\u0026#34;, classifiers=[ \u0026#34;Programming Language :: Python :: 3\u0026#34;, \u0026#34;License :: OSI Approved :: MIT License\u0026#34;, \u0026#34;Operating System :: OS Independent\u0026#34;, ], python_requires=\u0026#39;\u0026gt;=3.6\u0026#39;, ) 创建 pyproject.toml 文件\n在项目根目录下创建一个 pyproject.toml 文件，指定构建系统的要求。\n1 2 3 4 # pyproject.toml [build-system] requires = [\u0026#34;setuptools\u0026#34;, \u0026#34;wheel\u0026#34;] build-backend = \u0026#34;setuptools.build_meta\u0026#34; # 构建和安装 构建包\n在项目根目录下运行以下命令，生成源代码分发包和二进制分发包：\n1 python setup.py sdist bdist_wheel 这将会在 dist 目录下生成 .tar.gz 和 .whl 文件。\n安装包\n你可以使用 pip 安装生成的包文件：\n1 pip install dist/my_simple_package-0.1.0-py3-none-any.whl 那么安装了之后的包去了哪里呢？参考pip安装的包去了哪里\n# 测试包 你可以在 Python 环境中测试你的包是否正常工作：\n1 2 3 4 import my_simple_package.simple_module as sm print(sm.hello()) # Output: Hello, World! # 总结 这个简单的 Python 包演示了包的基本结构和构建过程。通过这个示例，你可以了解如何创建、打包和分发一个 Python 包。尽管这个包没有实际的功能，但它展示了所有必要的步骤和文件。\n","date":"2024-07-08T15:04:13+08:00","image":"https://blog.happyhacker.fun/images/covers/python-growing.webp","permalink":"https://blog.happyhacker.fun/p/%E6%9E%84%E5%BB%BA%E4%B8%80%E4%B8%AA%E6%9C%80%E7%AE%80%E5%8D%95%E7%9A%84python%E5%8C%85/","title":"构建一个最简单的Python包"},{"content":"虽然 Python 是一种解释型脚本语言，不需要像 C 或 C++ 那样进行编译，但在开发和发布 Python 项目时，仍然有一些构建步骤是必要的。这些步骤可以包括依赖管理、打包、测试、文档生成等。因此，Python 项目通常需要一些构建工具来简化和自动化这些任务。\n# 常见的 Python 构建工具 setuptools：\n功能：提供基础的打包和分发功能，支持依赖管理、入口点和扩展等。 使用：通过 setup.py 和 pyproject.toml 文件进行配置。 wheel：\n功能：将 Python 项目打包成 .whl 格式的二进制分发包，方便用户安装。 使用：通常与 setuptools 一起使用，通过 python setup.py bdist_wheel 命令生成。 pip：\n功能：主要用于包管理和安装，但也支持从源代码进行构建和安装。 使用：通过 requirements.txt 或 pyproject.toml 文件定义依赖。 build：\n功能：一个现代的构建工具，用于生成源代码分发包（sdist）和二进制分发包（wheel）。 使用：通过 python -m build 命令执行。 tox：\n功能：用于自动化测试和环境管理，支持在多个 Python 版本和虚拟环境中运行测试。 使用：通过 tox.ini 文件进行配置。 flit：\n功能：一个简化的打包和发布工具，主要用于纯 Python 包。 使用：通过 pyproject.toml 文件进行配置。 poetry：\n功能：一个现代的依赖管理和打包工具，集成了构建、依赖管理、版本管理和发布等功能。 使用：通过 pyproject.toml 文件进行配置，并且有自己的 poetry.lock 文件。 pbr：\n功能：专注于简化 setuptools 配置，自动化版本控制和包信息管理。 使用：通过 setup.cfg 文件进行配置。 # 为什么 Python 需要构建？ 尽管 Python 是解释型语言，但在实际开发和发布过程中，仍然有一些任务需要构建步骤：\n依赖管理：\n确保项目在不同环境中有相同的依赖版本，避免兼容性问题。 打包和分发：\n将项目打包成标准格式（如 wheel 或 sdist），方便用户安装和使用。 性能优化：\n通过构建，将一些关键模块编译为 C 扩展，以提高性能。 代码检查和测试：\n自动化运行代码检查工具（如 flake8、pylint）和测试框架（如 pytest），确保代码质量。 文档生成：\n使用工具（如 Sphinx）生成项目文档，确保文档的一致性和可维护性。 # 构建生成的结果 源代码分发包（sdist）：\n包含项目的源代码和元数据，通常是一个 .tar.gz 文件。 通过 python setup.py sdist 或 python -m build 生成。 二进制分发包（wheel）：\n包含编译后的二进制文件，通常是一个 .whl 文件。 通过 python setup.py bdist_wheel 或 python -m build 生成。 安装包：\n最终用户可以通过 pip install 命令安装打包好的项目，而不需要关心底层的构建细节。 # 总结 尽管 Python 是解释型语言，但项目的开发和发布过程仍然需要构建步骤来管理依赖、打包、测试和优化。通过使用适当的构建工具，可以显著简化和自动化这些任务，提高项目的可维护性和可移植性。\n","date":"2024-07-08T14:25:52+08:00","image":"https://blog.happyhacker.fun/images/covers/python-builders.png","permalink":"https://blog.happyhacker.fun/p/python%E6%9E%84%E5%BB%BA%E5%B7%A5%E5%85%B7/","title":"Python构建工具"},{"content":"pyproject.toml 文件是 Python 项目的新标准配置文件，它遵循 PEP 517 和 PEP 518 的规范。这个文件的主要目的是定义项目构建系统的要求，从而使得项目的构建过程更加标准化和独立于具体的构建工具。\n# pyproject.toml 的主要功能 定义构建系统要求：pyproject.toml 文件中可以指定项目所需的构建工具及其版本。例如：\n1 2 3 [build-system] requires = [\u0026#34;setuptools\u0026#34;, \u0026#34;wheel\u0026#34;] build-backend = \u0026#34;setuptools.build_meta\u0026#34; 这表示项目需要 setuptools 和 wheel 这两个工具来进行构建，并且使用 setuptools.build_meta 作为构建后端。\n项目元数据：除了构建系统要求，pyproject.toml 还可以包含项目的其他元数据，例如项目名称、版本、作者等信息。这些信息通常在 setup.py 中通过 setup() 函数来定义。\n1 2 3 4 5 6 7 [project] name = \u0026#34;example\u0026#34; version = \u0026#34;0.1.0\u0026#34; description = \u0026#34;An example package\u0026#34; authors = [ { name=\u0026#34;John Doe\u0026#34;, email=\u0026#34;john.doe@example.com\u0026#34; } ] 其他配置：pyproject.toml 还可以包含其他工具的配置，例如 pytest、black 等，使得项目的配置更加集中和统一。\n# 与 setup.py 的比较 标准化：pyproject.toml 提供了一种标准化的方式来定义构建系统要求，使得项目的构建过程更加一致和可预测。 独立性：pyproject.toml 使得项目的构建过程独立于具体的构建工具，用户不需要了解或修改 setup.py 文件，只需要安装指定的构建工具即可。 扩展性：pyproject.toml 可以包含更多的配置信息，不仅限于构建系统要求，还可以包含项目的其他元数据和工具配置。 # 之前的构建工具定义方式 在 pyproject.toml 出现之前，项目的构建工具通常在 setup.py 文件中定义。例如：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 from setuptools import setup setup( name=\u0026#39;example\u0026#39;, version=\u0026#39;0.1.0\u0026#39;, packages=[\u0026#39;example\u0026#39;], install_requires=[ \u0026#39;some_dependency\u0026#39;, ], entry_points={ \u0026#39;console_scripts\u0026#39;: [ \u0026#39;example-cli=example.main:main\u0026#39;, ], }, ) 这种方式的缺点是：\n依赖隐式：用户需要阅读 setup.py 文件才能了解项目所需的构建工具和依赖。 不标准化：不同的项目可能使用不同的方式来定义构建工具，导致构建过程不一致。 # 总结 pyproject.toml：提供了一种标准化的方式来定义构建系统要求，使得项目的构建过程更加一致和可预测。 功能增强：pyproject.toml 不仅限于构建系统要求，还可以包含项目的其他元数据和工具配置。 之前的定义方式：通常在 setup.py 文件中定义，依赖隐式且不标准化。 通过使用 pyproject.toml，Python 项目的构建过程变得更加标准化和独立于具体的构建工具，提高了项目的可维护性和可移植性。\n","date":"2024-07-08T14:23:21+08:00","image":"https://blog.happyhacker.fun/images/covers/python-pyproject.png","permalink":"https://blog.happyhacker.fun/p/pyproject%E7%AE%80%E6%98%8E%E4%BB%8B%E7%BB%8D/","title":"Pyproject简明介绍"},{"content":"引用一下Flutter中的章节，说明需要用到shiv的地方。\n我们知道，用Go语言写出来的程序是平台有关的二进制文件，随便复制到相同的平台下都可以运行，但Python很多时候不是，因为用PyPI（pip）安装的包并不会打到Python脚本里去，直接拿一个引用了很多其他包的脚本放在其他机器上是无法执行的，所以需要一个把整个运行环境都复制出来可随处移动的工具，而Shiv就是干这个用的。\n# Python 项目管理方式介绍 这是理解shiv的工作原理的重点，希望能仔细理解。\n# setup.py 这应该是最传统的方式了，把基础信息和包的信息都写在一个py脚本里，基本的样式如下\n1 2 3 4 5 6 7 8 9 10 11 12 13 # setup.py from setuptools import setup, find_packages setup( name=\u0026#39;example\u0026#39;, version=\u0026#39;0.1\u0026#39;, packages=find_packages(), entry_points={ \u0026#39;console_scripts\u0026#39;: [ \u0026#39;example-cli=example.main:main\u0026#39;, ], }, ) 简单介绍一下这些字段的含义。\nname就是项目名了，注意是项目名，而不是最终生成的可执行文件的名字，version是版本号，不再过多解释。\npackages=find_packages() 值得解释一下。正常来说这里是要手写一些包的，但这里使用了一个方法来动态查找这些包。\n那么问题来了，什么是包呢？\n在其他语言里，通常一级目录就是一个包了，但Python里并不是，还需要一个通常为空的文件 __init__.py，可以查一下为什么要有这个文件。当存在这个文件的时候才认为这是一个包，比如说我们有一个包 example，就需要是这样的结构\n1 2 3 4 5 -- example_project -- setup.py -- example -- __init__.py -- main.py 正常来说，这时候在 packages处应该填写的是 ['example']，但由于这个规则的存在，可以使用 find_packages方法查找整个目录下所有存在 __init__.py的目录，把它们的名字放在 packages里。当然这个方法还有别的参数，这里就不再赘述。\n下面就是程序的可执行文件了。顾名思义，console_scripts就是在终端能直接使用的脚本名，作为脚本语言，Python有两种写法\n1 print(\u0026#39;hello world\u0026#39;) 这时候直接 python main.py是可以的，但在标准化的项目里，通常是像下面这样的写法\n1 2 3 4 5 def main(): print(\u0026#39;hello world\u0026#39;) if __name__ == \u0026#39;__main__\u0026#39;: main() 这里就定义了一个 main方法，其实就类似于C系列语言中的main函数了。\n问题讨论到这里就已经很清晰了，我们有了包名 example，里面有一个 hello模块，其实就是 hello.py，现在又有了方法名 main，其实已经可以引用它了，没错就是 example.hello:main\n搞清楚了这些问题，就可以看看Python包的安装和使用了。\n1 2 3 pip install setuptools python setup.py pip install . 如果当前使用的Python环境是用virtualenv安装的，这时候就会在 venv/bin/下生成相应的可执行文件，对应上面的例子，就是 example-cli。执行 venv/bin/example-cli就能运行上面的代码了。\n到这里终于把前置的问题都说清楚了，现在可以讨论shiv的问题了。\n# 其他方式 # setup.cfg 后面又发展除了 setup.cfg，用ini的语法来写一些静态的内容，没有解决什么核心的问题。\n1 2 3 4 5 6 7 8 9 10 11 # setup.cfg [metadata] name = example version = 0.1 [options] packages = find: [options.entry_points] console_scripts = example-cli = example.main:main # pyproject.toml pyproject.toml是一个新的标准，用于定义构建系统的要求。它使用TOML格式，并且旨在取代setup.py和setup.cfg。pyproject.toml文件可以包含构建工具的配置，如setuptools、wheel、flit等。\n1 2 3 4 5 6 7 8 9 10 # pyproject.toml [build-system] requires = [\u0026#34;setuptools\u0026#34;, \u0026#34;wheel\u0026#34;] [project] name = \u0026#34;example\u0026#34; version = \u0026#34;0.1\u0026#34; [project.scripts] example-cli = \u0026#34;example:main\u0026#34; 这个看起来就有点Cargo.toml的味道了。\n# shiv解决了什么问题？ 简单来讲，就是把当前程序和它的所有依赖打成一个包，让它成为一个相对独立的发行版本。有点类似Java的fatjar。\n注意这里说的是相对独立，因为Python的很多包其实是一个Wrapper，也就是对其他可执行文件/库的Python封装，而不是实现，所以如果你引用的包是一个Wrapper，那么用shiv也不能解决这个问题。\n# 简单的例子 新建一个目录叫 shiv_demo，写一个脚本叫 hello.py，内容如下\n1 2 3 4 5 6 7 8 import simplejson as json def main(): r = json.dumps([\u0026#39;foo\u0026#39;, {\u0026#39;bar\u0026#39;: (\u0026#39;baz\u0026#39;, None, 1.0, 2)}]) print(r) if __name__ == \u0026#39;__main__\u0026#39;: main() 可以看到依赖 simplejson，安装它\n1 pip install simplejson shiv 配置一个 setup.py\n1 2 3 4 5 6 7 8 9 10 11 from setuptools import setup setup( name=\u0026#34;hello-world\u0026#34;, version=\u0026#34;0.0.1\u0026#34;, description=\u0026#34;Greet the world.\u0026#34;, py_modules=[\u0026#34;hello\u0026#34;], entry_points={ \u0026#34;console_scripts\u0026#34;: [\u0026#34;hello=hello:main\u0026#34;], }, ) 这时候就可以给它打包了\n1 shiv -c hello -o hello.pyz . 这时候打包生成了一个 hello.pyz文件，就可以把这个文件拿到别处去执行了，然后你就会发现报错。。。\n到这里验证的时候我才发现，其实shiv解决的问题非常有限，参考这里shiv解决了什么问题，但起码它可以让你的程序在安装了相同的Python执行器的机器之间移植。下面测试一下，在另外一个目录里再执行\n1 2 virtualenv venv --python=3.12 source venv/bin/activate 然后执行刚刚打包生成的 hello.pyz，结果还是报错\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 ❯ ./hello Traceback (most recent call last): File \u0026#34;/private/tmp/shiv3/./hello/_bootstrap/__init__.py\u0026#34;, line 76, in import_string File \u0026#34;/Users/qingchun3/.shiv/hello_e90513a0cebd72cf33c783fbb6faeb3783e9e41fd501901ce8b27f69a16aef65/site-packages/hello.py\u0026#34;, line 1, in \u0026lt;module\u0026gt; import simplejson as json ModuleNotFoundError: No module named \u0026#39;simplejson\u0026#39; During handling of the above exception, another exception occurred: Traceback (most recent call last): File \u0026#34;\u0026lt;frozen runpy\u0026gt;\u0026#34;, line 198, in _run_module_as_main File \u0026#34;\u0026lt;frozen runpy\u0026gt;\u0026#34;, line 88, in _run_code File \u0026#34;/private/tmp/shiv3/./hello/__main__.py\u0026#34;, line 3, in \u0026lt;module\u0026gt; File \u0026#34;/private/tmp/shiv3/./hello/_bootstrap/__init__.py\u0026#34;, line 253, in bootstrap File \u0026#34;/private/tmp/shiv3/./hello/_bootstrap/__init__.py\u0026#34;, line 81, in import_string File \u0026#34;/private/tmp/shiv3/./hello/_bootstrap/__init__.py\u0026#34;, line 59, in import_string File \u0026#34;/opt/homebrew/Cellar/python@3.12/3.12.4/Frameworks/Python.framework/Versions/3.12/lib/python3.12/importlib/__init__.py\u0026#34;, line 90, in import_module return _bootstrap._gcd_import(name[level:], package, level) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \u0026#34;\u0026lt;frozen importlib._bootstrap\u0026gt;\u0026#34;, line 1387, in _gcd_import File \u0026#34;\u0026lt;frozen importlib._bootstrap\u0026gt;\u0026#34;, line 1360, in _find_and_load File \u0026#34;\u0026lt;frozen importlib._bootstrap\u0026gt;\u0026#34;, line 1331, in _find_and_load_unlocked File \u0026#34;\u0026lt;frozen importlib._bootstrap\u0026gt;\u0026#34;, line 935, in _load_unlocked File \u0026#34;\u0026lt;frozen importlib._bootstrap_external\u0026gt;\u0026#34;, line 995, in exec_module File \u0026#34;\u0026lt;frozen importlib._bootstrap\u0026gt;\u0026#34;, line 488, in _call_with_frames_removed File \u0026#34;/Users/qingchun3/.shiv/hello_e90513a0cebd72cf33c783fbb6faeb3783e9e41fd501901ce8b27f69a16aef65/site-packages/hello.py\u0026#34;, line 1, in \u0026lt;module\u0026gt; import simplejson as json ModuleNotFoundError: No module named \u0026#39;simplejson\u0026#39; 唉？不是说好的会把依赖也打包进来吗？为什么没有？原来是因为没有添加\n1 -r requirements.txt 选项，那么这个文件从哪里来呢？如果你已经通过pip安装了所有依赖，通过执行 pip freeze \u0026gt; requirements.txt就可以了。\n完整命令就是\n1 shiv -c hello -o hello.pyz -r requirements.txt . 再测试就通过了。\n# 总结 其实到最后我是很失望的，因为一开始并没有完全搞清楚shiv到底解决了什么问题，我想做的是完全的可移植性，在一台机器上将环境配置好之后拿到同平台的其他机器上直接可以执行的那种，但明显shiv并不符合我的要求，这时候我突然就发现了Java和Golang的厉害之处了，Java可以一个jar包走天下，只要有JRE就能运行，而Golang更厉害了，甚至什么都不需要，直接就是一个包含所有依赖的二进制文件。\n但是，问题还是要解决，再问问GPT，继续踏上征程吧。\n","date":"2024-07-08T08:49:34+08:00","image":"https://blog.happyhacker.fun/images/covers/python-shiv.png","permalink":"https://blog.happyhacker.fun/p/python-shiv%E7%AE%80%E6%98%8E%E6%95%99%E7%A8%8B/","title":"Python shiv简明教程"},{"content":"仅半年在B站也发了不少视频，想着在博客上也能引用一下，但国内的视频网站嘛，海外的产品支持可能没有那么好，加上也想看看Hugo的shortcode到底是怎么工作的，所以就有了这篇文章。\n# 找到要引用的视频地址 这并不是直接点开视频的播放地址，而是要使用嵌入地址，俗称落地页。在B站是这样的\n具体的内容是这样\n1 \u0026lt;iframe src=\u0026#34;//player.bilibili.com/player.html?aid=1302993488\u0026amp;bvid=BV1DM4m1Q71a\u0026amp;cid=1499653065\u0026amp;p=1\u0026#34; scrolling=\u0026#34;no\u0026#34; border=\u0026#34;0\u0026#34; frameborder=\u0026#34;no\u0026#34; framespacing=\u0026#34;0\u0026#34; allowfullscreen=\u0026#34;true\u0026#34;\u0026gt; \u0026lt;/iframe\u0026gt; 就是一段HTML片段。\n# 优化引用地址 无意间搜到这篇文章Embed Bilibili Video To HTML，博主指出B站的这个片段不是响应式的，对移动端的支持不好，提供了一种新的方式\n1 2 3 \u0026lt;div style=\u0026#34;position: relative; padding: 30% 45%;\u0026#34;\u0026gt; \u0026lt;iframe style=\u0026#34;position: absolute; width: 100%; height: 100%; left: 0; top: 0;\u0026#34; src=\u0026#34;//player.bilibili.com/player.html?aid=1302993488\u0026amp;bvid=BV1DM4m1Q71a\u0026amp;cid=1499653065\u0026amp;p=1\u0026#34; frameborder=\u0026#34;no\u0026#34; scrolling=\u0026#34;no\u0026#34; allowfullscreen=\u0026#34;true\u0026#34;\u0026gt;\u0026lt;/iframe\u0026gt; \u0026lt;/div\u0026gt; 简单对比一下就能发现其实是在外层嵌套了一些支持Responsive的样式，妙！\n# Hugo的shortcode 没有仔细研读，看了一个例子基本就理解了，几个基本的规则\n基于新版本的目录结构，应该放在layouts/shortcodes/name-of-short-code.html中，其中name-of-short-code就是在文章中插入的名字，比如这里新建的文件名叫bilibili，那么就应该放在layouts/shortcodes/bilibili.html，而且具体的shortcode应该这么写{{\u0026lt; bilibili \u0026gt;}} shortcode中除了写名字之外还有一些其他东西，这里就涉及到模板引擎了，写在shortcode名字后面的东西，可以通过{{ .Get 0 }}这种语法来获取，比如这个例子就是获取第一个参数 根据上面的例子，具体的shortcode内容是这样的\n1 2 3 \u0026lt;div style=\u0026#34;position: relative; padding: 30% 45%;\u0026#34;\u0026gt; \u0026lt;iframe style=\u0026#34;position: absolute; width: 100%; height: 100%; left: 0; top: 0;\u0026#34; src=\u0026#34;{{ .Get 0 }}\u0026#34; frameborder=\u0026#34;no\u0026#34; scrolling=\u0026#34;no\u0026#34; allowfullscreen=\u0026#34;true\u0026#34;\u0026gt;\u0026lt;/iframe\u0026gt; \u0026lt;/div\u0026gt; 在需要引用它的地方这么写\n1 {{\u0026lt; bilibili \u0026#34;//player.bilibili.com/player.html?aid=1302993488\u0026amp;bvid=BV1DM4m1Q71a\u0026amp;cid=1499653065\u0026amp;p=1\u0026#34; \u0026gt;}} 搞定收工！如果要引用别的视频网站，可以根据需求写相应的shortcode。\n","date":"2024-04-16T21:50:49+08:00","image":"https://blog.happyhacker.fun/images/covers/embed-video-in-hugo.png","permalink":"https://blog.happyhacker.fun/p/%E5%9C%A8hugo%E4%B8%AD%E5%B5%8C%E5%85%A5%E8%A7%86%E9%A2%91/","title":"在Hugo中嵌入视频"},{"content":" # 背景 有一个很优秀的C语言实现的AC自动机代码，但它不是线程安全的，为什么这么说呢，我们来看下代码。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 typedef struct ac_trie { struct act_node *root; /**\u0026lt; The root node of the trie */ size_t patterns_count; /**\u0026lt; Total patterns in the trie */ short trie_open; /**\u0026lt; This flag indicates that if trie is finalized * or not. After finalizing the trie you can not * add pattern to trie anymore. */ struct mpool *mp; /**\u0026lt; Memory pool */ /* ******************* Thread specific part ******************** */ /* It is possible to search a long input chunk by chunk. In order to * connect these chunks and make a continuous view of the input, we need * the following variables. */ struct act_node *last_node; /**\u0026lt; Last node we stopped at */ size_t base_position; /**\u0026lt; Represents the position of the current chunk, * related to whole input text */ AC_TEXT_t *text; /**\u0026lt; A helper variable to hold the input chunk */ size_t position; /**\u0026lt; A helper variable to hold the relative current * position in the given text */ MF_REPLACEMENT_DATA_t repdata; /**\u0026lt; Replacement data structure */ ACT_WORKING_MODE_t wm; /**\u0026lt; Working mode */ } AC_TRIE_t; 这是整个自动机的定义，其实作者也意识到了那几个变量是线程相关的，但他并没有选择实现线程安全，因为这个项目是为一个命令行程序服务的，而这个命令行程序显然是没有机会处理多线程场景的。\n那为什么有这几个线程相关的变量就无法实现线程安全呢？再看下面的代码\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 /** * @brief Search in the input text using the given trie. * * @param thiz pointer to the trie * @param text input text to be searched * @param keep indicated that if the input text the successive chunk of the * previous given text or not * @param callback when a match occurs this function will be called. The * call-back function in turn after doing its job, will return an integer * value, 0 means continue search, and non-0 value means stop search and return * to the caller. * @param user this parameter will be send to the call-back function * * @return * -1: failed; trie is not finalized * 0: success; input text was searched to the end * 1: success; input text was searched partially. (callback broke the loop) *****************************************************************************/ int ac_trie_search (AC_TRIE_t *thiz, AC_TEXT_t *text, int keep, AC_MATCH_CALBACK_f callback, void *user) { size_t position; ACT_NODE_t *current; ACT_NODE_t *next; AC_MATCH_t match; if (thiz-\u0026gt;trie_open) return -1; /* Trie must be finalized first. */ if (thiz-\u0026gt;wm == AC_WORKING_MODE_FINDNEXT) position = thiz-\u0026gt;position; else position = 0; current = thiz-\u0026gt;last_node; if (!keep) ac_trie_reset (thiz); /* This is the main search loop. * It must be kept as lightweight as possible. */ while (position \u0026lt; text-\u0026gt;length) { if (!(next = node_find_next_bs (current, text-\u0026gt;astring[position]))) { if(current-\u0026gt;failure_node /* We are not in the root node */) current = current-\u0026gt;failure_node; else position++; } else { current = next; position++; } if (current-\u0026gt;final \u0026amp;\u0026amp; next) /* We check \u0026#39;next\u0026#39; to find out if we have come here after a alphabet * transition or due to a fail transition. in second case we should not * report match, because it has already been reported */ { /* Found a match! */ match.position = position + thiz-\u0026gt;base_position; match.size = current-\u0026gt;matched_size; match.patterns = current-\u0026gt;matched; /* Do call-back */ if (callback(\u0026amp;match, user)) { if (thiz-\u0026gt;wm == AC_WORKING_MODE_FINDNEXT) { thiz-\u0026gt;position = position; thiz-\u0026gt;last_node = current; } return 1; } } } /* Save status variables */ thiz-\u0026gt;last_node = current; thiz-\u0026gt;base_position += position; return 0; } 可以看到thiz就是正在使用的AC自动机的实例，但在查找过程中它改变了它的相关属性，这在多线程环境中肯定是会有冲突的，那么怎么解决呢？\n# 方案 设想这样一个简单的场景，我们在处理链表相关的算法题时，最常做的事情是什么呢？对，是建立一个dummyHead，仔细想想为什么要这么做。\n是的，根本目的是不希望我们遍历完成之后改变原来的链表，其实就是保持链表的不变性。\n这里也是一样的道理，既然不能改变Trie里的这些属性，那么就把它提出来，最终让它们成为线程内部的变量，这样就做到了线程之间的隔离，每个线程只处理自己的查找，把Trie变成不可变的。\n# 实现一个不可变的Trie 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 /* * The A.C. Trie data structure */ typedef struct ac_trie { struct act_node *root; /**\u0026lt; The root node of the trie */ size_t patterns_count; /**\u0026lt; Total patterns in the trie */ short trie_open; /**\u0026lt; This flag indicates that if trie is finalized * or not. After finalizing the trie you can not * add pattern to trie anymore. */ struct mpool *mp; /**\u0026lt; Memory pool */ MF_REPLACEMENT_DATA_t repdata; /**\u0026lt; Replacement data structure */ ACT_WORKING_MODE_t wm; /**\u0026lt; Working mode */ } AC_TRIE_t; MF_REPLACEMENT_DATA_t repdata; 这个很可疑，但它是用来实现替换功能的，我暂时没有这个需求，所以先不动它。\n但整个搜索过程还是需要那些变量的，我们把它放在一个新定义的结构体中。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 typedef struct ac_search { /* ******************* Thread specific part ******************** */ /* It is possible to search a long input chunk by chunk. In order to * connect these chunks and make a continuous view of the input, we need * the following variables. */ struct act_node *last_node; /**\u0026lt; Last node we stopped at */ size_t base_position; /**\u0026lt; Represents the position of the current chunk, * related to whole input text */ AC_TEXT_t *text; /**\u0026lt; A helper variable to hold the input chunk */ size_t position; /**\u0026lt; A helper variable to hold the relative current * position in the given text */ } AC_SEARCH_t; 相应地，也需要改查找方法的实现\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 int ac_trie_search(AC_TRIE_t *thiz, AC_SEARCH_t *search, int keep, AC_MATCH_CALBACK_f callback, void *user) { size_t position; ACT_NODE_t *current; ACT_NODE_t *next; AC_MATCH_t match; if (thiz-\u0026gt;trie_open) return -1; /* Trie must be finalized first. */ if (thiz-\u0026gt;wm == AC_WORKING_MODE_FINDNEXT) position = search-\u0026gt;position; else position = 0; current = search-\u0026gt;last_node; if (!keep) ac_trie_reset(thiz); /* This is the main search loop. * It must be kept as lightweight as possible. */ while (position \u0026lt; search-\u0026gt;text-\u0026gt;length) { if (!(next = node_find_next_bs(current, search-\u0026gt;text-\u0026gt;astring[position]))) { if (current-\u0026gt;failure_node /* We are not in the root node */) current = current-\u0026gt;failure_node; else position++; } else { current = next; position++; } if (current-\u0026gt;final \u0026amp;\u0026amp; next) /* We check \u0026#39;next\u0026#39; to find out if we have come here after a alphabet * transition or due to a fail transition. in second case we should not * report match, because it has already been reported */ { /* Found a match! */ match.position = position + search-\u0026gt;base_position; match.size = current-\u0026gt;matched_size; match.patterns = current-\u0026gt;matched; /* Do call-back */ if (callback(\u0026amp;match, user)) { if (thiz-\u0026gt;wm == AC_WORKING_MODE_FINDNEXT) { search-\u0026gt;position = position; search-\u0026gt;last_node = current; } return 1; } } } /* Save status variables */ search-\u0026gt;last_node = current; search-\u0026gt;base_position += position; return 0; } 把其中原先是thiz的地方改成search。 接下来就需要考虑怎么初始化AC_SEARCH_t。\n1 2 3 4 5 6 7 8 9 10 AC_SEARCH_t *ac_search_create(void) { AC_SEARCH_t *search = (AC_SEARCH_t *)malloc(sizeof(AC_SEARCH_t)); search-\u0026gt;text = NULL; search-\u0026gt;position = 0; search-\u0026gt;last_node = NULL; search-\u0026gt;base_position = 0; return search; } 这是一个完全0值的初始化过程，事实上我们得先考虑把text初始化，因为要知道查的文本是什么。\n1 2 3 4 5 AC_SEARCH_t *search = ac_search_create(); AC_TEXT_t chunk; chunk.astring = \u0026#34;experience the ease and simplicity of multifast\u0026#34;; chunk.length = strlen(chunk.astring); search-\u0026gt;text = \u0026amp;chunk; 这时执行会发现报段错误，具体错误就不展示了，原因是在node_find_next_bs方法中，有这样一段代码\n1 max = nod-\u0026gt;outgoing_size - 1; 而nod是在ac_trie_search方法中的current，如果传入的search-\u0026gt;last_node是空值，这里就直接一个空指针异常了。\n检查Trie的初始化代码发现，每次开始查询之前都需要重置一下状态（其实就是我们这里处理的这些变量）\n1 2 3 4 5 6 7 8 9 10 11 /** * @brief reset the trie and make it ready for doing new search * * @param thiz pointer to the trie *****************************************************************************/ static void ac_trie_reset(AC_TRIE_t *thiz) { thiz-\u0026gt;last_node = thiz-\u0026gt;root; thiz-\u0026gt;base_position = 0; mf_repdata_reset(\u0026amp;thiz-\u0026gt;repdata); } 重点就在这里了，需要先把search-\u0026gt;last_node = trie-\u0026gt;root，这样才能开始。\n# 最终效果 1 2 3 4 5 6 AC_SEARCH_t *search = ac_search_create(); AC_TEXT_t chunk; chunk.astring = chunks[i]; chunk.length = strlen(chunk.astring); search-\u0026gt;text = \u0026amp;chunk; search-\u0026gt;last_node = trie-\u0026gt;root; 这就是一个完整的AC_SEARCH_t的初始化过程了。\n# 多线程测试 这里只展示关键部分代码\n首先定义一个结构体来向线程传递参数。\n1 2 3 4 5 typedef struct { AC_TRIE_t *automata; AC_SEARCH_t *search; } ThreadParams; 创建多个线程并等待执行结束\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 pthread_t threads[3]; for (i = 0; i \u0026lt; 3; i++) { ThreadParams *threadParams = (ThreadParams *)malloc(sizeof(ThreadParams)); threadParams-\u0026gt;automata = trie; AC_SEARCH_t *search = ac_search_create(); AC_TEXT_t chunk; chunk.astring = chunks[i]; chunk.length = strlen(chunk.astring); search-\u0026gt;text = \u0026amp;chunk; search-\u0026gt;last_node = trie-\u0026gt;root; threadParams-\u0026gt;search = search; pthread_create(\u0026amp;threads[i], NULL, chlid_handler, threadParams); } for (i = 0; i \u0026lt; 3; i++) { pthread_join(threads[i], NULL); } 定义线程内的回调方法，把传进来的参数解出来，调用查找方法\n1 2 3 4 5 6 7 8 9 void *chlid_handler(void *arg) { ThreadParams *params = (ThreadParams *)arg; AC_MATCH_t match; printf(\u0026#34;Searching: \\\u0026#34;%s\\\u0026#34; in thread: %ld\\n\u0026#34;, params-\u0026gt;search-\u0026gt;text-\u0026gt;astring, (unsigned long int)pthread_self()); ac_trie_search(params-\u0026gt;automata, params-\u0026gt;search, 0, match_handler, 0); return NULL; } 大功告成。\n","date":"2024-04-10T23:47:17+08:00","image":"https://blog.happyhacker.fun/images/covers/a-threadsafe-ac-automaton-with-c.png","permalink":"https://blog.happyhacker.fun/p/%E4%BD%BF%E7%94%A8c%E8%AF%AD%E8%A8%80%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%BA%BF%E7%A8%8B%E5%AE%89%E5%85%A8%E7%9A%84ac%E8%87%AA%E5%8A%A8%E6%9C%BA/","title":"使用C语言实现一个线程安全的AC自动机"},{"content":"记录一下快速配置virtualenv。\n# 1. 安装virtualenv # macOS 1 brew install virtualenv # Linux(以Debian为例) 1 sudo apt install virtualenv # 2. 指定版本 1 virtualenv venv --python=3.11 这时会自动在 venv下复制一份全新的3.11的python环境\n注意：这种写法只能使用系统已经安装的Python版本，否则需要指定Python的安装路径\n# 3. 让指定的版本生效 1 source venv/bin/activate 这时命令行提示符前会有一个 (venv)的前缀，如下，这时在这里执行 pip install就不会影响到别的环境。\n1 (venv) frost@debian:~/workspace/ # 4. 不再使用这个Python环境 1 deactivate ","date":"2024-04-10T00:28:44+08:00","image":"https://blog.happyhacker.fun/images/covers/python-virtualenv.png","permalink":"https://blog.happyhacker.fun/p/virtualenv%E5%BF%AB%E9%80%9F%E8%AE%BE%E7%BD%AE/","title":"Virtualenv快速设置"},{"content":"PHP 调用 http 服务此前一直都是封装的 curl，这事儿我也干过不少次了，不过今天碰到了一个新问题。\n接口是 clickhouse 服务，查询的响应体比较大，放在内存里要超过 1 G，再继续修改 max_memory_limit 意义不大，所以就希望把结果直接放入文件，然后逐行读取处理，以减少整个过程的内存消耗，那么怎么把结果写入文件呢？\n相关的选项是这个\n1 curl_setopt($ch, CURLOPT_RETURNTRANSFER, 1); 这个选项设置为 1/true 时，就会把结果放在 $response = curl_exec($ch) 里的这个 $response 里，这也是现在这个问题的来源，$response 太大了。但如果把它设置为 0/false，它会写到标准输出，又没办法用代码处理。\n参考了这个问题，给出了两个思路。\n# 直接写入文件 1 2 3 $fh = fopen($file, \u0026#39;w\u0026#39;); curl_setopt($ch, CURLOPT_RETURNTRANSFER, 0); curl_setopt($ch, CURLOPT_FILE, $fh); 一定要注意的是，如果 $file 对应的目录不存在，则无法自动创建目录，整个过程会失败。\n# 写回调方法 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 # setup a global file pointer $GlobalFileHandle = null; function saveRemoteFile($url, $filename) { global $GlobalFileHandle; set_time_limit(0); # Open the file for writing... $GlobalFileHandle = fopen($filename, \u0026#39;w+\u0026#39;); $ch = curl_init(); curl_setopt($ch, CURLOPT_URL, $url); curl_setopt($ch, CURLOPT_FILE, $GlobalFileHandle); curl_setopt($ch, CURLOPT_RETURNTRANSFER, false); # Assign a callback function to the CURL Write-Function curl_setopt($ch, CURLOPT_WRITEFUNCTION, \u0026#39;curlWriteFile\u0026#39;); # Exceute the download - note we DO NOT put the result into a variable! curl_exec($ch); # Close CURL curl_close($ch); # Close the file pointer fclose($GlobalFileHandle); } function curlWriteFile($cp, $data) { global $GlobalFileHandle; $len = fwrite($GlobalFileHandle, $data); return $len; } 这个看起来有点多此一举了，因为不指定这个回调函数也会写入这个方法，只是无法对写入的方式做一些精细控制，这个回调函数多数时候是不需要的，因为我更倾向于先整体写入文件，上层的方法再根据业务需求去处理，而不是在这里决定。\n","date":"2024-04-09T17:46:03+08:00","image":"https://blog.happyhacker.fun/images/covers/curl-with-large-response-oom-fix.png","permalink":"https://blog.happyhacker.fun/p/php%E8%A7%A3%E5%86%B3curl%E5%93%8D%E5%BA%94%E8%BF%87%E5%A4%A7%E6%97%B6%E7%88%86%E5%86%85%E5%AD%98%E7%9A%84%E9%97%AE%E9%A2%98/","title":"PHP解决curl响应过大时爆内存的问题"},{"content":" # 背景 一直以来，尤其是自从使用Apple Silicon的芯片的MacBook Pro以来，在macOS上使用Linux虚拟机就变得很困难了，加上我家里用的是Ryzentosh，两台电脑都很难使用虚拟机，就搞得很头疼。尤其最近需要调试那个C语言的程序，依赖一些libevent、inotify这类东西，不想把宿主机搞得太乱，所以选择一个虚拟机管理器的事情又得重新考虑了。\nIntel芯片的macOS还是有很多选择的，Parallels Desktop就不说了，唯一的缺点就是贵，只说免费的方案。\nVirtualBox肯定是首选，但在Apple Silicon芯片发布后5年后的今天（2024年4月），支持苹果M系列芯片的版本仍然没有发布，甚至早前公布的虽然不可用的预览版也早已下架了，搞不好Oracle内部早已经放弃了。\n废话不多说，直接介绍今天的主角吧，Lima——Linux Machine，名字简单直接，其实我第一次看到类似的名字是Colima，是在apppleboy64大佬的博客上看到的，是一个本地管理K3s的工具，后来顺着了解到了Lima，可以理解为为了在macOS上运行Containerd而需要先启动一个Linux虚拟机，那么这个Lima就是为了运行这个虚拟机的。但我们现在要的就是这个虚拟机，至于后面是否要用K3s/K8s，不是现在考虑的事情。\n其实所有这些东西背后都是QEMU，无非是使用哪个前端了。我知道的还有一个libvirt，但配置相对很复杂，而我只需要一个能简单run起来的虚拟机而已，所以使用如此简单的Lima几乎是一个完美的选择。\n# 快速入门 # 安装 前面说了，我们是要在macOS平台上使用，那么肯定优先选择用homebrew安装了。\n1 brew install lima 可以看到，会自动安装QEMU依赖。\n# 配置虚拟机 1 limactl start 这时会弹出几个选项，不要着急选，先看下自己的需求\n1 2 3 4 5 6 7 8 $ limactl start ? Creating an instance \u0026#34;default\u0026#34; [Use arrows to move, type to filter] \u0026gt; Proceed with the current configuration Open an editor to review or modify the current configuration Choose another template (docker, podman, archlinux, fedora, ...) Exit ... INFO[0029] READY. Run `lima` to open the shell. 默认用的是Ubuntu 23.10，我不喜欢用Ubuntu，所以会先选择 Choose another template (docker, podman, archlinux, fedora, ...)来重新配置，比如我选择了Debian 12，配置如下\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 # This template requires Lima v0.7.0 or later images: # Try to use release-yyyyMMdd image if available. Note that release-yyyyMMdd will be removed after several months. - location: \u0026#34;https://cloud.debian.org/images/cloud/bookworm/20240211-1654/debian-12-genericcloud-amd64-20240211-1654.qcow2\u0026#34; arch: \u0026#34;x86_64\u0026#34; digest: \u0026#34;sha512:6856277491c234fa1bc6f250cbd9f0d44f77524479536ecbc0ac536bc07e76322ebb4d42e09605056d6d3879c8eb87db40690a2b5dfe57cb19b0c673fc4c58ca\u0026#34; - location: \u0026#34;https://cloud.debian.org/images/cloud/bookworm/20240211-1654/debian-12-genericcloud-arm64-20240211-1654.qcow2\u0026#34; arch: \u0026#34;aarch64\u0026#34; digest: \u0026#34;sha512:c8f3746aa979cdc95c13cd4b8cc032151f1e5685525a85c2b3b2e30defa02dacb1058b68f955ac16f3f2dbd473d13dfef15d2a22f348bcc4abb427e0713fa9a4\u0026#34; # Fallback to the latest release image. # Hint: run `limactl prune` to invalidate the cache - location: \u0026#34;https://cloud.debian.org/images/cloud/bookworm/latest/debian-12-genericcloud-amd64.qcow2\u0026#34; arch: \u0026#34;x86_64\u0026#34; - location: \u0026#34;https://cloud.debian.org/images/cloud/bookworm/latest/debian-12-genericcloud-arm64.qcow2\u0026#34; arch: \u0026#34;aarch64\u0026#34; mounts: - location: \u0026#34;~\u0026#34; - location: \u0026#34;/tmp/lima\u0026#34; writable: true 绝大多数情况下，前面images相关的配置我们完全不用关心，只需要知道它能虚拟出来一个完整的虚拟机就行了，最重要的是mounts选项，不得不说，yaml配置真是很难读\n1 2 3 4 mounts: - location: \u0026#34;~\u0026#34; - location: \u0026#34;/tmp/lima\u0026#34; writable: true 这个配置的意思是它会把~也就是宿主机的家目录挂载到虚拟机，让我们可以在虚拟机内访问到宿主机家目录中的问题，但默认是只读的，不要以为这里有writable: true就不用管了，其实这个writable是给/tmp/lima用的，所以需要简单修改一下，改成\n1 2 3 4 5 mounts: - location: \u0026#34;~\u0026#34; writable: true - location: \u0026#34;/tmp/lima\u0026#34; writable: true 就可以保存了，一直下一步就可以了。这里如果不用魔法可能会非常慢，具体这里就不说了，自己想办法解决。\n# 访问虚拟机 这就是为什么我说Lima几乎完美的原因了，它是真的知道开发人员需要什么，跟我大声说——SSH！所以默认启动之后就配置好了ssh访问，可以执行lima命令进入虚拟机。\n注意：这是因为前面我们没有指定虚拟机的名字，所以默认是default，所以执行lima也就不需要指定名字了。如果你想启动多个虚拟机，相应的命令可以替换成limactl start debian、lima debian这样。\n这时候更有意思的来了，进虚拟机之后它会把你当前在宿主机的位置带进虚拟机，如果你不小心去了别的目录，还想回到宿主机的家目录，只需要cd /Users/your_name/path/to/your/location/就行了，别提多贴心了。\n现在你可以开心地使用虚拟机了，是不是很有WSL的味道？\n# One More Thing 考虑以下两种情况\n在Apple Silicon的芯片上运行X86_64的Linux虚拟机 在Ryzontosh上运行虚拟机 QEMU表示：我太难了！！！\n但还是可以的，第一种情况你在前面编辑配置的时候可能已经看到了，配置文件中有几项带默认值的，这里简单列一下\nkey value vmType 默认QEMU就行了，vz性能更差 | os | 也不用动，你既然找它肯定是要用Linux | | arch |关键就是这里了，Apple Silicon的机器默认肯定都选的是aarch64，但如果确实需要X86的虚拟机，可以改成 x86_64\n第二种情况，需要手动给QEMU指定运行参数，把加上 QEMU_SYSTEM_X86_64=\u0026quot;QEMU-system-x86_64 -cpu max -machine q35\u0026quot;环境变量再执行limactl start就可以了，但运行速度会比较慢就是了。\n那么好奇的小朋友就会问了，这是为什么呢？\n先看下GPT对于QEMU和KVM的区别的说明\nQEMU（Quick Emulator）和 KVM（Kernel-based Virtual Machine）之间的关系是相互补充的，它们一起在Linux上提供了一种高效的虚拟化解决方案。 QEMU是一个通用的开源机器模拟器和虚拟器。它可以执行硬件虚拟化，使您能够运行一个操作系统的完整副本（称为客户机或虚拟机）在另一个操作系统上。QEMU可以在用户模式下运行，提供软件仿真虚拟化，但这通常会导致较慢的性能。 KVM是Linux内核的一部分，它允许Linux将自身转换为一个类型1（裸金属）的虚拟机监控器。KVM需要处理器支持硬件虚拟化扩展（Intel的VT-x或AMD的AMD-V）。当KVM用于QEMU时，它提供硬件辅助虚拟化，显著提高虚拟机的性能，特别是对于CPU密集型应用程序。 KVM本身不执行任何模拟，它依赖于用户空间程序（如QEMU）来设置虚拟机的环境、虚拟硬件等。QEMU在使用KVM时，负责设备模拟，而CPU密集型任务则由KVM在硬件虚拟化扩展的帮助下运行，这样可以近乎本地速度执行虚拟机。\n简单来说，QEMU用于模拟硬件，而KVM让QEMU利用CPU扩展来提供更快的虚拟化性能。如果无法使用KVM（例如在不支持硬件虚拟化的平台上），QEMU仍然可以独立工作，只是性能会有所下降。\n可以理解为Hypervisor.Framework就是macOS平台上的KVM，而在Ryzentosh上，无法访问Hypervisor.Framework，所以就只能完全依赖QEMU自己模拟和计算了，经过一层转化自然就慢了。\n","date":"2024-04-05T23:11:59+08:00","image":"https://blog.happyhacker.fun/images/covers/Lima-for-linux-virtual-machine-on-macOS.png","permalink":"https://blog.happyhacker.fun/p/%E4%BD%BF%E7%94%A8lima%E7%AE%A1%E7%90%86macos%E4%B8%8A%E7%9A%84linux%E8%99%9A%E6%8B%9F%E6%9C%BA/","title":"使用Lima管理macOS上的Linux虚拟机"},{"content":"我们知道，普通的硬盘格式化只是把区块标记为“可写”，但并没有擦除上面的内容，这也是部分损坏的硬盘可以通过外部方式读取的原因。但有时如果需要彻底让别人无法读取到硬盘存储过的内容——比如借别人的硬盘中转一下数据，你肯定不想造成信息泄露吧。\n# 三种清除数据的方式 # 普通/快速/高级格式化 这是我们普通人最常用的格式化的方式，它会删除文件系统的元数据，并在硬盘上创建一个新的文件系统，使硬盘准备好存储数据。但需要注意，这只是删除了文件系统中的元数据，而没有擦除硬盘上的数据，所以可以使用特殊软件恢复，如果是自己的硬盘，用这种方式就足够了。\n# 低级格式化 这是相对高级格式化而言的，知道有这么个东西就行，普通人没办法执行这个操作，通常是硬盘制造商做的事情，用于定义硬盘的物理结构，如磁道和扇区等。当然它也会彻底删除硬盘中的数据，但现代硬盘通常不给用户提供这种选项，所以我们也没办法操作。\n# 0填充 这是在高级格式化的基础上，用0填满硬盘，这样就没办法通过简单的恢复软件来恢复数据了，比较适合我说的借用别人硬盘存个人数据，在还硬盘之前的操作。\n# Linux/Unix(macOS) 上还是比较方便操作的 1 dd if=/dev/zero of=/dev/sdX bs=1M 在这条命令中：\nif 表示输入文件（input file），这里是/dev/zero（一个特殊的文件，提供无限的零值）。 of 表示输出文件（output file），这里是/dev/sdX（要被写入的硬盘，X是硬盘标识符）。 bs 表示块大小（block size），在这里设置为每次写入1兆字节。 不过这相当于把整个硬盘写满，需要的时间还是比较长的。\n# Windows上选择就更多了 format命令 使用命令行工具format可以进行格式化操作。在命令提示符下，使用/p参数可以指定要在格式化期间执行的零填充遍数，例如：\n1 format X: /fs:NTFS /p:1 这将格式化X:驱动器，使用NTFS文件系统，并对所有可用空间进行一次零填充。\ncipher命令 虽然cipher命令主要用于管理文件加密，但它也能用来清除未使用的磁盘空间。使用/w选项，你可以擦除可用空间（未分配的磁盘空间），如下所示：\n1 cipher /w:X: 这会在X:驱动器上清除未使用的空间，首先填充为零，然后是一些随机数据。\n第三方软件 存在许多第三方软件，如CCleaner、DBAN（Darik\u0026rsquo;s Boot and Nuke）、Eraser等，它们提供数据擦除功能。这些工具通常提供了多种数据擦除标准，包括写零、DoD 5220.22-M标准等。\nWindows PowerShell 你可以编写一个简单的PowerShell脚本来模拟dd命令的功能。以下是一个基本的例子，它使用System.IO命名空间中的类来填充一个驱动器的内容为零：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 $driveLetter = \u0026#34;X\u0026#34; $file = \u0026#34;$($driveLetter):\\zero.bin\u0026#34; $fileStream = [System.IO.File]::Create($file) $buffer = New-Object byte[] 10MB $totalSize = 500GB $written = 0 while ($written -lt $totalSize) { $fileStream.Write($buffer, 0, $buffer.Length) $written += $buffer.Length } $fileStream.Close() Remove-Item $file ","date":"2024-04-04T22:52:24+08:00","image":"https://blog.happyhacker.fun/images/covers/low-level-format-hard-disk.png","permalink":"https://blog.happyhacker.fun/p/%E5%AE%8C%E5%85%A8%E6%B8%85%E9%99%A4%E7%A1%AC%E7%9B%98%E7%9A%84%E6%95%B0%E6%8D%AE/","title":"完全清除硬盘的数据"},{"content":"使用CMake的小伙伴可能大部分都是在（类）Unix环境下，所以一般都是用Unix Makefile作为默认后端，我这两天在修改一个多年前的C项目，把它从一坨Automake、Autoconf中拯救出来，改成了CMake的形式，清爽多了。\n为什么这么说呢？是因为原先的方式就是把项目本身的代码放在根目录，然后几个依赖放在和自己的代码同一级的文件夹里，对于这几个依赖也是动态链接的方式，所以就需要先手动按顺序编译、安装这些依赖，然后再编译自己的代码。而改成CMake之后就变成静态依赖了，编译过程简化不少。\n但问题也来了，我发现每次编译都要花挺长时间，那么有没有办法加速呢？这时候我想到了在很多开源项目都会用到的Ninja，于是了解了下，发现像Chromium、Android（部分）都是用它编译的，主打一个增量编译，说白了就是只编译修改了的部分，它能够更智能地决定哪些部分需要重新构建，从而减少不必要的编译。\n# 使用方法 首先当然是安装了，在Debian上的名字叫ninja-build，其他系统可以参考这个名字。\n正常使用CMake是这样的：\n1 2 3 4 mkdir build cd build cmake .. cmake --build 而对比使用Ninja作为状态后端，流程是这样的\n1 2 3 4 mkdir build cd build cmake -G \u0026#34;Ninja\u0026#34; cmake --build 区别就在于加了-G \u0026quot;Ninja\u0026quot;，一定注意这里要大写，否则你可以试试。\n由于代码量比较少，所以比较下来区别不大，但公司的那个C项目，实际使用时还是有点明显的。\n查了一下Help，如下\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Generators The following generators are available on this platform (* marks default): Green Hills MULTI = Generates Green Hills MULTI files (experimental, work-in-progress). * Unix Makefiles = Generates standard UNIX makefiles. Ninja = Generates build.ninja files. Ninja Multi-Config = Generates build-\u0026lt;Config\u0026gt;.ninja files. Watcom WMake = Generates Watcom WMake makefiles. CodeBlocks - Ninja = Generates CodeBlocks project files. CodeBlocks - Unix Makefiles = Generates CodeBlocks project files. CodeLite - Ninja = Generates CodeLite project files. CodeLite - Unix Makefiles = Generates CodeLite project files. Eclipse CDT4 - Ninja = Generates Eclipse CDT 4.0 project files. Eclipse CDT4 - Unix Makefiles= Generates Eclipse CDT 4.0 project files. Kate - Ninja = Generates Kate project files. Kate - Unix Makefiles = Generates Kate project files. Sublime Text 2 - Ninja = Generates Sublime Text 2 project files. Sublime Text 2 - Unix Makefiles = Generates Sublime Text 2 project files. 所以-G其实就是指定generator，默认是Unix Makefiles。\n# 再进一步 前面mkdir build \u0026amp;\u0026amp; cd build这些步骤其实有点初级，可以改成这样的\n1 2 cmake -G \u0026#34;Ninja\u0026#34; -B build cmake --build build 第一行的意思是指定Ninja作为generator，指定build作为编译用的目录，如果build目录不存在就会自动创建。 第二行的意思是在build目录里执行编译过程。\n这就更适合自己写一个编译脚本来执行了。\n# 总结 Ninja的优势主要表现在以下几个方面：\n速度：Ninja被设计为比Make更快。Ninja的设计重点是实现高性能，这意味着它能够更快地开始编译过程并更高效地执行构建。 简化的构建文件：Ninja的构建文件通常比Makefiles更加简单和易于理解。这是因为CMake负责生成这些文件，而Ninja仅仅执行它们。 并行构建：虽然Make也支持并行构建（例如通过make -j 参数），但Ninja通常在并行构建方面更加高效和智能，它会自动推断出最优的任务数来使用所有可用的处理器核心。 更好的构建进度估计：Ninja提供了更准确的构建进度信息，这对于长时间的构建过程来说非常有用。 更少的重新构建：Ninja更智能地处理构建文件的生成，从而避免了一些不必要的重新构建，这可能发生在Makefiles中。 快速的无操作构建：当没有任何东西需要构建时（即所有目标都是最新的），Ninja可以更快地确定没有工作要做，并立即完成构建过程。 总的来说，虽然Make和Ninja都支持增量构建，但Ninja在执行构建任务时通常会更快，尤其是对于大型项目。这不仅节省了开发者的时间，也提高了构建系统的整体效率。对于有大量源文件的复杂项目，采用Ninja可能会显著减少构建的时间。\n","date":"2024-04-04T22:00:30+08:00","image":"https://blog.happyhacker.fun/images/covers/CMake-with-Ninja-cover.png","permalink":"https://blog.happyhacker.fun/p/%E4%BD%BF%E7%94%A8ninja%E4%BD%9C%E4%B8%BAcmake%E7%9A%84%E5%90%8E%E7%AB%AF/","title":"使用Ninja作为CMake的后端"},{"content":" # 什么是 CMake？它能干什么？ CMake 是一个开源的跨平台自动化构建系统，它使用平台和编译器独立的配置文件来生成标准的构建文件，这使得开发者可以使用统一的方法来管理项目的构建过程。CMake的配置文件通常命名为 CMakeLists.txt，在这个文件中，开发者可以定义项目的源码文件、依赖关系、目标（如可执行文件和库）、编译选项以及其他构建参数。\nCMake的主要优势在于它的跨平台能力：它可以生成适用于多种平台和IDE的构建配置，包括Unix的Makefiles、Microsoft Visual Studio的解决方案文件、Apple的Xcode项目等。这样，使用CMake的项目可以轻松地在不同的开发环境中编译和运行，无需修改构建配置。\n简单来说，CMake让复杂的构建过程变得简单化，提供一种高效、可扩展的方式来自动化构建过程，支持大型项目和多平台编译。\n# 一个最简单的 CMake 项目 # 只包含必要元素的 CMakeLists.txt 一个最简单的CMakeLists.txt文件通常包含以下几个基本元素：\ncmake_minimum_required: 指定运行此项目所需的最低版本的CMake。 project: 定义了项目的名称和可选的版本号。 add_executable 或 add_library: 添加一个可执行文件或者库到项目中，并指定它的源文件。 下面是一个最简单的示例：\n1 2 3 4 cmake_minimum_required(VERSION 3.0) # 设置CMake的最低版本要求 project(HelloWorld) # 定义项目名称 add_executable(HelloWorld main.c) # 添加一个可执行文件 \u0026#34;HelloWorld\u0026#34;，源文件是 \u0026#34;main.c\u0026#34; 这个 CMakeLists.txt 文件定义了一个简单的项目，其中包含一个可执行文件 HelloWorld，它是由单个源文件 main.c 编译而成的。当你运行CMake时，它会根据这个配置文件生成适合你系统的构建文件（比如Unix的Makefile或者Visual Studio的项目文件）。然后你可以使用相应的构建系统来编译和链接你的程序。\n# 最简单的 HelloWorld 1 2 3 4 5 6 #include \u0026lt;stdio.h\u0026gt; int main() { printf(\u0026#34;hello world!\\n\u0026#34;); return 0; } # 构建这个项目 # 创建 build 目录 在 CMakeLists.txt 所在的目录下，创建一个 build 目录。\n1 mkdir build \u0026amp;\u0026amp; cd build # 生成对应当前系统的构建文件 1 cmake .. [!QUESTION] 为什么要费这劲呢？ 你可能已经看到了，我们先创建了 build 目录，然后在它里面执行的是 cmake ..，也就是说 cmake 需要的文件在 build 的上层目录，那是不是直接在它的上层目录执行 cmake . 也是可以的呢？ 是的，但是这样做其实是个最佳实践，好处在于在必要时可以直接删除 build 目录，而不会对项目产生影响，否则 CMake 生成的文件散落在你的代码中，一定不是你想看到的结果。\n这个命令通常在一个空的构建目录（通常是项目根目录的子目录，如 build）中执行。这是一种被推荐的外部构建方法，可以避免在源代码目录中生成构建文件。. 表示当前目录，而 .. 表示当前目录的父目录，通常这个父目录包含顶层的 CMakeLists.txt 文件。\n执行 cmake .. 会：\n检查系统环境。 根据顶层 CMakeLists.txt 文件来确定如何编译项目的源代码。 生成对应于当前系统的构建文件（如Makefiles或者Visual Studio解决方案等）。 [!INFO] CMake 怎么知道要生成什么平台的文件呢？ CMake 确定使用哪种具体构建工具（如 Make 或 Visual Studio）的过程是在初次配置构建系统时发生的，也就是在你运行 cmake 命令来生成构建文件时。这个决定基于两个主要因素：\n可用的构建工具：CMake 会检测你的系统上安装了哪些构建工具。例如，如果你在 Windows 上且安装了 Visual Studio，CMake 默认会生成 Visual Studio 解决方案文件。如果你在 Linux 或 macOS 上，通常默认生成 Makefile。 用户指定的生成器：用户可以通过 -G 选项显式指定使用哪个生成器，也就是构建系统类型。例如，即使在 Windows 上，你也可以通过 -G \u0026quot;Unix Makefiles\u0026quot; 选项告诉 CMake 生成 Makefile，使用 Make 工具进行构建，而不是使用 Visual Studio。类似地，如果你想要使用其他类型的构建系统，如 Ninja，你也可以通过 -G \u0026quot;Ninja\u0026quot; 来指定。 当运行 cmake 命令没有指定 -G 选项时，CMake 会根据它检测到的环境和默认优先顺序选择一个生成器。一旦构建文件被生成，运行 cmake --build . 命令时，CMake 会使用相应的构建工具来编译项目，而不需要用户关心具体是使用 Make、Visual Studio 还是其他工具。这种抽象化的好处是，你可以用相同的命令在不同的平台和环境下构建你的项目。\n# 生成最终的可执行文件或库文件 1 cmake --build . 这个命令是用来实际编译和链接程序的。在你已经生成了构建系统文件后（例如，使用 cmake ..），可以使用此命令来启动构建过程。\n执行 cmake --build . 会：\n根据当前目录中的构建系统文件（Makefile、Visual Studio解决方案文件等）来编译源代码。 生成最终的可执行文件或库文件。 . 在这里也表示当前目录，它应该是包含了构建系统文件的目录。这条命令的优点是它抽象了具体的构建工具（make、ninja、msbuild等），使得构建过程与构建系统无关。\n执行完这一步，就生成了最终的可执行文件（或库文件，如果你写的是一个库的话）。在这个例子里可执行文件的文件名是 HelloWorld，因为 CMakeLists.txt 文件的第二行 project(HelloWorld) 里已经定义了。\n如果只是这么简单，就没必要使用 CMake 了，直接 gcc 都能满足了，所以下面我们来看如何用 CMake 来管理依赖。\n# 管理一个库项目 有时候我们需要写的项目是一个让其他项目依赖的库，就不能像上面生成一个可执行文件，而是生成一个 .a 或 .so 文件，下面是一个最简单的例子。完成这个例子之后我们会对两种情况做一个简单的对比。\n# 只包含必要元素的 CMakeLists.txt 1 2 3 4 5 6 7 8 cmake_minimum_required(VERSION 3.27) project(my_math) # 创建一个明为my_math的静态库 add_library(my_math include/my_math/my_math.h src/my_math.c) # 指定库的头文件搜索路径 target_include_directories(my_math PUBLIC ${CMAKE_CURRENT_SOURCE_DIR}/include) [!QUESTION] 什么是静态库？是不是还有动态库？ 在CMake中，当你使用 add_library 命令创建一个库时，SHARED 和 STATIC 关键字用来指定库是动态链接的还是静态链接的：\nSTATIC：创建一个静态库。静态库通常是一个包含多个对象文件的单一归档文件（在Windows上通常是 .lib 文件，在Unix-like系统上是 .a 文件）。当静态库被链接到一个可执行文件时，库中的代码会被复制到最终的可执行文件中。静态链接发生在编译时，一旦完成，可执行文件不再需要静态库文件。静态库的优点是最终的可执行文件是自包含的，不依赖外部的动态库。缺点是如果多个程序都使用同一个静态库，它们各自都会包含一份库代码的副本，这会导致冗余和更大的程序体积。\nSHARED：创建一个动态链接库（在Windows上是 .dll 文件，在Unix-like系统上是 .so 文件）。当动态库被链接到一个可执行文件时，并不是将代码复制到可执行文件中，而是在程序运行时由操作系统动态地加载和链接。这意味着程序在运行时需要能够找到这个动态库文件。动态库的优点是多个程序可以共享同一份库代码，节省空间，并可以在不重新编译程序的情况下更新库代码（只要接口没有改变）。缺点是可能会遇到库版本不匹配或找不到库文件的运行时错误。\n在实际应用中，这两种类型的库各有用处：\n如果你希望你的程序易于部署，不想处理动态库可能带来的复杂性，你可能会选择静态链接。这样你的程序可以在没有额外依赖的情况下运行。 如果你希望你的应用程序能够共享公共代码并且轻量级，或者希望能够独立于应用程序更新你的库，那么你可能会选择动态链接。 在 CMake 中，你可以根据需要选择使用 STATIC 或 SHARED 关键字来构建你的库，或者使用 add_library 命令不带任何关键字来创建类型取决于构建类型的库（默认情况下可能是静态的，但可以通过 CMake 变量来控制）。\n# 一个简单的库的实现 [!tip] include/my_math/my_math.h\n1 2 3 4 5 6 #ifndef MY_MATH_HEADER #define MY_MATH_HEADER int max(int, int); #endif // !MY_MATH_HEADER [!QUESTION] 为什么要有 #ifndef、#define、#endif 这些？ 简单讲就是为了避免重复引用。 先判断 #ifndef 也就是如果没有定义过这个 HEADER，那反过来讲如果定义过这个 HEADER 呢？是不是就执行后面所有的语句了？ 然后如果进入了，说明就是还没有引入过这个文件，就定义这个常量，这也帮助了第一步的判断 最后是 #endif，说明整个 #ifndef 结束了\n1 2 3 4 5 6 7 8 9 #include \u0026#34;my_math/my_math.h\u0026#34; int max(int a, int b) { if (a \u0026gt; b) { return a; } return b; } 这是整个项目的所有文件，读者可以对照一下\n1 2 3 4 5 6 7 8 ├── CMakeLists.txt ├── include │ └── my_math │ └── my_math.h └── src └── my_math.c 4 directories, 3 files # 构建这个项目 1 mkdir build \u0026amp;\u0026amp; cd build \u0026amp;\u0026amp; cmake .. \u0026amp;\u0026amp; cmake --build . 执行上述命令之后会生成很多文件，但目前我们需要关注的只有 libmy_math.a，这就是其他项目需要依赖的库了。\n# 管理依赖 下面看如何让 HelloWorld 项目依赖 my_math 项目。\n首先要明确的是，CMake 并不能像 Maven/Cargo 那样直接从互联网下载你需要的包，而是需要你自己下载了包之后放在项目依赖的目录下，然后编辑 CMakeLists.txt，让你的项目能够识别到这个依赖。这更像早期使用 Ant 编译 Java 的时代吧。\n# 最简单的依赖 HelloWorld 的结构暂时不变，但需要创建一个 include 目录来存放依赖，先把 lib-demo 的所有代码放在 HelloWorld 的 include 目录下。\n1 2 3 4 5 6 7 8 9 cmake_minimum_required(VERSION 3.27) project(HelloWorld) add_executable(HelloWorld src/main.c) add_subdirectory(${CMAKE_CURRENT_SOURCE_DIR}/include/my_math) target_link_libraries(HelloWorld my_math) target_include_directories(HelloWorld PRIVATE include/my_math/include) [!INFO] target_include_directories 的第二个参数 PRIVATE 是什么意思？ 这个命令为目标（可执行文件或库）指定包含目录，第二个参数可选值有 3 个：PUBLIC、PRIVATE 和 INTERFACE，这决定了包含目录的范围和传播行为：\nPRIVATE：指定的包含目录仅用于这个目标的构建，并且不会传递给依赖这个目标的其他目标。如果你有一个目标（如一个库或可执行文件），而这个目标的头文件仅在源文件中内部使用，没有在任何对外的头文件中使用，则应该将这些内部使用的头文件目录标记为 PRIVATE。\nPUBLIC：指定的包含目录既用于这个目标的构建，也会传递给依赖这个目标的其他目标。使用 PUBLIC 意味着连接到这个库的任何目标也将自动添加这些包含目录到它们的包含目录列表中。如果你的库的公共头文件需要某些路径才能被找到，那么这些路径应该被标记为 PUBLIC。\nINTERFACE：指定的包含目录不用于这个目标的构建，但会传递给依赖这个目标的其他目标。这通常用在只包含头文件的接口库上，这种情况下构建目标本身并不需要这些包含目录，但是使用该目标的其他目标需要。\n简而言之，PRIVATE 意味着仅用于构建当前目标，PUBLIC 意味着既用于构建当前目标也用于依赖它的目标，而 INTERFACE 意味着仅用于依赖当前目标的其他目标。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 #include \u0026#34;my_math/my_math.h\u0026#34; #include \u0026lt;stdio.h\u0026gt; int main() { int a = 10, b = 5; int c = max(a, b); if (c != 10) { printf(\u0026#34;max is not correct\\n\u0026#34;); } else { printf(\u0026#34;c is %d, and max is included\\n\u0026#34;, c); } printf(\u0026#34;hello world!\\n\u0026#34;); return 0; } 接下来怎么让这个代码运行起来，这里就不再赘述了。\n# 嵌套的依赖 前面其实已经完整讲述了一个项目和它的依赖应该如何组织了，其实更深层的依赖管理和上面描述的也没有什么区别，比如上面的 my_math 库又依赖了别的库比如 his_math，这时你有两个选择：\n如果 HelloWorld 也使用到了 his_math，那你应该把它放在 HelloWorld 项目的 include 里，这样两个项目都可以使用它。 如果只有 my_math 用它，那就可以把它放在 my_math 的 include 里，具体的做法和上面的并无其他不同。 # 单元测试 没错，CMake 还能做单元测试。\n在 my_math 项目里添加一个 tests 目录，把 CMakeLists.txt 改一下，判断是否存在 tests 目录，如下\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 cmake_minimum_required(VERSION 3.27) project(my_math) # 设置C标准 set(CMAKE_C_STANDARD 11) set(CMAKE_C_STANDARD_REQUIRED True) # 创建一个明为my_math的静态库 add_library(${PROJECT_NAME} include/my_math/my_math.h src/my_math.c) add_subdirectory(third_party/his_math) target_link_libraries(${PROJECT_NAME} his_math) if(EXISTS ${CMAKE_CURRENT_SOURCE_DIR}/tests/CMakeLists.txt) enable_testing() add_subdirectory(tests) include(CTest) endif() # 指定库的头文件搜索路径 target_include_directories(${PROJECT_NAME} PUBLIC ${CMAKE_CURRENT_SOURCE_DIR}/include) 这里我把检测到 tests/CMakeLists.txt 之后的操作全部写在一个 if 里了。\n然后 tests 目录本身也需要一个 CMakeLists.txt，如下\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 cmake_minimum_required(VERSION 3.27) project(my_math_tests) # 设置C标准 set(CMAKE_C_STANDARD 11) set(CMAKE_C_STANDARD_REQUIRED True) enable_testing() add_executable(${PROJECT_NAME} test_my_math.c) # 指定库的头文件搜索路径 target_include_directories(${PROJECT_NAME} PUBLIC ${CMAKE_SOURCE_DIR}/include) target_link_libraries(${PROJECT_NAME} his_math my_math) add_test(NAME my_math_tests COMMAND ${PROJECT_NAME}) 这里要注意的点还是挺多的，比如前面写的都是 ${CMAKE_CURRENT_SOURCE_DIR}/include，这里却是 ${CMAKE_SOURCE_DIR}/include。\n其实这个单元测试就是一个的 executable，加了一个通过 ctest 命令启动的入口。\n然后写一个 test_my_math.c，如下\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 #include \u0026#34;my_math/my_math.h\u0026#34; #include \u0026lt;stdio.h\u0026gt; void test_max() { int result = max(1, 2); if (result == 2) { printf(\u0026#34;test_max passed.\\n\u0026#34;); } else { printf(\u0026#34;test_max failed. Expected 2 but got %d.\\n\u0026#34;, result); } } int main() { test_max(); // 这里可以添加更多的测试 return 0; // 如果有测试失败，你可能想要返回非零值 } 这时候去 my_math 的 build 目录里执行 cmake .. \u0026amp;\u0026amp; cmake --build . \u0026amp;\u0026amp; ctest ，就会发现单元测试已经可以执行了。\n1 2 3 4 5 6 7 8 ❯ ctest Test project /Users/frost/workspace/private/cmake-tutorial/simple-demo/third_party/my_math/build Start 1: my_math_tests 1/1 Test #1: my_math_tests .................... Passed 0.39 sec 100% tests passed, 0 tests failed out of 1 Total Test time (real) = 0.39 sec ","date":"2024-04-02T00:13:48+08:00","image":"https://blog.happyhacker.fun/p/%E5%85%89%E9%80%9F%E5%AD%A6%E4%BC%9Acmake/cover_hu792f1a0561a8d6d66c4c119859805226_1770492_120x120_fill_box_smart1_3.png","permalink":"https://blog.happyhacker.fun/p/%E5%85%89%E9%80%9F%E5%AD%A6%E4%BC%9Acmake/","title":"光速学会CMake"},{"content":"公司发的2020款M1芯片MacBook Pro前几天升级了macOS 13.0.1，噩梦开始了。\n是从官网看到新版本的macOS更新了更好用的Spotlight、邮件等app，觉得是挺好用的新特性，加上已经更新了一个小的bugfix版本，就更新了，结果遇到了以下问题\n无端突然黑屏隔几秒后回到登录界面，输入密码登录后提示WindowServer crash，一大堆没什么用的信息 Spotlight反应明显慢亿拍，这是我每天用几十次的功能，结果现在变成了唤起之后输入等5秒左右才显示在输入框里，然后开始搜索，所以是反向升级了 这是非常影响工作的，几乎变成不可用的状态了，毕竟一崩溃所有正在用的窗口全都异常关闭，好在没出现数据异常丢失的情况。而且最常用的功能反应慢了那么多。\n问了周围几个同事，有的很早之前就更新了也没有出现这个问题，我就纳闷了，后来发现好像他们都不是M1芯片。\nReddit上扒了一些帖子，发现遇到这个问题的还不在少数，而且目前看起来13.1Beta2也没有解决。好在有网友发现了问题的根源，按照这个帖子删除以下两个文件\n1 2 ~/Library/Preferences/ByHost/com.apple.windowserver.displays.\u0026lt;LONG HEX STRING\u0026gt;.plist /Library/Preferences/com.apple.windowserver.displays.plist 重启，就好了。不过这只是解决了黑屏崩溃的问题，并没有解决Spotlight反应慢的问题。\n其实中间想着降级来着，但不知道为什么已经回不到出厂的10.12 Monterey版本了，重新装了一遍13.0.1（已经备份的数据的情况），数据没有丢，问题也没有解决。好在现在这个最严重的问题解决了，就记录一下。\n","date":"2022-11-29T23:28:48+08:00","permalink":"https://blog.happyhacker.fun/p/windowserver-crash-on-macos-13/","title":"WindowServer Crash on Macos 13"},{"content":"用Filebeat收集日志一直有一个痛点，就是不知道要什么时候才能把要收集的文件删除。\n之前的解决方案是定时删除文件，但删除的时候其实也不保证已经收集了，所以肯定是有概率会删除正在收集中的文件，这时候其实文件句柄正在被filebeat持有，所以删除也不能释放空间。\n简单的搜索就发现了这篇文章FileBeats -Are there any ways we can delete the log files after file beat harvest the data to logstash 。\n总结下来原理很简单，就是比较registry中的文件对应的offset，如果offset和当前文件的size相同，就表示已经收集完成（只是表示已经收集到文件最后的位置，并不表示文件不再继续写入了），这时候就可以删除文件了。\n而基于之前升级filebeat的经验，我们把所有filebeat实例配置都放在同一个目录下，我们的代码就只需要便利这个目录下所有的data/registry/filebeat/log.json文件，分析其中的数据即可。这里放一个简单的例子\n这个文件虽然是叫log.json，但其实内容是多行json，也就是每行是一个json串，这里要注意一下。\n1 2 {\u0026#34;k\u0026#34;:\u0026#34;filebeat::logs::native::361-2055\u0026#34;,\u0026#34;v\u0026#34;:{\u0026#34;timestamp\u0026#34;:[698286230,1641268968],\u0026#34;ttl\u0026#34;:-1,\u0026#34;type\u0026#34;:\u0026#34;log\u0026#34;,\u0026#34;FileStateOS\u0026#34;:{\u0026#34;inode\u0026#34;:361,\u0026#34;device\u0026#34;:2055},\u0026#34;source\u0026#34;:\u0026#34;path/to/log.log\u0026#34;,\u0026#34;offset\u0026#34;:153040051,\u0026#34;identifier_name\u0026#34;:\u0026#34;native\u0026#34;,\u0026#34;id\u0026#34;:\u0026#34;native::361-2055\u0026#34;,\u0026#34;prev_id\u0026#34;:\u0026#34;\u0026#34;}} {\u0026#34;op\u0026#34;:\u0026#34;set\u0026#34;,\u0026#34;id\u0026#34;:8250639157} 这里面需要处理的就是v.source和v.offset。处理过程中需要注意很多异常，但多数都是可以忽略的，主要是因为可能这个文件已经不存在了，但是registry信息还在这个log.json里面。\n不得不说用golang处理这种问题简直是“干净又卫生”，比用shell处理降低了非常多的心智负担，而且非常快速就能解决问题。在Mac上写的代码打一个linux的包放在/etc/cron.hourly目录下就能每小时执行一次了。\n","date":"2022-01-04T11:46:03+08:00","permalink":"https://blog.happyhacker.fun/p/filebeat-%E8%87%AA%E5%8A%A8%E5%88%A0%E9%99%A4%E5%B7%B2%E7%BB%8F%E6%94%B6%E9%9B%86%E5%AE%8C%E7%9A%84%E6%96%87%E4%BB%B6/","title":"Filebeat 自动删除已经收集完的文件"},{"content":"终于看到Flink承认自己在背压高的时候Checkpoint慢的事实了。甚至详细介绍的文章都才只写了第一篇。\n关于Unaligned Checkpoint（非对齐检查点）的详细介绍官网上已经有很多了，前段时间刚发布了系列文章的第一篇 From Aligned to Unaligned Checkpoints - Part 1: Checkpoints, Alignment, and Backpressure。其中明确提到了以下内容\nDespite all these great properties, Flink’s checkpointing method has an Achilles Heel: the speed of a completed checkpoint is determined by the speed at which data flows through the application. When the application backpressures, the processing of checkpoints is backpressured as well (Appendix 1 recaps what is backpressure and why it can be a good thing). In such cases, checkpoints may take longer to complete or even time out completely.\n之前一直觉得Flink在流式计算领域是神一样的存在，没有缺点。但实际用了之后才发现就这一点就够喝一壶了。所谓流式数据其实就是（没有边界的）消息队列了，那么消息队列的一大用途就是削峰填谷，好了，这里面的_消峰_就是在流量高峰的时候能以其极高的性能扛住压力，保证在数据压力降下来之前数据的不丢失。没错，Kafka在这里扛住了，但Flink掉链子了。\n所谓背压（有的叫反压，原文Back Pressure），对于数据源（DataSource）来说，其实就是下游的消费能力不足，导致上游数据无法完成整个流程（从数据源流入数据汇DataSink），具体到Kafka的这个场景来说就是业务处理的流程慢。\n正常来说，我们是希望当数据流量大的时候系统能加快处理，比如设计处理能力是1000tps，实际平时只有300tps，那么当流量上来时我们是期望它能按设计处理能力消费数据，让数据高峰尽快消散的，但实际情况是当数据量增大时，处理能力从300tps变成了2tps。\n是的，堆积越多处理越慢。反过来处理越慢，堆积越快。陷入了死循环。\n上面文章里也说了，导致这个结果的原因并不是真的是业务代码处理的慢，确确实实就是在背压出现时，Checkpoint变慢了。所以在新版本推出了非对齐检查点模式。\n这里有一个Inflight-data的概念，我理解就是新的检查点方式是把每个TaskManager中处理的数据都快照下来了，也不用管水位线什么的，直接搞起，完成一个删除上一个，带来的效果就是完成检查点的速度和背压没有太直接的关系了，实际的使用也验证了这一点。但和预期还是有稍稍的不同，从Kafka监控来看，按照之前对齐检查点方式，每个检查点完成后立即就能看到监控上的消费波峰，但非对齐检查点的完成和波峰就没有直接关系，不过它起码比对齐检查点好在不会在数据流量高峰到来时全部超时，导致系统瘫痪。\n带来的好处直观而明显，但不方便之处也是有的。\n对检查点存储后端的压力会非常大。 之前每个检查点大小是24K左右，而改成新的方式后就达到了200MB左右，对IO的压力增加可想而知，不过由于我们用的是rocksdb后端，所以这个压力可以承受。\n这种情况下自动创建的检查点不能用来扩容/缩容。 由于没有对齐，就没办法做内部的rescale，重启前后的TaskManager数量必须一致。但好在可以通过人工生成SavePoint的方式来创建一个完整的保存点，用保存点保证重启过程的数据不丢失。\n# 总结 总之Flink的这个新功能还是非常有用的，在使用这个功能之前数据量增大的时候只能祈祷它不超时，然而总是事与愿违。 看文档说后面的目标是把非对齐检查点作为默认的检查点模式，从目前看还有很长的路要走。\n","date":"2021-02-04T22:55:48+08:00","permalink":"https://blog.happyhacker.fun/p/flink%E7%9A%84%E6%96%B0%E7%89%B9%E6%80%A7unaligned-checkpoints/","title":"Flink的新特性——Unaligned Checkpoints"},{"content":"我们都自称 CRUD boy 了，没有数据库拿什么 CRUD？\n# 实验环境 简单起见，我们在本地搭建一个 MySQL 服务，使用brew install mysql即可安装，之后通过brew services start mysql即可启动 MySQL 服务。\n相应的还可以通过brew services list查看运行中的服务，通过brew services stop mysql来停止 mysql 服务。如果是首次安装还需要执行mysql_secure_installation来初始化密码等。\n# 建库建表 1 2 3 4 5 6 7 create database happyhacker; use happyhacker; create table employee ( `id` int unsigned auto_increment primary key, `name` varchar(40) not null default \u0026#39;\u0026#39;, `age` tinyint unsigned not null default 0 ) engine=innodb default charset=utf8mb4; # JDBC 简单来讲 JDBC 就是一套操作数据库的 API（interface），它是不包含实现的（implementation）。各数据库供应商（如 MySQL 等）提供驱动（实现）来完成对数据库的操作。\nJDBC 提供了两个功能：\nJDBC API 提供 Java 应用和 JDBC Manager 之间的通信 JDBC driver 提供了 JDBC Manager 和数据库驱动之间的通信 我们先来感受一下使用原生的 JDBC 怎样操作数据库。\n# 建立连接（Connection） 1 Connection connection = DriverManager.getConnection(\u0026#34;jdbc:mysql://127.0.0.1:3306/happyhacker\u0026#34;, \u0026#34;root\u0026#34;, \u0026#34;12345678\u0026#34;); # 创建语句（Statement） 1 2 Statement stmt = connection.createStatement(); String sql = \u0026#34;select * from employee\u0026#34;; # 执行语句并返回结果（ResultSet） 1 ResultSet rs = stmt.executeQuery(sql); # 对结果集进行迭代取出数据 1 2 3 4 5 6 7 while (rs.next()) { int id = rs.getInt(\u0026#34;id\u0026#34;); String name = rs.getString(\u0026#34;name\u0026#34;); int age = rs.getInt(\u0026#34;age\u0026#34;); System.out.printf(\u0026#34;id: %d,\\t name: %s,\\t age: %d\\n\u0026#34;, id, name, age); } # 关闭连接和语句 1 2 stmt.close(); connection.close(); 从上面的例子中也可以看出，通过 JDBC 对数据库的操作分为以下步骤：\n建立连接（Connection） 建立语句（Statement） 执行语句并返回结果（ResultSet） 对结果集进行迭代取出数据 下面列出的完整代码包含对数据库的增删改查\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 package fun.happyhacker.springbootdemo; import java.sql.*; public class JDBCTest { public static void main(String[] args) { jdbcTest(); } private static void jdbcTest() { Connection connection = null; Statement stmt = null; try { Class.forName(\u0026#34;com.mysql.cj.jdbc.Driver\u0026#34;); connection = DriverManager.getConnection(\u0026#34;jdbc:mysql://127.0.0.1:3306/happyhacker\u0026#34;, \u0026#34;root\u0026#34;, \u0026#34;12345678\u0026#34;); stmt = connection.createStatement(); String selectSql = \u0026#34;select * from employee\u0026#34;; ResultSet rs = stmt.executeQuery(selectSql); while (rs.next()) { int id = rs.getInt(\u0026#34;id\u0026#34;); String name = rs.getString(\u0026#34;name\u0026#34;); int age = rs.getInt(\u0026#34;age\u0026#34;); System.out.printf(\u0026#34;id: %d,\\t name: %s,\\t age: %d\\n\u0026#34;, id, name, age); } String insertSql = \u0026#34;insert into employee (`name`, `age`) values (\u0026#39;John\u0026#39;, 13)\u0026#34;; long effectedRows1 = stmt.executeUpdate(insertSql, new String[]{\u0026#34;id\u0026#34;}); if (effectedRows1 \u0026gt; 0) { System.out.println(\u0026#34;insert ok\u0026#34;); } else { System.out.println(\u0026#34;insert failed\u0026#34;); } String updateSql = \u0026#34;update employee set `name` = \u0026#39;Tam\u0026#39; where id = 3\u0026#34;; long effectedRows2 = stmt.executeUpdate(updateSql); if (effectedRows2 \u0026gt; 0) { System.out.println(\u0026#34;update ok\u0026#34;); } else { System.out.println(\u0026#34;update failed\u0026#34;); } String deleteSql = \u0026#34;delete from employee where id = 5\u0026#34;; long effectedRows3 = stmt.executeUpdate(deleteSql); if (effectedRows3 \u0026gt; 0) { System.out.println(\u0026#34;delete ok\u0026#34;); } else { System.out.println(\u0026#34;delete failed\u0026#34;); } rs.close(); stmt.close(); connection.close(); } catch (SQLException e) { e.printStackTrace(); } finally { if (stmt != null) { try { stmt.close(); } catch (SQLException throwables) { throwables.printStackTrace(); } } if (connection != null) { try { connection.close(); } catch (SQLException throwables) { throwables.printStackTrace(); } } } } } 不要忘了需要在pom.xml中添加如下依赖\n1 2 3 4 5 \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;mysql\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;mysql-connector-java\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;8.0.20\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; 注意其中的Class.forName(\u0026quot;com.mysql.cj.jdbc.Driver\u0026quot;)这句，老的文档都会写这句，但其实使用新版的 MySQL Driver 的话这个已经不需要了，因为新版本有这个 # 连接池 前面我们看到Statement和Connection都是可以复用的，但这仅仅局限在一个方法中。想让一个连接在整个应用中都可用，就要把它做成全局变量。\n最简单的做法就是这样\n1 2 3 4 5 6 7 8 9 10 11 package fun.happyhacker.springbootdemo.jdbc; import java.sql.Connection; import java.sql.DriverManager; import java.sql.SQLException; public class JDBCConnection { public static Connection getConnection() throws SQLException { return DriverManager.getConnection(\u0026#34;jdbc:mysql://127.0.0.1:3306/happyhacker\u0026#34;, \u0026#34;root\u0026#34;, \u0026#34;12345678\u0026#34;); } } 这样在需要用它的地方直接调用JDBCConnection.getConnection()就可以了。但要注意用完之后可不要调用connection.close()，因为一旦 close 就无法再被别人使用了。\n再往下需要考虑的东西就很多了，比如\n用完之后不立即 close，那什么时候 close？ 一个连接用多少次 close？ 应用高并发的情况下不可能只用一个连接，多个连接如何保持？ 连接异常断开后如何自动重连？ 想想就觉得很复杂，好在这个问题已经有很完善的解决方案了。早前比较流行的有Apache 的 DBCP，老牌的 C3P0，打败它们俩的 BoneCP，后起之秀 Hikari CP等等，而 Hikari CP 得益于其优异的性能傲视群雄，目前已经是 Springboot 默认的连接池框架，搞的 BoneCP 的作者都公开声称不再更新了，建议大家去用 HikariCP。BoneCP 的 github 主页介绍只有这样一段了\n# BoneCP BoneCP is a Java JDBC connection pool implementation that is tuned for high performance by minimizing lock contention to give greater throughput for your applications. It beats older connection pools such as C3P0 and DBCP but should now be considered deprecated in favour of HikariCP.\n这是得多心灰意冷啊。所以我们直接 HikariCP 就行了。对了还有一个国产的阿里出品 Druid，说实话用了那么多开源的东西之后，对阿里家开源的东西还是要谨慎一些，这个东西文档不丰富，用户遇到了问题反馈了得不到回应。\n# 最简单的数据库配置 最简单的配置就是使用 Springboot 内置的模板配置了，使用数据库需要引入一个依赖即可\n1 2 3 4 \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-data-jdbc\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 引入这个依赖就不用再引用 HikariCP 了。\n然后在配置文件中加入这些\n1 2 3 4 5 spring.datasource.type=com.zaxxer.hikari.HikariDataSource spring.datasource.hikari.driver-class-name=com.mysql.cj.jdbc.Driver spring.datasource.hikari.username=root spring.datasource.url=jdbc:mysql://localhost:3306/happyhacker spring.datasource.hikari.password=12345678 接下来就可以直接注入DataSource了。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 package fun.happyhacker.springbootdemo.controller; import fun.happyhacker.springbootdemo.hikari.Employee; import org.springframework.beans.factory.annotation.Autowired; import org.springframework.web.bind.annotation.GetMapping; import org.springframework.web.bind.annotation.RestController; import javax.sql.DataSource; import java.sql.Connection; import java.sql.PreparedStatement; import java.sql.ResultSet; import java.sql.SQLException; import java.util.ArrayList; import java.util.List; @RestController public class EmployeeController { @Autowired private DataSource ds; @GetMapping(\u0026#34;/list\u0026#34;) public String list() { List\u0026lt;Employee\u0026gt; employees = new ArrayList\u0026lt;\u0026gt;(); try (Connection connection = ds.getConnection(); PreparedStatement ps = connection.prepareStatement(\u0026#34;select * from employee\u0026#34;); ResultSet rs = ps.executeQuery(); ) { Employee employee = new Employee(); while (rs.next()) { employee.setId(rs.getInt(\u0026#34;id\u0026#34;)); employee.setName(rs.getString(\u0026#34;name\u0026#34;)); employee.setAge(rs.getInt(\u0026#34;age\u0026#34;)); employees.add(employee); } } catch (SQLException throwables) { throwables.printStackTrace(); } return employees.toString(); } } 这里提一个小问题，访问这个接口你会发现你的响应是这样的\n1 [Employee(id=8, name=John, age=13), Employee(id=8, name=John, age=13), Employee(id=8, name=John, age=13), Employee(id=8, name=John, age=13), Employee(id=8, name=John, age=13), Employee(id=8, name=John, age=13), Employee(id=8, name=John, age=13)] 这和我们理解的 Json 是有区别的，更多信息可以参考实现 toJson() 方法\n# JPA和MyBatis JPA本质就是Hibernate，是一个“轻量级”的ORM框架（至于是不是真的轻量，得看跟谁比）。上手还是非常简单的。\n# JPA入门——增删改查 # 引入依赖 1 2 3 4 5 6 7 8 9 \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-data-jpa\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;mysql\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;mysql-connector-java\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;8.0.20\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; # 添加配置 application.properties 1 2 3 4 5 6 7 8 spring.datasource.type=com.zaxxer.hikari.HikariDataSource spring.datasource.hikari.driver-class-name=com.mysql.cj.jdbc.Driver spring.datasource.hikari.username=root spring.datasource.url=jdbc:mysql://localhost:3306/happyhacker?charset=utf8mb4 spring.datasource.hikari.password=12345678 spring.jpa.database-platform=org.hibernate.dialect.MySQL5InnoDBDialect spring.jpa.show-sql=true spring.jpa.hibernate.ddl-auto=validate # 创建实体类 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 package fun.happyhacker.springbootdemo.jpa.entity; import lombok.Data; import javax.persistence.*; @Entity @Table(name = \u0026#34;employee\u0026#34;) @Data public class Employee { @Id @GeneratedValue private Long id; @Column(length = 32) private String name; private Integer age; } 注意：上面的表名大小写不需区分，可以全部大写\n# 创建Repository接口 1 2 3 4 5 6 7 8 9 10 11 package fun.happyhacker.springbootdemo.jpa.repository; import fun.happyhacker.springbootdemo.jpa.entity.Employee; import org.springframework.data.jpa.repository.JpaRepository; import org.springframework.stereotype.Repository; import java.util.List; @Repository public interface EmployeeRepository extends JpaRepository\u0026lt;Employee, Long\u0026gt; { } # 验证功能 有了上面这些配置，就可以体验JPA带给你的强大功能了。\n先看在Controller类中测试。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 package fun.happyhacker.springbootdemo.controller; import fun.happyhacker.springbootdemo.hikari.ExtensionMethods; import fun.happyhacker.springbootdemo.jpa.entity.Employee; import fun.happyhacker.springbootdemo.jpa.repository.EmployeeRepository; import lombok.experimental.ExtensionMethod; import org.springframework.beans.factory.annotation.Autowired; import org.springframework.web.bind.annotation.GetMapping; import org.springframework.web.bind.annotation.RestController; @RestController @ExtensionMethod({ExtensionMethods.class}) public class EmployeeController { @Autowired private EmployeeRepository employeeRepository; @GetMapping(\u0026#34;/list\u0026#34;) public String list() { Employee john = new Employee(); john.setAge(20); john.setName(\u0026#34;John\u0026#34;); Employee lam = new Employee(); lam.setName(\u0026#34;lam\u0026#34;); lam.setAge(30); employeeRepository.save(john); employeeRepository.save(lam); Employee first = employeeRepository.getOne(1L); first.setName(\u0026#34;happyhacker\u0026#34;); employeeRepository.save(first); employeeRepository.deleteById(2L); return employeeRepository.findAll().toString(); } } 在看junit中的测试\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 package fun.happyhacker.springbootdemo.jpa.repository; import fun.happyhacker.springbootdemo.jpa.entity.Employee; import org.junit.jupiter.api.Test; import org.springframework.beans.factory.annotation.Autowired; import org.springframework.boot.test.context.SpringBootTest; import java.util.ArrayList; import java.util.List; import java.util.Optional; import static org.junit.jupiter.api.Assertions.assertEquals; import static org.junit.jupiter.api.Assertions.assertFalse; @SpringBootTest class EmployeeRepositoryTest { @Autowired private EmployeeRepository employeeRepository; @Test void crud() { Employee john = new Employee(); john.setAge(20); john.setName(\u0026#34;John\u0026#34;); Employee lam = new Employee(); lam.setName(\u0026#34;lam\u0026#34;); lam.setAge(30); Employee johnWithId = employeeRepository.save(john); assertEquals(john.getName(), johnWithId.getName()); Employee lamWithId = employeeRepository.save(lam); assertEquals(lam.getName(), lamWithId.getName()); lamWithId.setAge(45); Employee oldLam = employeeRepository.save(lamWithId); assertEquals(45, oldLam.getAge()); employeeRepository.delete(oldLam); Optional\u0026lt;Employee\u0026gt; employeeOptional = employeeRepository.findById(2L); assertFalse(employeeOptional.isPresent()); List\u0026lt;Employee\u0026gt; list = employeeRepository.findAll(); List\u0026lt;Employee\u0026gt; expected = new ArrayList\u0026lt;\u0026gt;(); expected.add(johnWithId); assertEquals(expected, list); } } # JPA处理联合主键 JPA处理联合主键主要有两种方式，用法有区别，最终生成的SQL语句也有区别，具体用哪种方式主要看场景。\n在典型的RBAC权限控制系统中会有很多联合主键的情况，下面来取一个简单的例子说明。\nRole是角色，包含id RoleAccount是角色和员工的映射，其中包含roleId和accountId # @IdClass 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 package fun.happyhacker.springbootdemo.jpa.idclass.entity; import lombok.Data; import javax.persistence.Entity; import javax.persistence.GeneratedValue; import javax.persistence.Id; @Entity @Data public class Role { @Id @GeneratedValue private Integer id; private String name; } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 package fun.happyhacker.springbootdemo.jpa.idclass.entity; import lombok.Data; import javax.persistence.Entity; import javax.persistence.GeneratedValue; import javax.persistence.Id; @Entity @Data public class Account { @Id @GeneratedValue private Integer id; private String name; private Integer age; } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 package fun.happyhacker.springbootdemo.jpa.idclass.entity; import lombok.Data; import javax.persistence.Entity; import javax.persistence.Id; import javax.persistence.IdClass; @Entity @Data @IdClass(RoleAccountId.class) public class RoleAccount { @Id private Integer roleId; @Id private Integer accountId; } 1 2 3 4 5 6 7 8 9 10 11 package fun.happyhacker.springbootdemo.jpa.idclass.entity; import lombok.Data; import java.io.Serializable; @Data public class RoleAccountId implements Serializable { private Integer roleId; private Integer accountId; } 注意：联合主键类需要实现Serializable接口\n1 2 3 4 5 6 7 8 package fun.happyhacker.springbootdemo.jpa.idclass.repository; import fun.happyhacker.springbootdemo.jpa.idclass.entity.RoleAccount; import fun.happyhacker.springbootdemo.jpa.idclass.entity.RoleAccountId; import org.springframework.data.jpa.repository.JpaRepository; public interface RoleAccountRepository extends JpaRepository\u0026lt;RoleAccount, RoleAccountId\u0026gt; { } 1 2 3 4 5 6 7 package fun.happyhacker.springbootdemo.jpa.idclass.repository; import fun.happyhacker.springbootdemo.jpa.idclass.entity.Role; import org.springframework.data.jpa.repository.JpaRepository; public interface RoleRepository extends JpaRepository\u0026lt;Role, Integer\u0026gt; { } 1 2 3 4 5 6 7 package fun.happyhacker.springbootdemo.jpa.idclass.repository; import fun.happyhacker.springbootdemo.jpa.idclass.entity.Account; import org.springframework.data.jpa.repository.JpaRepository; public interface AccountRepository extends JpaRepository\u0026lt;Account, Integer\u0026gt; { } 下面是单元测试\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 package fun.happyhacker.springbootdemo.jpa.idclass.repository; import fun.happyhacker.springbootdemo.jpa.idclass.entity.Account; import fun.happyhacker.springbootdemo.jpa.idclass.entity.Role; import fun.happyhacker.springbootdemo.jpa.idclass.entity.RoleAccount; import org.junit.jupiter.api.Test; import org.springframework.beans.factory.annotation.Autowired; import org.springframework.boot.test.context.SpringBootTest; @SpringBootTest class RoleAccountRepositoryTest { @Autowired private RoleRepository roleRepository; @Autowired private AccountRepository accountRepository; @Autowired private RoleAccountRepository roleAccountRepository; @Test void idClassTest() { Role admin = new Role(); admin.setId(1); admin.setName(\u0026#34;admin\u0026#34;); Role role1 = new Role(); role1.setId(2); role1.setName(\u0026#34;role1\u0026#34;); Role role2 = new Role(); role2.setId(3); role2.setName(\u0026#34;role2\u0026#34;); roleRepository.save(admin); roleRepository.save(role1); roleRepository.save(role2); Account john = new Account(); john.setAge(30); john.setName(\u0026#34;john\u0026#34;); john.setId(4); accountRepository.save(john); RoleAccount roleAccount = new RoleAccount(); roleAccount.setRoleId(admin.getId()); roleAccount.setAccountId(john.getId()); roleAccountRepository.save(roleAccount); } } # @Embeddable和@EmbeddedId 和上面相同的代码就再贴了，只贴不同的文件。\n1 2 3 4 5 6 7 8 9 10 11 12 13 package fun.happyhacker.springbootdemo.jpa.embeddable.entity; import lombok.Data; import javax.persistence.EmbeddedId; import javax.persistence.Entity; @Entity @Data public class RoleAccount { @EmbeddedId private RoleAccountId roleAccountId; } 1 2 3 4 5 6 7 8 9 10 11 12 13 package fun.happyhacker.springbootdemo.jpa.embeddable.entity; import lombok.Data; import javax.persistence.Embeddable; import java.io.Serializable; @Data @Embeddable public class RoleAccountId implements Serializable { private Integer roleId; private Integer accountId; } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 package fun.happyhacker.springbootdemo.jpa.embeddable.repository; import fun.happyhacker.springbootdemo.jpa.embeddable.entity.Account; import fun.happyhacker.springbootdemo.jpa.embeddable.entity.Role; import fun.happyhacker.springbootdemo.jpa.embeddable.entity.RoleAccount; import fun.happyhacker.springbootdemo.jpa.embeddable.entity.RoleAccountId; import org.junit.jupiter.api.Test; import org.springframework.beans.factory.annotation.Autowired; import org.springframework.boot.test.context.SpringBootTest; @SpringBootTest class RoleAccountRepositoryTest { @Autowired private RoleRepository roleRepository; @Autowired private AccountRepository accountRepository; @Autowired private RoleAccountRepository roleAccountRepository; @Test void idClassTest() { Role admin = new Role(); admin.setId(1); admin.setName(\u0026#34;admin\u0026#34;); Role role1 = new Role(); role1.setId(2); role1.setName(\u0026#34;role1\u0026#34;); Role role2 = new Role(); role2.setId(3); role2.setName(\u0026#34;role2\u0026#34;); roleRepository.save(admin); roleRepository.save(role1); roleRepository.save(role2); Account john = new Account(); john.setAge(30); john.setName(\u0026#34;john\u0026#34;); john.setId(4); accountRepository.save(john); RoleAccount roleAccount = new RoleAccount(); RoleAccountId roleAccountId = new RoleAccountId(); roleAccountId.setRoleId(admin.getId()); roleAccountId.setAccountId(john.getId()); roleAccount.setRoleAccountId(roleAccountId); roleAccountRepository.save(roleAccount); } } # 小结 从上面的比较可以看出，最终使用的时候差别不大，都可以对联合主键进行很好的控制。区别在于，如果RoleAccountId类是从别人的jar包中引用的，你无法修改其中的内容，那么就无法添加@Embeddable注解，而这时候用@IdClass就比较简单了。除此之外的其他情况，我认为使用@Embeddable会更清楚一些。\n# MyBatis——基础用法 MyBatis是面向数据表的，而JPA（Hibernate）则是面向领域对象的，前者更接近数据库，而后者更接近业务。JPA的JpaRepository提供了非常多的内置方法，但面对国内互联网公司灵活多变的需求，这些内置方法还是不够多。这也是为什么MyBatis在国内比较流行的原因，据我了解很多人在用国内的一个封装——MyBatis-Plus。同时，在做这个datalink项目的过程中，我也越来越发现现在这种方式的弊端（复杂、添加schema需要做的事情太多等等），所以以下的MyBatis的例子我们就使用MyBatis-Plus替代。\n# 引入依赖 1 2 3 4 5 \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.baomidou\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;mybatis-plus-boot-starter\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;3.4.2\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; # 创建表 1 2 3 4 5 6 7 8 9 10 DROP TABLE IF EXISTS user; CREATE TABLE user ( id BIGINT(20) NOT NULL COMMENT \u0026#39;主键ID\u0026#39;, name VARCHAR(30) NULL DEFAULT NULL COMMENT \u0026#39;姓名\u0026#39;, age INT(11) NULL DEFAULT NULL COMMENT \u0026#39;年龄\u0026#39;, email VARCHAR(50) NULL DEFAULT NULL COMMENT \u0026#39;邮箱\u0026#39;, PRIMARY KEY (id) ); # 填充数据 1 2 3 4 5 6 INSERT INTO user (id, name, age, email) VALUES (1, \u0026#39;Jone\u0026#39;, 18, \u0026#39;test1@baomidou.com\u0026#39;), (2, \u0026#39;Jack\u0026#39;, 20, \u0026#39;test2@baomidou.com\u0026#39;), (3, \u0026#39;Tom\u0026#39;, 28, \u0026#39;test3@baomidou.com\u0026#39;), (4, \u0026#39;Sandy\u0026#39;, 21, \u0026#39;test4@baomidou.com\u0026#39;), (5, \u0026#39;Billie\u0026#39;, 24, \u0026#39;test5@baomidou.com\u0026#39;); 如果你自己测试用内存数据库h2，这些sql就不需要提前执行，只需要把建表语句schema.sql和数据语句data.sql放在resource目录下，添加以下两行配置即可\n1 2 spring.datasource.schema=classpath:database/h2/schema.sql spring.datasource.data=classpath:database/h2/data.sql # 添加配置 1 2 3 spring.datasource.url=jdbc:mysql://localhost:3306/happyhacker?charset=utf8mb4 spring.datasource.hikari.username=root spring.datasource.hikari.password=12345678 # 创建实体类 1 2 3 4 5 6 7 8 9 10 11 package fun.happyhacker.springbootdemo.mybatis.entity; import lombok.Data; @Data public class User { private Long id; private String name; private Integer age; private String email; } # 创建Mapper 1 2 3 4 5 6 7 8 package fun.happyhacker.springbootdemo.mybatis.mapper; import fun.happyhacker.springbootdemo.mybatis.entity.User; import org.apache.ibatis.annotations.Mapper; @Mapper public interface UserMapper { } # 添加方法 接下来就可以在UserMapper类中添加增删改查的方法了。\n# 在UserMapper.java中添加相关方法 1 2 3 4 5 6 7 8 9 package fun.happyhacker.springbootdemo.mybatis.mapper; import fun.happyhacker.springbootdemo.mybatis.entity.User; import org.apache.ibatis.annotations.Mapper; @Mapper public interface UserMapper extends BaseMapper { List\u0026lt;User\u0026gt; queryByNameEquals(String userName); } # 在resources/fun/happyhacker/springbootdemo/mybatis/mapper中添加UserMapper.xml，内容如下 1 2 3 4 5 6 7 8 9 10 11 12 13 \u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;!DOCTYPE mapper PUBLIC \u0026#34;-//mybatis.org//DTD Mapper 3.0//EN\u0026#34; \u0026#34;http://mybatis.org/dtd/mybatis-3-mapper.dtd\u0026#34;\u0026gt; \u0026lt;mapper namespace=\u0026#34;fun.happyhacker.springbootdemo.mybatis.mapper.UserMapper\u0026#34;\u0026gt; \u0026lt;resultMap id=\u0026#34;user\u0026#34; type=\u0026#34;fun.happyhacker.springbootdemo.mybatis.entity.User\u0026#34;\u0026gt; \u0026lt;id column=\u0026#34;id\u0026#34; property=\u0026#34;id\u0026#34; javaType=\u0026#34;int\u0026#34;/\u0026gt; \u0026lt;result column=\u0026#34;name\u0026#34; property=\u0026#34;name\u0026#34; javaType=\u0026#34;String\u0026#34;/\u0026gt; \u0026lt;result column=\u0026#34;age\u0026#34; property=\u0026#34;age\u0026#34; javaType=\u0026#34;int\u0026#34;/\u0026gt; \u0026lt;/resultMap\u0026gt; \u0026lt;select id=\u0026#34;queryByNameEquals\u0026#34; parameterType=\u0026#34;java.lang.String\u0026#34; resultMap=\u0026#34;user\u0026#34; resultType=\u0026#34;fun.happyhacker.springbootdemo.mybatis.entity.User\u0026#34;\u0026gt; SELECT * FROM user WHERE `name` = #{userName, jdbcType=VARCHAR} \u0026lt;/select\u0026gt; \u0026lt;/mapper\u0026gt; 注意：UserMapper.xml和UserMapper.java所在的包名一定要相同。\n这样就可以以同样的方式调用userMapper.queryByNameEquals(name)方法了。\n# 单元测试 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 package fun.happyhacker.springbootdemo.mybatis.mapper; import fun.happyhacker.springbootdemo.mybatis.entity.User; import org.junit.jupiter.api.Test; import org.springframework.beans.factory.annotation.Autowired; import org.springframework.boot.test.context.SpringBootTest; import java.util.List; import static org.junit.jupiter.api.Assertions.assertEquals; @SpringBootTest class UserMapperTest { @Autowired private UserMapper userMapper; @Test void testSelect() { List\u0026lt;User\u0026gt; userList = userMapper.queryByNameEquals(\u0026#34;Jone\u0026#34;); assertEquals(1, userList.size()); } } 如果不想在每个xxxMapper上面写@Mapper注解，可以在启动类上加@MapperScan(\u0026quot;fun.happyhacker.springbootdemo.mybatis.mapper\u0026quot;)，让系统自动扫描\n# MyBatis——MyBatis-Plus 上节虽然用的是MyBatis-Plus，但并没有用到任何它独有的特性，下面来介绍一下。\n# 内置方法 前面我们自己写了一个 List\u0026lt;User\u0026gt; queryByNameEquals(String userName);，但实际上这种增删改查的方法已经内置在BaseMapper\u0026lt;T\u0026gt;中了，多数时候我们只需要让UserMapper扩展BaseMapper\u0026lt;User\u0026gt;即可。\n# 代码生成 这个和原生MyBatis的代码生成功能差不多，多出的功能在于它能生成controller、service层的代码，虽然都是空的。而且能生成带lombok的的代码。\n这一点其实和国外团队的思路是不同的，他们可能更多的考虑职责分离，克制的加入带有个人主观偏好的功能，但国人开发的MyBatis-Plus则不同，更多从实际开发角度出发考虑问题，而不太关注设计模式。这也很像诸多的Android定制版，国内的各种定制版你可能说它臃肿、不简洁，但你不能说它不好用。\n# 添加依赖 1 2 3 4 5 \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.baomidou\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;mybatis-plus-generator\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;3.4.1\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; # 编写CodeGenerator.java 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 package fun.happyhacker.springbootdemo.mybatis; import com.baomidou.mybatisplus.core.exceptions.MybatisPlusException; import com.baomidou.mybatisplus.core.toolkit.StringUtils; import com.baomidou.mybatisplus.generator.AutoGenerator; import com.baomidou.mybatisplus.generator.config.DataSourceConfig; import com.baomidou.mybatisplus.generator.config.GlobalConfig; import com.baomidou.mybatisplus.generator.config.PackageConfig; import com.baomidou.mybatisplus.generator.config.StrategyConfig; import com.baomidou.mybatisplus.generator.config.rules.NamingStrategy; import java.util.Scanner; public class CodeGenerator { public static void main(String[] args) { AutoGenerator mpg = new AutoGenerator(); GlobalConfig gc = new GlobalConfig(); String projectPath = System.getProperty(\u0026#34;user.dir\u0026#34;) + \u0026#34;/springboot-demo\u0026#34;; gc.setOutputDir(projectPath + \u0026#34;/src/main/java\u0026#34;); gc.setAuthor(\u0026#34;happyhacker\u0026#34;); gc.setOpen(false); mpg.setGlobalConfig(gc); DataSourceConfig dsc = new DataSourceConfig(); dsc.setUrl(\u0026#34;jdbc:mysql://localhost:3306/happyhacker?charset=utf8mb4\u0026#34;); dsc.setDriverName(\u0026#34;com.mysql.cj.jdbc.Driver\u0026#34;); dsc.setUsername(\u0026#34;root\u0026#34;); dsc.setPassword(\u0026#34;12345678\u0026#34;); mpg.setDataSource(dsc); PackageConfig pc = new PackageConfig(); pc.setModuleName(scanner(\u0026#34;模块名\u0026#34;)); pc.setParent(\u0026#34;fun.happyhacker.mybatis\u0026#34;); mpg.setPackageInfo(pc); // InjectionConfig cfg = new InjectionConfig() { // @Override // public void initMap() { // // } // }; // String templatePath = \u0026#34;/templates/mapper.xml.vm\u0026#34;; // List\u0026lt;FileOutConfig\u0026gt; focList = new ArrayList\u0026lt;\u0026gt;(); // focList.add(new FileOutConfig(templatePath) { // @Override // public String outputFile(TableInfo tableInfo) { // return projectPath + \u0026#34;/src/main/resources/mapper/\u0026#34; + pc.getModuleName() + \u0026#34;/\u0026#34; + tableInfo.getEntityName() + \u0026#34;Mapper\u0026#34; + StringPool.DOT_XML; // } // }); // cfg.setFileOutConfigList(focList); // mpg.setCfg(cfg); // // TemplateConfig templateConfig = new TemplateConfig(); // templateConfig.setXml(null); // mpg.setTemplate(templateConfig); StrategyConfig strategyConfig = new StrategyConfig(); strategyConfig.setNaming(NamingStrategy.underline_to_camel); strategyConfig.setColumnNaming(NamingStrategy.underline_to_camel); // strategyConfig.setSuperEntityClass(\u0026#34;父类Entity，没有可以不用设置\u0026#34;); strategyConfig.setEntityLombokModel(true); strategyConfig.setRestControllerStyle(true); // strategyConfig.setSuperControllerClass(\u0026#34;父类Controller，没有就不用设置\u0026#34;); strategyConfig.setSuperEntityColumns(\u0026#34;id\u0026#34;); strategyConfig.setTablePrefix(pc.getModuleName() + \u0026#34;_\u0026#34;); mpg.setStrategy(strategyConfig); mpg.execute(); } private static String scanner(String tip) { Scanner scanner = new Scanner(System.in); System.out.println(\u0026#34;请输入 \u0026#34; + tip + \u0026#34;：\u0026#34;); if (scanner.hasNext()) { String ipt = scanner.next(); if (StringUtils.isNotBlank(ipt)) { return ipt; } } throw new MybatisPlusException(\u0026#34;请输入正确的 \u0026#34; + tip + \u0026#34;！\u0026#34;); } } 测试就不再赘述了。\n# 小结 数据库方面总的来说有JPA和MyBatis两种选择，一般来说国内用MyBatis比较多，这和我们在设计系统时先从数据库开始考虑的思维方式有关系，因为JPA更适应DDD（领域驱动设计）的开发方式，而这种方式在我们实际项目开发中几乎不存在。不过在我看来其实MyBatis-Plus提供的BaseMapper和JPA提供的JpaRepository作用其实差不多，而且后者多数时候更好用。\n虽说我个人更倾向于使用JPA，但考虑到国内用户的使用习惯和招聘难度，加上之前项目选择React而不是Vue引起大部分同事的抵触，最终还是决定使用MyBatis。\n# 实际项目中的数据库配置 实际的项目不会像上面那样简单，需要考虑更多的问题\n多数据库实例，肯定就不能用spring.datasource.type这种配置了 主从分离 得益于dynamic-datasource，我们就不用再费劲的自己通过代码配置数据源和主从分离了，而是简单的通过增加配置即可实现。\n# 回顾 首先让我们来回顾一下前面的经验，要实现一个简单的CRUD，需要如下的class/interface\nEntity -\u0026gt; User Mapper -\u0026gt; UserMapper Service -\u0026gt; UserService ServiceImpl -\u0026gt; UserServiceImpl 在UserService中，可能存在List\u0026lt;User\u0026gt; allUsers()和int createUser(User user)两个方法，如下\n1 2 3 4 5 6 7 8 9 10 11 package fun.happyhacker.springbootdemo.mybatis.service; import fun.happyhacker.springbootdemo.mybatis.entity.User; import java.util.List; public interface UserService { List\u0026lt;User\u0026gt; listAllUsers(); int createUser(User user); } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 package fun.happyhacker.springbootdemo.mybatis.service.impl; import com.baomidou.dynamic.datasource.annotation.DS; import com.baomidou.mybatisplus.core.conditions.query.QueryWrapper; import com.baomidou.mybatisplus.extension.service.impl.ServiceImpl; import fun.happyhacker.springbootdemo.mybatis.entity.User; import fun.happyhacker.springbootdemo.mybatis.mapper.UserMapper; import fun.happyhacker.springbootdemo.mybatis.service.UserService; import org.springframework.stereotype.Service; import java.util.List; import java.util.Objects; @Service public class UserServiceImpl extends ServiceImpl\u0026lt;UserMapper, User\u0026gt; implements UserService { @Override @DS(\u0026#34;account_slave\u0026#34;) public List\u0026lt;User\u0026gt; listAllUsers() { QueryWrapper\u0026lt;User\u0026gt; wrapper = new QueryWrapper\u0026lt;\u0026gt;(); wrapper.gt(\u0026#34;id\u0026#34;, 0); return baseMapper.selectList(wrapper); } @Override @DS(\u0026#34;account_master\u0026#34;) public int createUser(User user) { if (Objects.isNull(user)) { return 0; } return baseMapper.insert(user); } } 你可能已经注意到了这里面的@DS注解，它就是用来配置主从分离的。要实现上述的功能我们需要引入以下依赖\n1 2 3 4 5 \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.baomidou\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;dynamic-datasource-spring-boot-starter\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;3.3.1\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; dynamic-datasource的配置比较多，用yaml写会更方便一些\n添加数据源相关的配置\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 spring: datasource: dynamic: hikari: connection-timeout: 5000 leak-detection-threshold: 5000 max-lifetime: 30000 primary: account_master strict: false datasource: account_master: url: jdbc:mysql://192.168.99.101:3306/account username: account password: account! driver-class-name: com.mysql.cj.jdbc.Driver account_slave: url: jdbc:mysql://192.168.99.101:3307/account username: account_r password: account! driver-class-name: com.mysql.cj.jdbc.Driver 通过这个配置，我们可以很简单的配置主从分离。但这个方法有个致命的问题——分表。它提供了一个TableNameHandler可以用来根据传入的SQL来选择相应的表，看起来很美好，但实际上很难实现，看下代码\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 protected String changeTable(String sql) { TableNameParser parser = new TableNameParser(sql); List\u0026lt;TableNameParser.SqlToken\u0026gt; names = new ArrayList\u0026lt;\u0026gt;(); parser.accept(names::add); StringBuilder builder = new StringBuilder(); int last = 0; for (TableNameParser.SqlToken name : names) { int start = name.getStart(); if (start != last) { builder.append(sql, last, start); String value = name.getValue(); TableNameHandler handler = tableNameHandlerMap.get(value); if (handler != null) { builder.append(handler.dynamicTableName(sql, value)); } else { builder.append(value); } } last = name.getEnd(); } if (last != sql.length()) { builder.append(sql.substring(last)); } return builder.toString(); } 可以看到，它提供的方法是接收PreparedStatement中的sql部分，通俗点说就是还没有替换变量的SQL，比如我们要通过其中的uid字段以uid%128作为表名的后缀，通过这种方式就很难实现了。所以下一步就要祭出大杀器Apache ShardingSphere，它提供了针对分库分表的一整套解决方案。在实际应用中，我们只关心ShardingSphere-JDBC即可。\n# ShardingSphere-JDBC 在开始介绍之前，首先设定一个场景。\n假设我们的项目是要处理订单，由于订单很多，需要不仅要分表还要分库。为了简化问题，我们就分成2个库，每个库4张表，每个库一主一从。\norders_0 主库，包含表t_orders_0, t_orders_2, t_orders_4, t_orders_8 orders_1 主库，包含表 t_orders_1, t_orders_3, t_orders_5, t_orders_7 slave_0 从库，包含表t_orders_0, t_orders_2, t_orders_4, t_orders_8 slave_1从库，包含表 t_orders_1, t_orders_3, t_orders_5, t_orders_7 根据user_id分库，库名后缀为user_id%2，后缀可选范围为0和1 根据user_id分表，表名后缀为user_id%8，后缀可选范围为0-7 为了演示不需要分表的场景，添加一个t_order_config表，位于orders_0主库和orders_0从库 # 启动数据库服务 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 version: \u0026#34;3.3\u0026#34; services: db-3306: image: mysql:latest restart: always ports: - \u0026#34;3306:3306\u0026#34; volumes: - \u0026#34;3306:/var/lib/mysql:rw\u0026#34; environment: MYSQL_ALLOW_EMPTY_PASSWORD: \u0026#34;yes\u0026#34; MYSQL_ROOT_PASSWORD: \u0026#34;root\u0026#34; db-3307: image: mysql:latest restart: always ports: - \u0026#34;3307:3306\u0026#34; volumes: - \u0026#34;3307:/var/lib/mysql:rw\u0026#34; environment: MYSQL_ALLOW_EMPTY_PASSWORD: \u0026#34;yes\u0026#34; MYSQL_ROOT_PASSWORD: \u0026#34;root\u0026#34; db-3308: image: mysql:latest restart: always ports: - \u0026#34;3308:3306\u0026#34; volumes: - \u0026#34;3308:/var/lib/mysql:rw\u0026#34; environment: MYSQL_ALLOW_EMPTY_PASSWORD: \u0026#34;yes\u0026#34; MYSQL_ROOT_PASSWORD: \u0026#34;root\u0026#34; db-3309: image: mysql:latest restart: always ports: - \u0026#34;3309:3306\u0026#34; volumes: - \u0026#34;3309:/var/lib/mysql:rw\u0026#34; environment: MYSQL_ALLOW_EMPTY_PASSWORD: \u0026#34;yes\u0026#34; MYSQL_ROOT_PASSWORD: \u0026#34;root\u0026#34; volumes: \u0026#34;3306\u0026#34;: \u0026#34;3307\u0026#34;: \u0026#34;3308\u0026#34;: \u0026#34;3309\u0026#34;: 使用上面的docker-compose.yml文件，启动数据库服务\n1 docker-compose up 得到4个数据库端口。对应关系如下表\n端口 库名 主/从 3306 orders_0 主 3307 orders_1 主 3308 slave_0 从 3309 slave_1 从 # 建表 在3306和3308端口执行以下SQL语句\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 create database orders_0; use orders_0; SET NAMES utf8mb4; SET FOREIGN_KEY_CHECKS = 0; -- ---------------------------- -- Table structure for t_orders_0 -- ---------------------------- DROP TABLE IF EXISTS `t_orders_0`; CREATE TABLE `t_orders_0` ( `id` bigint(11) NOT NULL AUTO_INCREMENT COMMENT \u0026#39;订单编号\u0026#39;, `user_id` int(16) DEFAULT NULL COMMENT \u0026#39;用户编号\u0026#39;, PRIMARY KEY (`id`) ) ENGINE=InnoDB AUTO_INCREMENT=8 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_bin COMMENT=\u0026#39;订单表\u0026#39;; -- ---------------------------- -- Table structure for t_orders_2 -- ---------------------------- DROP TABLE IF EXISTS `t_orders_2`; CREATE TABLE `t_orders_2` ( `id` bigint(11) NOT NULL AUTO_INCREMENT COMMENT \u0026#39;订单编号\u0026#39;, `user_id` int(16) DEFAULT NULL COMMENT \u0026#39;用户编号\u0026#39;, PRIMARY KEY (`id`) ) ENGINE=InnoDB AUTO_INCREMENT=8 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_bin COMMENT=\u0026#39;订单表\u0026#39;; -- ---------------------------- -- Table structure for t_orders_4 -- ---------------------------- DROP TABLE IF EXISTS `t_orders_4`; CREATE TABLE `t_orders_4` ( `id` bigint(11) NOT NULL AUTO_INCREMENT COMMENT \u0026#39;订单编号\u0026#39;, `user_id` int(16) DEFAULT NULL COMMENT \u0026#39;用户编号\u0026#39;, PRIMARY KEY (`id`) ) ENGINE=InnoDB AUTO_INCREMENT=8 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_bin COMMENT=\u0026#39;订单表\u0026#39;; -- ---------------------------- -- Table structure for t_orders_6 -- ---------------------------- DROP TABLE IF EXISTS `t_orders_6`; CREATE TABLE `t_orders_6` ( `id` bigint(11) NOT NULL AUTO_INCREMENT COMMENT \u0026#39;订单编号\u0026#39;, `user_id` int(16) DEFAULT NULL COMMENT \u0026#39;用户编号\u0026#39;, PRIMARY KEY (`id`) ) ENGINE=InnoDB AUTO_INCREMENT=8 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_bin COMMENT=\u0026#39;订单表\u0026#39;; -- ---------------------------- -- Table structure for order_config -- ---------------------------- DROP TABLE IF EXISTS `t_order_config`; CREATE TABLE `t_order_config` ( `id` int(11) NOT NULL AUTO_INCREMENT COMMENT \u0026#39;编号\u0026#39;, `pay_timeout` int(11) DEFAULT NULL COMMENT \u0026#39;支付超时时间;单位：分钟\u0026#39;, PRIMARY KEY (`id`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_bin COMMENT=\u0026#39;订单配置表\u0026#39;; SET FOREIGN_KEY_CHECKS = 1; 在3307和3309端口执行以下SQL语句\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 create database orders_1; use orders_1; SET NAMES utf8mb4; SET FOREIGN_KEY_CHECKS = 0; -- ---------------------------- -- Table structure for t_orders_1 -- ---------------------------- DROP TABLE IF EXISTS `t_orders_1`; CREATE TABLE `t_orders_1` ( `id` bigint(11) NOT NULL AUTO_INCREMENT COMMENT \u0026#39;订单编号\u0026#39;, `user_id` int(16) DEFAULT NULL COMMENT \u0026#39;用户编号\u0026#39;, PRIMARY KEY (`id`) ) ENGINE=InnoDB AUTO_INCREMENT=400675304294580226 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_bin COMMENT=\u0026#39;订单表\u0026#39;; -- ---------------------------- -- Table structure for t_orders_3 -- ---------------------------- DROP TABLE IF EXISTS `t_orders_3`; CREATE TABLE `t_orders_3` ( `id` bigint(11) NOT NULL AUTO_INCREMENT COMMENT \u0026#39;订单编号\u0026#39;, `user_id` int(16) DEFAULT NULL COMMENT \u0026#39;用户编号\u0026#39;, PRIMARY KEY (`id`) ) ENGINE=InnoDB AUTO_INCREMENT=8 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_bin COMMENT=\u0026#39;订单表\u0026#39;; -- ---------------------------- -- Table structure for t_orders_5 -- ---------------------------- DROP TABLE IF EXISTS `t_orders_5`; CREATE TABLE `t_orders_5` ( `id` bigint(11) NOT NULL AUTO_INCREMENT COMMENT \u0026#39;订单编号\u0026#39;, `user_id` int(16) DEFAULT NULL COMMENT \u0026#39;用户编号\u0026#39;, PRIMARY KEY (`id`) ) ENGINE=InnoDB AUTO_INCREMENT=8 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_bin COMMENT=\u0026#39;订单表\u0026#39;; -- ---------------------------- -- Table structure for t_orders_7 -- ---------------------------- DROP TABLE IF EXISTS `t_orders_7`; CREATE TABLE `t_orders_7` ( `id` bigint(11) NOT NULL AUTO_INCREMENT COMMENT \u0026#39;订单编号\u0026#39;, `user_id` int(16) DEFAULT NULL COMMENT \u0026#39;用户编号\u0026#39;, PRIMARY KEY (`id`) ) ENGINE=InnoDB AUTO_INCREMENT=8 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_bin COMMENT=\u0026#39;订单表\u0026#39;; SET FOREIGN_KEY_CHECKS = 1; # 配置依赖 1 2 3 4 5 6 7 8 9 10 \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.baomidou\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;mybatis-plus-boot-starter\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;3.4.2\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.shardingsphere\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;sharding-jdbc-spring-boot-starter\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;4.1.1\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; # 编写相应的POJO # Entity 1 2 3 4 5 6 7 8 9 10 11 package fun.happyhacker.springbootdemo.mybatis.orders.entity; import com.baomidou.mybatisplus.annotation.TableName; import lombok.Data; @Data @TableName(value = \u0026#34;t_orders\u0026#34;) public class Order { private Integer id; private Integer userId; } 1 2 3 4 5 6 7 8 9 10 11 12 13 package fun.happyhacker.springbootdemo.mybatis.orders.entity; import com.baomidou.mybatisplus.annotation.TableName; import lombok.Data; import java.time.LocalDateTime; @Data @TableName(value = \u0026#34;t_order_config\u0026#34;) public class OrderConfig { private Integer id; private LocalDateTime pay_timeout; } # Mapper 1 2 3 4 5 6 7 8 9 package fun.happyhacker.springbootdemo.mybatis.orders.mapper; import com.baomidou.mybatisplus.core.mapper.BaseMapper; import fun.happyhacker.springbootdemo.mybatis.orders.entity.OrderConfig; import org.apache.ibatis.annotations.Mapper; @Mapper public interface OrderConfigMapper extends BaseMapper\u0026lt;OrderConfig\u0026gt; { } 1 2 3 4 5 6 7 8 9 10 package fun.happyhacker.springbootdemo.mybatis.orders.mapper; import com.baomidou.mybatisplus.core.mapper.BaseMapper; import fun.happyhacker.springbootdemo.mybatis.orders.entity.Order; import org.apache.ibatis.annotations.Mapper; @Mapper public interface OrderMapper extends BaseMapper\u0026lt;Order\u0026gt; { Order selectByUserId(Integer userId); } # XML Mapper 1 2 3 4 5 6 7 \u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;!DOCTYPE mapper PUBLIC \u0026#34;-//mybatis.org//DTD Mapper 3.0//EN\u0026#34; \u0026#34;http://mybatis.org/dtd/mybatis-3-mapper.dtd\u0026#34;\u0026gt; \u0026lt;mapper namespace=\u0026#34;fun.happyhacker.springbootdemo.mybatis.orders.mapper.OrderMapper\u0026#34;\u0026gt; \u0026lt;select id=\u0026#34;selectByUserId\u0026#34; parameterType=\u0026#34;Integer\u0026#34; resultType=\u0026#34;Order\u0026#34;\u0026gt; select * from t_orders where user_id = #{userId} order by id desc limit 1 \u0026lt;/select\u0026gt; \u0026lt;/mapper\u0026gt; # Service 和 ServiceImpl 1 2 3 4 5 6 7 package fun.happyhacker.springbootdemo.mybatis.orders.service; import com.baomidou.mybatisplus.extension.service.IService; import fun.happyhacker.springbootdemo.mybatis.orders.entity.OrderConfig; public interface OrderConfigService extends IService\u0026lt;OrderConfig\u0026gt; { } 1 2 3 4 5 6 7 8 9 10 11 package fun.happyhacker.springbootdemo.mybatis.orders.service; import com.baomidou.mybatisplus.extension.service.IService; import fun.happyhacker.springbootdemo.mybatis.orders.entity.Order; import org.apache.ibatis.annotations.Param; public interface OrderService extends IService\u0026lt;Order\u0026gt; { Order selectByUserId(@Param(\u0026#34;userId\u0026#34;) Integer userId); Order selectAndUpdate(@Param(\u0026#34;userId\u0026#34;) Integer userId); } 1 2 3 4 5 6 7 8 9 10 11 package fun.happyhacker.springbootdemo.mybatis.orders.service.impl; import com.baomidou.mybatisplus.extension.service.impl.ServiceImpl; import fun.happyhacker.springbootdemo.mybatis.orders.entity.OrderConfig; import fun.happyhacker.springbootdemo.mybatis.orders.mapper.OrderConfigMapper; import fun.happyhacker.springbootdemo.mybatis.orders.service.OrderConfigService; import org.springframework.stereotype.Service; @Service public class OrderConfigServiceImpl extends ServiceImpl\u0026lt;OrderConfigMapper, OrderConfig\u0026gt; implements OrderConfigService { } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 package fun.happyhacker.springbootdemo.mybatis.orders.service.impl; import com.baomidou.mybatisplus.extension.service.impl.ServiceImpl; import fun.happyhacker.springbootdemo.mybatis.orders.entity.Order; import fun.happyhacker.springbootdemo.mybatis.orders.mapper.OrderMapper; import fun.happyhacker.springbootdemo.mybatis.orders.service.OrderService; import lombok.extern.log4j.Log4j2; import org.springframework.stereotype.Service; import org.springframework.transaction.annotation.Transactional; @Service @Log4j2 public class OrderServiceImpl extends ServiceImpl\u0026lt;OrderMapper, Order\u0026gt; implements OrderService { @Override public Order selectByUserId(Integer userId) { return baseMapper.selectByUserId(userId); } @Transactional @Override public Order selectAndUpdate(Integer userId) { Order aOrder = new Order(); aOrder.setUserId(20); baseMapper.insert(aOrder); return baseMapper.selectByUserId(20); } } # 应用配置 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 server: port: 8080 spring: shardingsphere: datasource: names: master0, master1, slave0, slave1 master0: type: com.zaxxer.hikari.HikariDataSource driver-class-name: com.mysql.cj.jdbc.Driver jdbcUrl: jdbc:mysql://192.168.99.101:3306/orders_0?charset=utf8mb4\u0026amp;allowPublicKeyRetrieval=true\u0026amp;useSSL=false username: root password: root master1: type: com.zaxxer.hikari.HikariDataSource driver-class-name: com.mysql.cj.jdbc.Driver jdbcUrl: jdbc:mysql://192.168.99.101:3307/orders_1?charset=utf8mb4\u0026amp;allowPublicKeyRetrieval=true\u0026amp;useSSL=false username: root password: root slave0: type: com.zaxxer.hikari.HikariDataSource driver-class-name: com.mysql.cj.jdbc.Driver jdbcUrl: jdbc:mysql://192.168.99.101:3308/orders_0?charset=utf8mb4\u0026amp;allowPublicKeyRetrieval=true\u0026amp;useSSL=false username: root password: root slave1: type: com.zaxxer.hikari.HikariDataSource driver-class-name: com.mysql.cj.jdbc.Driver jdbcUrl: jdbc:mysql://192.168.99.101:3309/orders_1?charset=utf8mb4\u0026amp;allowPublicKeyRetrieval=true\u0026amp;useSSL=false username: root password: root sharding: tables: t_orders: key-generator: column: id type: SNOWFLAKE actual-data-nodes: ds0.t_orders_$-\u0026gt;{[0,2,4,6]}, ds1.t_orders_$-\u0026gt;{[1,3,5,7]} table-strategy: inline: algorithm-expression: t_orders_$-\u0026gt;{user_id % 8} sharding-column: user_id database-strategy: inline: algorithm-expression: ds$-\u0026gt;{user_id % 2} sharding-column: user_id t_order_config: actual-data-nodes: ds0.t_order_config master-slave-rules: ds0: master-data-source-name: master0 slave-data-source-names: slave0 ds1: master-data-source-name: master1 slave-data-source-names: slave1 default-database-strategy: inline: sharding-column: user_id algorithm-expression: master$-\u0026gt;{user_id % 2} props: log: show: true # 在日志中打印执行过程 sql: show: true # 打印Logic SQL和 Actual SQL，非常方便排查问题 mybatis-plus: configuration: map-underscore-to-camel-case: true # 类属性中的驼峰字段到数据库中的下划线字段 global-config: db-config: id-type: none logic-delete-value: 1 logic-not-delete-value: 0 mapper-locations: classpath*:mapper/*.xml # 搜索resources/mapper/目录下的XML文件作为Mapper type-aliases-package: fun.happyhacker.springbootdemo.mybatis.orders # 使XML Mapper中可以直接使用短类名，而不需要包名 上面的配置文件展示了需要和不需要分库分表的情况下的不同配置，理清楚之后也很清晰。一定要注意的是database-strategy的值引用的是spring.shardingsphere.datasource.names中的值，而不是实际的库名。\n*-strategy.algorithm-expression是Groovy脚本，本身Apache ShardingSphere支持两种写法\nmaster${user_id % 2} master$-\u0026gt;{user_id % 2} 但第一种方式在Spring环境中会被认为user_id % 2是一个变量名而不是表达式，所以在Spring环境中需要使用第二种方式。\n# 事务 上面的代码中有一个方法标记了@Transactional，表明是支持事务的。在本例中，插入数据之后后面的方法也会去相应的主库中查找对应的数据而不是去从库中查，你可以把@Transactional注解移除验证一下。\n# 总结 关于数据库的所有内容到这里就结束了，我们从最基础的JDBC开始学习，了解了主流的JPA和MyBatis的基础用法，以及在使用这些框架时实际遇到的问题，后面引入了MyBatis-Plus来消除模板代码，使用内置的方法减少代码量。\n在介绍主从分离和分库分表时我们遇到了挑战，使用Dynamic-Datasource不能解决问题，因此我们引入了Apache ShardingSphere来解决。问中给出了非常详细的代码，希望对你的学习有帮助。\n","date":"2021-02-04T22:43:50+08:00","permalink":"https://blog.happyhacker.fun/p/springboot%E4%BD%BF%E7%94%A8shardingsphere%E9%85%8D%E7%BD%AE%E6%95%B0%E6%8D%AE%E5%BA%93/","title":"Springboot使用Shardingsphere配置数据库"},{"content":"有时候fork了一个项目，过几天发现原项目已经更新了，这时候要保持和原始项目的同步。\n比如原始项目是https://github.com/apache/flink.git，而我fork的项目是git@github.com:lovelock/flink.git，这时在我自己的本地仓库可以执行这些命令\n1 2 3 4 5 git remote add remote https://github.com/apache/flink.git git fetch remote master git merge remote/master # 上面两步也可以简化成 git pull remote master git push origin master 其实我觉得这个功能应该是让Github提供才更合适，干嘛要在本地操作呢？直接加个按钮，同步上游代码不就行了。\n","date":"2021-02-03T12:35:16+08:00","permalink":"https://blog.happyhacker.fun/p/fork%E7%9A%84repo%E4%BF%9D%E6%8C%81%E5%92%8C%E5%8E%9Frepo%E5%90%8C%E6%AD%A5/","title":"Fork的repo保持和原repo同步"},{"content":"Java 中 POJO 的toString()方法和我们预期的 JSON 格式不符合，而如果直接覆盖它写一个生成 JSON 的又不合适，因为当需要那种格式的时候就没得用的，所以本着各司其职的原则，我们来实现一个toJson()方法。\n# 序列化库选择 我比较熟悉的就是 Jackson 和 Gson，其中 Gson 的使用较为简单，所以这里用 Gson 来实现。其实主要原因是 Jackson 在序列化对向的时候会抛出异常，而 Gson 就不会。这里简单验证一下。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 package fun.happyhacker.json; import com.fasterxml.jackson.core.JsonProcessingException; import com.fasterxml.jackson.databind.ObjectMapper; import com.google.gson.Gson; public class JsonTest { public static void main(String[] args) { Employee employee = new Employee(); employee.setAge(10); employee.setId(1); employee.setName(\u0026#34;John\u0026#34;); ObjectMapper objectMapper = new ObjectMapper(); String jackson = \u0026#34;\u0026#34;; try { jackson = objectMapper.writeValueAsString(employee); } catch (JsonProcessingException e) { e.printStackTrace(); } System.out.println(jackson); String gson = new Gson().toJson(employee); System.out.println(gson); System.out.println(new GsonBuilder().serializeNulls().create().toJson(employee)); } } 1 2 3 {\u0026#34;id\u0026#34;:1,\u0026#34;name\u0026#34;:null,\u0026#34;age\u0026#34;:10} {\u0026#34;id\u0026#34;:1,\u0026#34;age\u0026#34;:10} {\u0026#34;id\u0026#34;:1,\u0026#34;name\u0026#34;:null,\u0026#34;age\u0026#34;:10} 所以，起码在序列化 JSON 这方面，可以认为两个库的作用是一致的，但 Gson 用起来很简单。\n# 给每个 POJO 添加toJson方法 所以是不是就要人肉给每个 POJO 添加这个方法了，比如上面提到的Employee\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 package fun.happyhacker.json; import com.google.gson.GsonBuilder; import lombok.Data; @Data public class Employee { private int id; private String name; private int age; public String toJson() { return new GsonBuilder().serializeNulls().create().toJson(this); } } 虽说是可行，但明显有太多了模板代码可以消除了。\n# 利用 lombok 其实这个方法我都是在 lombok 相关的帖子下看到的，考虑到目前 lombok 并没有提供类似@ToJson这种注解，那么可以利用它的ExtensionMethod来实现。\n首先创建一个扩展方法集\n1 2 3 4 5 6 7 8 9 10 11 package fun.happyhacker.json; import com.google.gson.GsonBuilder; import java.io.Serializable; public class Extensions { public static \u0026lt;T extends Serializable\u0026gt; String toJson(T t) { return new GsonBuilder().serializeNulls().create().toJson(t); } } 这里面我加了一点小的限制，让POJO 类必须要可以序列化才能使用toJson()方法。\n然后在需要调用 toJson() 方法的所在的类上加上 @ExtensionMethod 注解\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 package fun.happyhacker.json; import lombok.experimental.ExtensionMethod; @ExtensionMethod({Extensions.class}) public class JsonTest { public static void main(String[] args) { Employee employee = new Employee(); employee.setAge(10); employee.setId(1); employee.setName(\u0026#34;John\u0026#34;); System.out.println(employee.toJson()); } } 这时你会发现 IDEA 识别不了这个toJson()方法，但没关系，它是可以正常执行的，这是 IDEA 的 lombok 扩展不支持而已。在这一点上 Eclipse 已经领先了，虽然在其他所有方面 Eclipse 都是惨遭碾压。\n简单讲就是 lombok 在编译期把employee.toJson()这个方法改写成了new GsonBuilder().serializeNulls().create().toJson(employee)，这也解释了为什么Extensions中的方法需要是静态的。\n# 总结 虽说是解决了一部分问题，但我觉得这个问题解决的不够优雅，按我们正常的思维，这个注解是应该加到Employee这个类上的，而不是加到调用它的类上。所以就很尴尬了。不知道能不能参与到 lombok 官方项目中，给它加上这个功能。\n经过简单的搜索发现这个问题已经被讨论过无数遍了，核心问题是核心开发者认为这个和项目设计之初的目标不符合（我觉得这点站不住脚），他们拿toString()来做比较，说本来每个类也都有toString，只是没有实现，但几乎没有类存在toJson这个方法。其实本质上还是因为实现起来太复杂了，没有一个轻量级、高性能的 JSON 序列化库可以用，虽然 Jackson 和 Gson 都可以达成目的，但他们认为都太重了。\n所以这已经不是技术问题了，而是哲学问题。可能这些“库”作者压根不能理解我们应用开发者的痛点吧。\n","date":"2021-01-21T17:53:05+08:00","permalink":"https://blog.happyhacker.fun/p/%E5%AE%9E%E7%8E%B0-tojson-%E6%96%B9%E6%B3%95/","title":"实现 toJson() 方法"},{"content":"我们知道 Java1.8 推荐使用 LocalDate和LocalDateTime类来处理日期和时间，但之前的版本是用GregorianCalendar。之前一直以为只是易用性上的差别，没想到还有一个神坑。\n# 事情经过 今天发现应用崩了，查日志发现是接收到了一个Sat Jan 01 00:00:00 +0800 0001这样的时间，当我用\n1 LocalDateTime.parse(\u0026#39;Sat Jan 01 00:00:00 +0800 0001\u0026#39;, DateTimeFormatter.ofPattern(\u0026#34;EEE MMM dd HH:mm:ss Z yyyy\u0026#34;)); 来解析的时候报了这样一个错\n1 2 3 4 java.time.format.DateTimeParseException: Text \u0026#39;Sat Jan 01 00:00:00 +0800 0001\u0026#39; could not be parsed: Conflict found: Field DayOfWeek 1 differs from DayOfWeek 6 derived from 0001-01-01 at java.time.format.DateTimeFormatter.createError(DateTimeFormatter.java:1920) ~[?:1.8.0_231] at java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1855) ~[?:1.8.0_231] at java.time.LocalDateTime.parse(LocalDateTime.java:492) ~[?:1.8.0_231] 看起来像是星期一和星期六有冲突，然后我就去百度公元0001年 1 月 1 日到底是星期几，得到的答案有的说是星期一，有的说是星期六，当然是找不到这样一个日历的，网上能找到的日历最多也就到 1900 年，这就勾起了我的兴趣，于是在想这个日期肯定不是人为填写的，那么就只能是因为生成这个时间的方法和我现在用的解析这个时间的方法有出入。业务方很早之前就开始用 Java 了，而我们是最近刚开始用，所以他们即便可能现在用的是 Java8，里面的很多写法应该还是保留了更早的方式，所以我就验证了一下这个日期\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 package fun.happyhacker.springbootdemo; import java.time.LocalDate; import java.time.Month; import java.util.Calendar; import java.util.Date; import java.util.GregorianCalendar; public class DateTest { public static void main(String[] args) { Date birthDay = new GregorianCalendar(1, Calendar.JANUARY, 1).getTime(); System.out.println(birthDay); LocalDate birthDay1 = LocalDate.of(1, Month.JANUARY, 1); System.out.println(birthDay1.getDayOfWeek()); } } 执行一下，神奇的事情发生了\n1 2 Sat Jan 01 00:00:00 CST 1 MONDAY 也就是说在GregorianCalendar认为这天是星期六，而LocalDateTime认为这天是星期一。\n# 总结 从这件事儿得出的结论就是，从 JDK7 升级到 JDK8 的过程中，不光要注意什么语法的问题，说不定还会出现这种历史遗留问题。虽然这个日期比较特殊，但保不齐还有其他的特殊情况。\n","date":"2021-01-21T16:33:43+08:00","permalink":"https://blog.happyhacker.fun/p/java%E6%97%A5%E6%9C%9F%E5%87%BD%E6%95%B0%E7%9A%84%E4%B8%80%E4%B8%AA%E5%9D%91/","title":"Java日期函数的一个坑"},{"content":"简单记录一下 MySQL 服务的安装方法。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 $ brew install mysql mysql-client $ brew services start mysql $ mysql_secure_installation Securing the MySQL server deployment. Connecting to MySQL using a blank password. VALIDATE PASSWORD COMPONENT can be used to test passwords and improve security. It checks the strength of password and allows the users to set only those passwords which are secure enough. Would you like to setup VALIDATE PASSWORD component? Press y|Y for Yes, any other key for No: y There are three levels of password validation policy: LOW Length \u0026gt;= 8 MEDIUM Length \u0026gt;= 8, numeric, mixed case, and special characters STRONG Length \u0026gt;= 8, numeric, mixed case, special characters and dictionary file Please enter 0 = LOW, 1 = MEDIUM and 2 = STRONG: 0 Please set the password for root here. New password: Re-enter new password: Estimated strength of the password: 50 Do you wish to continue with the password provided?(Press y|Y for Yes, any other key for No) : y By default, a MySQL installation has an anonymous user, allowing anyone to log into MySQL without having to have a user account created for them. This is intended only for testing, and to make the installation go a bit smoother. You should remove them before moving into a production environment. Remove anonymous users? (Press y|Y for Yes, any other key for No) : y Success. Normally, root should only be allowed to connect from \u0026#39;localhost\u0026#39;. This ensures that someone cannot guess at the root password from the network. Disallow root login remotely? (Press y|Y for Yes, any other key for No) : y Success. By default, MySQL comes with a database named \u0026#39;test\u0026#39; that anyone can access. This is also intended only for testing, and should be removed before moving into a production environment. Remove test database and access to it? (Press y|Y for Yes, any other key for No) : y - Dropping test database... Success. - Removing privileges on test database... Success. Reloading the privilege tables will ensure that all changes made so far will take effect immediately. Reload privilege tables now? (Press y|Y for Yes, any other key for No) : y Success. All done! 之后就可以通过mysql -uroot -p使用刚刚设置的新密码登录服务了。\n注意要保证/usr/local/var/mysql 目录是空的，不然会有影响。\n","date":"2021-01-21T11:04:09+08:00","permalink":"https://blog.happyhacker.fun/p/%E4%BD%BF%E7%94%A8homebrew%E5%AE%89%E8%A3%85-mysql/","title":"使用homebrew安装 MySQL"},{"content":"这个博客也写了不少东西了，但功能还是有些简陋，比如不支持搜索，也不能评论，甚至不知道有多少人访问过。\n逛知乎的时候无意间看到了 Maupassant 这个主题，总体看上去还挺简洁大方的，主要是信息展示的比较丰富，不像之前的 even 主题，想看标签和分类还要专门去单独的页面。\n所以就开始了改造。\n# 访问计数 这个就比较简单了，之前用的 even 主题里也有这个位置，只是没有打开而已。\n1 2 3 4 5 6 [params] [params.busuanzi] # count web traffic by busuanzi # 是否使用不蒜子统计站点访问量 enable = true siteUV = true sitePV = true pagePV = true # 评论 评论也比较简单，使用的是utteranc\n1 2 3 4 5 6 [params.utteranc] # https://utteranc.es/ enable = true repo = \u0026#34;lovelock/blog-comments\u0026#34; # The repo to store comments issueTerm = \u0026#34;pathname\u0026#34; #表示你选择以那种方式让github issue的评论和你的文章关联。 theme = \u0026#34;github-light\u0026#34; # 样式主题，有github-light和github-dark两种 async = true # 站内搜索 重头戏是站内搜索。本来也没想弄，发现这个主题有这个功能就想着把它搞定。但按主题作者的说法配置完成了之后并没有任何效果，所以我就研究了下 hugo 下实现站内搜索的方案。\n首先是打开\n1 2 [params] localSearch = true 然后在content/search目录下新建index.md文件，并添加以下内容\n1 2 3 4 5 --- title: \u0026#34;搜索\u0026#34; description: \u0026#34;搜索页面\u0026#34; type: \u0026#34;search\u0026#34; --- 做这些的目的是在使用搜索功能的时候可以跳转到${baseURL}/search/q=keyword这个页面，而它对应的页面模板就是single.html。我读了下源码，其实它做的事情是在这个页面 load 完成时从public/index.xml中找到所有文章的标题，然后在标题列表中查找相应的关键词。\n这存在几个问题\n中文分词的支持肯定不好 public/index.html无法下载 # 选型 所以我就转而去 hugo 的官网查解决方案了。查了一圈对 lunr.js 比较感兴趣，好像功能也比较强大，对应也有 hugo 的小工具。说来也搞笑，hugo 本来是要取代 hexo 作为新一代的静态站点生成引擎的，这是 go 语言和 node 的竞争，但当涉及到这个领域时，竟然需要在 hugo 中引入 node 模块来解决。不过还好，我对这个也没有洁癖，能工作就行。\n原理其实和上面描述的差不多，通过hugo-lunr-zh生成一个index.json文件，页面加载的时候把这个文件加载过来，通过 lunr 的搜索功能在里面查到关键词，进而进行下一步的展示。 这个hugo-lunr-zh其实项目本身和 lunr 并无关系，它只是用来生成 lunr 可以识别的数据而已。而要使用hugo-lunr-zh则是为了支持中文分词。\n# 行动 既然改了搜索方案，现有的single.html也肯定不能用了，先 fork 一个。然后npm -g install hugo-lunr-zh，然后在博客的根目录执行hugo-lunr-zh，简直是涕泗横流，跑不通。\n搜索了一下确实这个方案是有些问题的，主要是作者也不更新了。但我觉得这个原理很清晰，于是进入了不断的 debug 阶段。\n在 npm 上的版本是 1.0.3，而我自己魔改成功运行之后才发现 github 上的 master 已经是 2.1.0 了，但我把 master 版本下载下来竟然都无法安装。我也没兴趣研究到底哪里出了问题了。\n遇到了以下问题\n路径解析的不对 在 https 页面上向 http 的接口发起请求被浏览器拦截 在$document.ready(() =\u0026gt; {})中操作lunr 为新方案适配页面 # 1. 路径解析的不对 不知道是为什么作者会设计成这样的，我的文件是在类似post/life/a.md、post/devops/b.md这种路径，为什么它会认为最终的访问路径是posts/a和posts/b呢？这个解决的比较简单，就是破坏了原本的设计，主要是我也不认同它的设计。\n# 2. 在 https 页面上向 http 的接口发起请求被浏览器拦截 这个问题是我这个站点没有使用 https，在配置文件中的baseURL=http://blog.happyhacker.fun，也没有用 https，相应的搜索框的写法是{{ \u0026quot;search/\u0026quot; | absURL }}，经过 hugo 黑盒的解析之后这个完整的链接变成了https://blog.happyhacker.fun/search；由于无法从public目录下载index.json文件，所以我把它放在static/js/目录下了，在页面模板中的写法是{{ \u0026quot;js/index.json\u0026quot; | absURL }}，最终的完成链接却变成了http://blog.happyhacker.fun/js/index.json。\n注意到区别了吗？js 目录下的文件没有 https，而跳转的页面有 https。\n所以我干脆就在配置文件里把baseURL也改成https了，问题就这么解决了。因为所有的请求都变成了 https，虽然我并没有配置证书。\n# 3. 在$document.ready(() =\u0026gt; {})中操作lunr 在完成这块之前我是没想着要用到分词的，只是简单分析了在命令行生成的词表的结构，想着干脆把这些内容用空格分开生成一个大列表，然后用Array.prototype.includes()方法来实现搜索，后来研究了一下发现用 lunr 自身的搜索引擎看起来要强大许多。\n核心代码如下\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 $(document).ready(function () { var q = getUrlParameter(\u0026#34;q\u0026#34;); $(\u0026#34;span.keyword\u0026#34;).text(q); $(\u0026#34;article.post\u0026#34;).remove(); $.ajax({ url: \u0026#39;{{\u0026#34;js/index.json\u0026#34;|absURL}}\u0026#39;, dataType: \u0026#34;json\u0026#34;, success: function (data) { lunr.zh = function () { this.pipeline.reset(); this.pipeline.add(lunr.zh.trimmer, lunr.stopWordFilter, lunr.stemmer); }; lunr.zh.trimmer = function (token) { return token.update((str) =\u0026gt; { if (/[\\u4E00-\\u9FA5\\uF900-\\uFA2D]/.test(str)) return str; return str.replace(/^\\W+/, \u0026#34;\u0026#34;).replace(/\\W+$/, \u0026#34;\u0026#34;); }); }; lunr.Pipeline.registerFunction(lunr.zh.trimmer, \u0026#34;trimmer-zh\u0026#34;); const idx = lunr(function () { this.use(lunr.zh); this.ref(\u0026#34;uri\u0026#34;); this.field(\u0026#34;content\u0026#34;); this.field(\u0026#34;tags\u0026#34;); this.field(\u0026#34;categories\u0026#34;); this.field(\u0026#34;title\u0026#34;); data.forEach(function (e) { this.add(e); }, this); }); const result = idx.search(q); const hitRefs = result.map((e) =\u0026gt; e.ref); result.forEach((e) =\u0026gt; { const item = data.filter((d) =\u0026gt; d.uri == e.ref)[0]; const oriTitle = item[\u0026#34;oriTitle\u0026#34;]; const content = item[\u0026#34;content\u0026#34;]; const title = item[\u0026#34;title\u0026#34;]; const uri = item[\u0026#34;uri\u0026#34;]; const score = result.filter((f) =\u0026gt; uri == f.ref)[0][\u0026#34;score\u0026#34;]; let searchItem = `\u0026lt;article class=\u0026#34;post\u0026#34;\u0026gt;\u0026lt;header\u0026gt;\u0026lt;h1 class=\u0026#34;post-title\u0026#34;\u0026gt;\u0026lt;a href=\u0026#34;` + uri + `\u0026#34;\u0026gt;` + oriTitle + `\u0026lt;/a\u0026gt;\u0026lt;/h1\u0026gt;\u0026lt;/header\u0026gt;`; const pubDate = new Date(item[\u0026#34;date\u0026#34;]); searchItem += `\u0026lt;date class=\u0026#34;post-meta meta-date\u0026#34;\u0026gt;` + pubDate.getFullYear() + `年` + (pubDate.getMonth() + 1) + `月` + pubDate.getDate() + `日\u0026amp;nbsp; 匹配度：` + score + `\u0026lt;/date\u0026gt;`; searchItem += `\u0026lt;div class=\u0026#34;post-content\u0026#34;\u0026gt;` + item[\u0026#34;content\u0026#34;].replace(/\\s*/g, \u0026#34;\u0026#34;).substring(0, 100) + `……\u0026lt;p class=\u0026#34;readmore\u0026#34;\u0026gt;\u0026lt;a href=\u0026#34;` + uri + `\u0026#34;\u0026gt;阅读全文\u0026lt;/a\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;/div\u0026gt;\u0026lt;/article\u0026gt;`; $(\u0026#34;div.res-cons\u0026#34;).append(searchItem); }); }, }); }); 梳理下逻辑就是拿到词表之后在前端构建一个 lunr 搜索引擎，从中搜索想要的结果，然后根据结果中的 ref 再回到原始结果中拿数据，拼页面。\n这里提一点，hugo 的设计还是很灵活的，可以在config.toml中配置customJs可以在所有页面引入一个 js，简直不要太方便。\n# 4. 为新方案适配页面 代码已经在上面贴出来了，比较简单。\n# 总结 开源项目真的是很依赖作者，虽说用的人可能很多，但真正去改它的人可能凤毛麟角。作者一时兴起发起了一个开源项目，后面不想维护了，就把一堆烂摊子丢给了用户。开源免费的东西我们不能要求太多，那就只好自己在前人的基础上研究了，只是要多花些时间。\n本次改造中新建了两个 repo\nMaupassant 主题 hugo-lunr-zh ","date":"2021-01-17T00:11:45+08:00","permalink":"https://blog.happyhacker.fun/p/hugo%E6%94%AF%E6%8C%81%E7%AB%99%E5%86%85%E6%90%9C%E7%B4%A2/","title":"Hugo支持站内搜索"},{"content":"Oracle JDK 现在收费了，macOS 上安装个 JDK 还挺麻烦。\n# TL；DR 简单来讲，可以直接运行brew search openjdk\n这样搜索出来的是 Oracle 发布的 OpenJDK。其中openjdk就是最新版本的，带@的就是指定版本的，其中 8 和 11 是 LTS 版本，所以可以拥有姓名，至于其他的短期版本，这里就干脆也没有了。\n# AdoptOpenJDK 这个名字有点长，其实是 Eclipse 基金会在维护的发行版，它和 Oracle OpenJDK 的关系有点类似于 MIUI 和 Andriod AOSP 的关系，功能上应该是一样的，不过添加了一些特色的功能，如图所示\n它提供了不同的垃圾收集器和所有的版本号，社区应该也比较流行吧（好吧，其实更多人还是会去下载 Oracle JDK，只是它的免费的 JDK1.8 永远的停留在了8u231）。\n参考这个https://github.com/AdoptOpenJDK/homebrew-openjdk\n强烈推荐使用这个脚本\n1 2 3 4 5 jdk() { version=$1 export JAVA_HOME=$(/usr/libexec/java_home -v\u0026#34;$version\u0026#34;); java -version } 可以让你在不同版本的 JDK 中自由切换。\n","date":"2021-01-14T16:28:04+08:00","permalink":"https://blog.happyhacker.fun/p/%E9%80%9A%E8%BF%87-homebrew-%E5%AE%89%E8%A3%85-jdk/","title":"通过 homebrew 安装 JDK"},{"content":"自从知道了Optional的用法，我现在基本上能不用if/else就不用了，但现在发现这东西还是有一些局限性。\n# 例子使用的两个类 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 package fun.happyhacker.optional; public class Person { private int age; private String name; private Book book; public int getAge() { return age; } public void setAge(int age) { this.age = age; } public String getName() { return name; } public void setName(String name) { this.name = name; } public Book getBook() { return book; } public void setBook(Book book) { this.book = book; } } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 package fun.happyhacker.optional; public class Book { private String author; public String getAuthor() { return author; } public void setAuthor(String author) { this.author = author; } public String getIsbn() { return isbn; } public void setIsbn(String isbn) { this.isbn = isbn; } private String isbn; } # 正常的使用场景 1 2 3 4 5 6 Person person = new Person(); person.setAge(20); person.setName(\u0026#34;Frost\u0026#34;); Book book = Optional.ofNullable(person.getBook()).orElse(new Book()); System.out.println(Optional.ofNullable(book.getAuthor()).orElse(\u0026#34;anonymous\u0026#34;)); 很明显这会输出anonymous，但也很明显它的适用场景太局限。\n# 不能支持的场景 在上面的例子中，我需要调用两次Optional.ofNullable方法才能完成最终的判断，那如果我想这么用呢？\n1 2 3 4 5 Person person = new Person(); person.setAge(20); person.setName(\u0026#34;Frost\u0026#34;); String author = Optional.ofNullable(person.getBook().getAuthor()).orElse(\u0026#34;anonymous\u0026#34;); System.out.println(author); 这时就会报错了\n1 2 Exception in thread \u0026#34;main\u0026#34; java.lang.NullPointerException at fun.happyhacker.optional.OptionalTest.main(OptionalTest.java:15) 也就是说Optional.ofNullable的参数中的空值还是不能调别的方法，如果调了还是会抛出异常。 如果只能这么用真就没什么意思了。\n查看源码发现也就真的这样了 调用顺序就是1、2、3，其中没有对任何可能发生的NPE做捕获。\n","date":"2020-12-23T22:27:03+08:00","permalink":"https://blog.happyhacker.fun/p/java-optional-api-examples/","title":"Java Optional Api Examples"},{"content":"Python虽然简单，但长时间不写还是忘，记录一些常用的片段。\n# 1. 脚本中出现中文时报错 python2默认是不识别中文编码的，头部需要这么写\n1 2 #!/usr/bin/env python # -*- coding: utf-8 -*- python3默认就支持UTF-8了，这个也就不需要了。\n# 2. 获取本机IP 1 2 3 4 5 6 7 #!/usr/bin/env python # -*- coding: utf-8 -*- import netifaces as ni def get_hostname(): return ni.ifaddresses(\u0026#39;eth0\u0026#39;)[ni.AF_INET][0][\u0026#39;addr\u0026#39;] # 3. 判断一个包是否已经安装 1 2 3 4 5 6 7 #!/usr/bin/env python # -*- coding: utf-8 -*- import os output = os.popen(\u0026#39;rpm -q jdk1.8\u0026#39;).readlines() for line in output: print(line) os.popen('cmd').readlines()和os.system('cmd')的区别在于前者会把命令的输出保存到output中，而后者则不会，所以如果需要关注命令的输出，就用前面这个。\n# 4. 下载一个文件 1 2 3 4 5 6 #!/usr/bin/env python # -*- coding: utf-8 -*- import urllib urllib.urlretrieve(URL_JDK_8, filename=\u0026#39;/data1/jdk-8u231-linux-x64.rpm\u0026#39;) 如果文件已经存在，则会覆盖\n# 5. 检查文件是否存在 1 2 3 4 5 6 7 #!/usr/bin/env python # -*- coding: utf-8 -*- import os if os.path.isfile(\u0026#39;/path/to/file\u0026#39;): print(\u0026#39;文件存在\u0026#39;) ","date":"2020-11-27T16:37:50+08:00","permalink":"https://blog.happyhacker.fun/p/python%E7%9A%84%E4%B8%80%E4%BA%9B%E5%AE%9E%E7%94%A8%E6%8A%80%E5%B7%A7/","title":"Python的一些实用技巧"},{"content":"Shell总认为很简单，每到写的时候总是不会写。\n这里总结了写shell脚本时经常会遇到的问题。\n# 1. sudo 无法修改文件，而sudo -s切换到root后可以 比如本来是 sudo echo 'hahaha' \u0026gt; /etc/abcde，改成sudo sh -c \u0026quot;echo 'hahaha' \u0026gt; /etc/abcde\u0026quot; 即可。\n# 2. 从可信站点下载脚本直接执行而不需保存要怎么弄？ wget wget -0 - https://a.b.c/trusted-script.sh | sh curl curl -o - https://a.b.c/trusted-script.sh | sh 如果是其他脚本，后面换成相应的解释器如python即可\n","date":"2020-11-27T16:29:38+08:00","permalink":"https://blog.happyhacker.fun/p/shell%E7%9A%84%E4%B8%80%E4%BA%9B%E5%AE%9E%E7%94%A8%E6%8A%80%E5%B7%A7/","title":"Shell的一些实用技巧"},{"content":"macOS上用Vim觉得很卡顿，不流畅，终于找到原因了。\n有两个相关设置 这两个把Key Repeat设置到最快，把Delay Until Repeat设置到最短，但实际上这么设置之后还是不够，再小就不能通过配置页面设置了，只能通过命令行设置了。我觉得合适的配置是这样的\n1 2 defaults write -g InitialKeyRepeat -int 10 defaults write -g KeyRepeat -int 1 这样之后再操作就明显流畅多了。\n","date":"2020-11-21T22:48:25+08:00","permalink":"https://blog.happyhacker.fun/p/macos%E8%AE%BE%E7%BD%AEkey-repeat%E5%92%8Cdelay-until-repeat/","title":"macOS设置Key Repeat和Delay Until Repeat"},{"content":"Java日志真是一个过于复杂的问题了，花了大量的时间在这个本不应该花时间的地方。\n# 开篇 据说上古时期Java标准库内并没有日志类，所以社区就搞出来了log4j，后来Apache想着把log4j合并到JDK中去，但这Java官方肯定不想让别人制定标准而自己做别人标准的遵守者，所以就搞了java.util.logging，后来就越发混乱了，Apache就又搞了个Commons Logging，这应该就是最早的“日志门面”了，它和后来的slf4j解决的问题是一样的。\n想象一下这个场景，你的项目依赖了两个包，其中一个依赖log4j，另外一个依赖jul，但很显然你是想把项目中所有日志放在一起管理的，如果没有一个统一的框架去管理，是很难维护一个大型项目的。\n# 问题初现 那么问题来了，历史原因，别人的包就是不想换日志框架，那不能改变别人就只能改变自己了，所以我们有2个选择：\n把这两个实现选一个作为真正的实现，通过某种方式将另一个日志框架的调用转发到这个真正的实现上 这两个我谁都不用，我要用第三个，所有第三方的库写的日志都转发到我自己用的这个日志框架上 这就是所谓的bridge了，具体来说如下\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 package fun.happyhacker; import java.util.logging.Logger; public class JULTest { private static final Logger LOG = Logger.getLogger(JULTest.class.getCanonicalName()); public static void main(String[] args) { log(); } public static void log() { LOG.info(\u0026#34;hello log from JUL\u0026#34;); } } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 package fun.happyhacker; import org.apache.log4j.Logger; public class Log4jTest { private static final Logger LOG = Logger.getLogger(Log4jTest.class); public static void main(String[] args) { log(); } public static void log() { LOG.info(\u0026#34;hello log from log4j\u0026#34;); } } # 神奇的桥接 据说jul的性能比较差，我不想用，那么就需要把所有对它的调用转发到log4j上，但实际上没有所谓的jul-to-log4j ，但我们可以用jul-to-slf4j，然后再把slf4j的实现绑定到log4j，问题开始变得复杂了。简单画一下\n就现在的例子来说，加上\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.slf4j\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;slf4j-api\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.7.30\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.slf4j\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;jul-to-slf4j\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.7.30\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.slf4j\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;slf4j-log4j12\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.7.30\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; 这些依赖再执行JULTest类，发现还是一样的结果。。。。这是怎么回事儿呢。。。原来它不能自动桥接，还需要做一些改动。这里我们是将JULTest作为依赖使用的，也就是假设我们不能改变它的源码，所以只能改我们自己的应用。我们的应用原本是这样的\n1 2 3 4 5 6 7 8 9 10 11 12 package fun.happyhacker; import org.slf4j.Logger; import org.slf4j.LoggerFactory; public class App { private static final Logger LOG = LoggerFactory.getLogger(App.class); public static void main(String[] args) { JULTest.log(); } } 执行的结果如下 还需要改成这样\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 package fun.happyhacker; import org.slf4j.Logger; import org.slf4j.LoggerFactory; import org.slf4j.bridge.SLF4JBridgeHandler; import java.util.logging.LogManager; public class App { private static final Logger LOG = LoggerFactory.getLogger(App.class); static { LogManager.getLogManager().reset(); SLF4JBridgeHandler.install(); } public static void main(String[] args) { JULTest.log(); } } 官方文档专门强调了一点，jul到slf4j是有很大的性能损失的（本来jul的性能就差，这么一搞就更差了）因为其他的日志框架到slf4j的桥接都是通过重新实现相应的接口来完成的，但因为双亲委派机制的限制，我们是无法重新实现java.util.logging中的接口的，所以实际它是把jul中的LogRecord替换成了slf4j中的等价对象，这层转换在日志这个本不应该消耗太多资源的场景下就消耗了太多的资源。\n好消息是并没有太多的应用使用jul。所以啊，即便你是官方的实现，也不一定那么受欢迎。\n# 延伸 多数情况可能是要用log4j-over-slf4j来将对log4j的直接调用转调到slf4j-api上，然后再通过其他的日志框架，比如logback来写日志。这里可以简单的追踪以下调用过程。\n1 2 3 4 5 6 7 8 9 10 11 package fun.happyhacker; import org.apache.log4j.Logger; public class Log4jTest { private static final Logger LOG = Logger.getLogger(Log4jTest.class); public static void main(String[] args) { LOG.info(\u0026#34;hello from log4j\u0026#34;); } } 配合相应的log4j.properties就能有相应的输出。那么如果我不想改这块代码，而想让它直接通过logback输出，就需要引入另外两个依赖\n1 2 3 4 5 6 7 8 9 10 \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;ch.qos.logback\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;logback-classic\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.2.3\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.slf4j\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;log4j-over-slf4j\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.7.25\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; 这时在上述代码的第9行打个断点进入debug模式，走到第一步Category.java，就会发现上面会有一个提示，如图所示 但当你真的选了第二个源码文件的时候它又会有另一个提示 这说明它在执行的时候还是用的log4j包中的class文件，而不是log4j-over-slf4j。这时候把依赖中的log4j去掉（实际项目中应该是exclude掉）。再次执行就会发现没有这个提示了，取而代之的是代码的调用直接进入了log4j-over-slf4j中的，打开这个包的源码你就会发现它的包结构和log4j是一致的 所以其实就是用log4j-over-slf4j中的类“偷偷”的替换了log4j中的类，其实已经变成了基于slf4j中的实现。\n# 更实际的情况 上面讲述的这种情况很少见，但更常见的是什么呢？其中一个就是配置Spymemcached的日志级别。更复杂的场景在下面。\nSpringBoot可能是现在最常见的应用类型了，我们知道它默认的日志框架是logback。 而我这里还有一个基于Flink的应用，二者要复用一部分代码，也就是说需要在Flink应用里创建一个Spring容器。Spring用的是logback，Flink用的是log4j，又需要在二者中选择一个了。\n但这个问题其实又没有那么复杂，因为他们其实都是通过slf4j写日志的，也就是说我们没有log4j-to-slf4j或者logback-to-slf4j这种转换，而只需要选择一个实现即可。\n正常使用logback的应用会引入logback-classic包，它提供了slf4j-api和log4j-over-slf4j等依赖，所以当logback-classic和slf4j-log4j12同时存在时就会出现下面的报错 也就是说现在classpath中存在了两个LoggerFactoryBinder的实现，我们要做的就是把它们中的屏蔽一个。比如如果你不想用logback，只需要在spring相关依赖中添加\n1 2 3 4 5 6 7 8 9 10 11 \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-web\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.3.1.RELEASE\u0026lt;/version\u0026gt; \u0026lt;exclusions\u0026gt; \u0026lt;exclusion\u0026gt; \u0026lt;groupId\u0026gt;ch.qos.logback\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;logback-classic\u0026lt;/artifactId\u0026gt; \u0026lt;/exclusion\u0026gt; \u0026lt;/exclusions\u0026gt; \u0026lt;/dependency\u0026gt; 相应的，如果要用logback而不是log4j，就把slf4j-log4j12排除即可。\n# 一点提示 前面我们说到了，如果想把直接调用log4j的请求转发到slf4j-api上，再根据实际情况决定最终底层用哪个日志框架。而slf4j-log4j12就是那个决定让slf4j使用log4j写日志的包，那么如果这两个包同时出现又会怎么样呢？\n1 2 3 4 5 6 7 8 9 10 11 12 \u0026lt;dependencies\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.slf4j\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;slf4j-log4j12\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.7.30\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.slf4j\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;log4j-over-slf4j\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.7.30\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;/dependencies\u0026gt; 再执行App，就会像下面这样崩溃了 简单说就是出现了死循环，要出现栈溢出错误了。道理很简单，这里不再赘述了。\n# 总结 这篇基本就把Java日志里出现的几种情况解释清楚了。下面会开一篇具体说说这几种日志框架各自的配置方式。\n","date":"2020-11-21T15:21:05+08:00","permalink":"https://blog.happyhacker.fun/p/java%E6%97%A5%E5%BF%97%E6%BC%94%E5%8C%96%E6%A2%B3%E7%90%86/","title":"Java日志演化梳理"},{"content":"用这个net.spy.memcached最恶心的事情就是它的日志了，不管三七二十一先打印一组红色的INFO级别日志。\n之前也没有研究这个原因，最近在总结日志相关的坑，就把这里详细看了一下。\n# 观察现象 首先是这个日志是红色的，INFO级别，而且在log4j.properties中添加\n1 2 log4j.logger.net.spy.memcached=ERROR, console log4j.logger.addivitity.*=false 是不起作用的。\n# 分析原因 由于之前对log4j的配置文件也不太熟悉，所以一直想着是自己的配置文件没有写对导致的，而忽略了其他原因。昨天弄明白了如果要改变某个指定的包的日志配置就是这样做，所以就确定了配置文件没有问题。那么原因就只可能是这个包记录日志根本就没有用log4j。那它用的是啥呢？不想翻文档就只能debug了。\n首先找到记日志的地方，打个断点，执行程序，一步一步往下跟，找到这里\n很明显具体拿到的Logger类就是在这里决定的了，这段简单来说就是如果系统没有设置net.spy.log.LoggerImpl属性，就用默认的DefaultLogger，实际上跟到这里确实也发现就是没有设置这个属性，从而className拿到的是个空，所以也就没有log4j什么事儿了。\n# 改造方案 既然知道了问题的根源，那么我们就设置一下这个属性就行了，它是从System.getProperty()方法获取的，那么我们就从System.setProperty()方法设置它。那么要设置成什么呢？打开这个net.spy.log目录，就会发现它提供了几个默认的实现\n很明显我们要找的就是net.spy.memcached.compat.log.Log4jLogger，所以只需要在程序入口加上这行\n1 System.setProperty(\u0026#34;net.spy.log.LoggerImpl\u0026#34;, \u0026#34;net.spy.memcached.compat.log.Log4JLogger\u0026#34;); 这个很可能是slf4j这种日志门面出现之前的一种自己实现的方案，而slf4j-api/log4j-api就是解决这个问题的了。\n即可。加上之后再运行程序就会发现颜色已经和其他的日志一样了。\n# 总结 只要把问题回归到我们会的问题上，后面的问题就很容易解决了，前面已经配置了log4j.properties，所以当然也就可以方便的控制它的日志级别了。\n","date":"2020-11-21T14:14:29+08:00","permalink":"https://blog.happyhacker.fun/p/%E9%85%8D%E7%BD%AEspymemcached%E7%9A%84%E6%97%A5%E5%BF%97%E7%BA%A7%E5%88%AB/","title":"配置Spymemcached的日志级别"},{"content":"用默认的archetype太烦了，每次都要改很多东西，不如自己创建一个。\n# 为什么要自己定义archetype 当然是因为懒。我一般是用maven-archetype-quickstart创建项目，但每次创建完之后还要改一通，比如JDK版本号、maven插件等等，这些东西本身其实不应该花太多时间去记的。所以自己定义一个就好了。\n# 定义archetype的步骤 # 生成模板 从maven-archetype-quickstart创建一个正常的maven项目 这个maven项目是什么不重要，重要的是它会帮你生成一个标准的pom.xml文件。\n修改pom.xml文件 比如你几乎所有的项目都需要依赖log4j2这套日志套件，那么你就需要这两个依赖\n1 2 3 4 5 6 7 8 9 10 11 12 \u0026lt;dependencies\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.logging.log4j\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;log4j-api\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.14.0\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.logging.log4j\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;log4j-core\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.14.0\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;/dependencies\u0026gt; 那么你就可以在这个生成的pom.xml文件中加上这两个依赖。\n执行mvn archetype:create-from-project 这会在target目录中生成一个新的maven项目，进入这个项目，执行mvn clean install，你就把新创建的archetype安装到本地了。\n使用新创建的模板 执行mvn archetype:generate -DarchetypeCatalog=local，会产生一个交互\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 ➜ my-project mvn archetype:generate -DarchetypeCatalog=local [INFO] Scanning for projects... [INFO] [INFO] ---------------------\u0026lt; fun.happyhacker:my-project \u0026gt;--------------------- [INFO] Building my-project 1.0-SNAPSHOT [INFO] --------------------------------[ jar ]--------------------------------- [INFO] [INFO] \u0026gt;\u0026gt;\u0026gt; maven-archetype-plugin:3.1.2:generate (default-cli) \u0026gt; generate-sources @ my-project \u0026gt;\u0026gt;\u0026gt; [INFO] [INFO] \u0026lt;\u0026lt;\u0026lt; maven-archetype-plugin:3.1.2:generate (default-cli) \u0026lt; generate-sources @ my-project \u0026lt;\u0026lt;\u0026lt; [INFO] [INFO] [INFO] --- maven-archetype-plugin:3.1.2:generate (default-cli) @ my-project --- [INFO] Generating project in Interactive mode [INFO] No archetype defined. Using maven-archetype-quickstart (org.apache.maven.archetypes:maven-archetype-quickstart:1.0) Choose archetype: 1: local -\u0026gt; org.apache.flink:flink-quickstart-java (flink-quickstart-java) 2: local -\u0026gt; org.apache.maven.archetypes:maven-archetype-quickstart (quickstart) 3: local -\u0026gt; fun.happyhacker:my-quickstart-1.8-archetype (my-quickstart-1.8) 4: local -\u0026gt; fun.happyhacker:archetype-template-archetype (archetype-template) Choose a number or apply filter (format: [groupId:]artifactId, case sensitive contains): 2: 这时就可以在其中找到刚刚创建的archetype了，在这里就是第4个，所以在交互界面填4，就可以继续选择域名和项目名称，生成一个新的maven项目，在生成的项目中你就会发现你刚刚在第2步中添加的两个依赖已经包含在里面了。\n同样的道理，也可以在pom.xml中添加相关的maven插件，以使从模板创建的项目更符合平时的需求。\n","date":"2020-11-18T23:03:17+08:00","permalink":"https://blog.happyhacker.fun/p/%E5%88%9B%E5%BB%BA%E4%B8%80%E4%B8%AA%E8%87%AA%E5%AE%9A%E4%B9%89%E7%9A%84archetype/","title":"创建一个自定义的Archetype"},{"content":"一直以来我都有个疑问，就是为什么所有操作系统的网络代理设置都不能保存。\n以macOS为例，如果我设置了代理地址，而没有勾选左边的勾选项，那么保存之后再打开也同时没有了已经设置好的代理地址。 而我想要的是可以设置好一个固定的代理地址，我需要开启的时候在一个很明显的入口点一下，就配置上了。不想用的时候关掉即可，不用清空配置。然而我搜遍了全网也没有找到。\n于是我就想macOS系统既然脱胎于BSD，那会不会有相应的命令行可以做这件事儿呢，果然被我找到了一系列命令networksetup。\n先看一下这个命令都有什么功能吧，\n1 networksetup -help 输出太多了，这里就不贴了，可以自己尝试一下。我们关注的是代理（Proxy），所以在输出的信息里搜索proxy，就会找到以下内容\n1 2 3 4 5 6 7 8 Usage: networksetup -setautoproxyurl \u0026lt;networkservice\u0026gt; \u0026lt;url\u0026gt; Set proxy auto-config to url for \u0026lt;networkservice\u0026gt; and enable it. Usage: networksetup -getautoproxyurl \u0026lt;networkservice\u0026gt; Display proxy auto-config (url, enabled) info for \u0026lt;networkservice\u0026gt;. Usage: networksetup -setautoproxystate \u0026lt;networkservice\u0026gt; \u0026lt;on off\u0026gt; Set proxy auto-config to either \u0026lt;on\u0026gt; or \u0026lt;off\u0026gt;. 很明显也就看出来各自的用途了，知道这个了就很容写出两个脚本了\n由于我的是黑苹果，使用的是有线网络，所以这里的网络名是en0，你的电脑用的是什么网络需要你自己去发现了\n# 设置代理 1 2 3 4 5 6 7 8 9 10 11 #!/usr/bin/env bash echo \u0026#34;Enabling auto proxy....\u0026#34; networksetup -setautoproxyurl \u0026#34;Ethernet Adaptor (en0)\u0026#34; \u0026#34;http://your.host/proxy.pac\u0026#34; networksetup -setautoproxystate \u0026#34;Ethernet Adaptor (en0)\u0026#34; on networksetup -getautoproxyurl \u0026#34;Ethernet Adaptor (en0)\u0026#34; if [ $? -eq 0 ]; then echo \u0026#34;Proxy is set\u0026#34; fi # 关掉代理 1 2 3 4 5 6 7 8 9 #!/usr/bin/env bash echo \u0026#34;Disabling auto proxy...\u0026#34; networksetup -setautoproxystate \u0026#34;Ethernet Adaptor (en0)\u0026#34; off networksetup -getautoproxyurl \u0026#34;Ethernet Adaptor (en0)\u0026#34; if [ $? -eq 0 ]; then echo \u0026#34;Proxy is disabled\u0026#34; fi # 设置快捷方式 把这分别保存为disable_proxy和enable_proxy，放在$HOME/.local/bin/目录下，然后在你的$HOME/.zshrc或$HOME/.bashrc的最后一行追加\n1 export PATH=$PATH:$HOME/.local/bin 再执行source ~/.zshrc或source ~/.bashrc，就可以生效了。之后就可以通过执行enable_proxy开启代理，通过disable_proxy来关闭代理了。再也不用找那么深的入口去一遍遍的配置代理了。\n","date":"2020-11-17T22:34:31+08:00","permalink":"https://blog.happyhacker.fun/p/macos-%E7%BD%91%E7%BB%9C%E4%BB%A3%E7%90%86%E8%AE%BE%E7%BD%AE%E8%84%9A%E6%9C%AC/","title":"macOS 网络代理设置脚本"},{"content":"刚看完《Java高并发编程详解——多线程与架构设计》的第10章，实验一下类似Tomcat的热加载的方法。\n这一章的主要内容如下图\n# 背景 讲到基于类的加载和卸载来实现功能热更新时，我就想到了Tomcat可以检测webapps中的war包的变化来重新加载新的应用，所以就通过本章介绍的内容扩展一下，自己实现一个热加载功能。\n# 需求 当指定目录下的class文件发生变化时，系统能及时感知并重新加载。\n# 实现 指定一个存放class文件的目录，如/tmp/classloader/ 在其中放一个class文件，如Child.class，开始时只有一个walk()方法 系统每隔1秒中检测一次该目录中文件是否更新，如果更新了则重新加载 看是否可以通过反射拿到新类的方法列表 # 源码 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 package fun.happyhacker; import java.io.ByteArrayOutputStream; import java.io.IOException; import java.nio.file.Files; import java.nio.file.Path; import java.nio.file.Paths; public class MyClassLoader extends ClassLoader { private static final Path CLASS_DIR = Paths.get(\u0026#34;/tmp/classloader/\u0026#34;); private final Path classDir; public MyClassLoader() { super(); this.classDir = CLASS_DIR; } public MyClassLoader(String classDir) { this.classDir = Paths.get(classDir); } public MyClassLoader(ClassLoader parent) { super(parent); this.classDir = CLASS_DIR; } public MyClassLoader(ClassLoader parent, String classDir) { super(parent); this.classDir = Paths.get(classDir); } @Override protected Class\u0026lt;?\u0026gt; findClass(String name) throws ClassNotFoundException { byte[] classBytes = this.readClassBytes(name); if (classBytes.length == 0) { throw new ClassNotFoundException(\u0026#34;Can not load the class \u0026#34; + name); } return this.defineClass(name, classBytes, 0, classBytes.length); } private byte[] readClassBytes(String name) throws ClassNotFoundException { String classPath = name.replace(\u0026#34;.\u0026#34;, \u0026#34;/\u0026#34;); Path classFullPath = classDir.resolve(Paths.get(classPath + \u0026#34;.class\u0026#34;)); if (!classFullPath.toFile().exists()) { throw new ClassNotFoundException(\u0026#34;The class \u0026#34; + name + \u0026#34; not found\u0026#34;); } try (ByteArrayOutputStream baos = new ByteArrayOutputStream()) { Files.copy(classFullPath, baos); return baos.toByteArray(); } catch (IOException e) { throw new ClassNotFoundException(\u0026#34;load the class \u0026#34; + name + \u0026#34; occur error.\u0026#34;, e); } } @Override public String toString() { return \u0026#34;MyClassLoader\u0026#34;; } } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 package fun.happyhacker; import java.lang.reflect.InvocationTargetException; import java.lang.reflect.Method; import java.nio.file.Path; import java.nio.file.Paths; import java.util.concurrent.TimeUnit; public class Daemon { private static final String CLASS_PATH = \u0026#34;/tmp/classloader/\u0026#34;; private static final String CLASS_NAME = \u0026#34;a.b.c.Child\u0026#34;; private static final int CHECK_INTERVAL = 5; private long lastModified; private ClassLoader classLoader; private Class\u0026lt;?\u0026gt; klass; public static void main(String[] args) throws InterruptedException, ClassNotFoundException, NoSuchMethodException, InvocationTargetException, InstantiationException, IllegalAccessException { Daemon daemon = new Daemon(); daemon.listen(); } private void run() throws ClassNotFoundException, IllegalAccessException, InstantiationException, NoSuchMethodException, InvocationTargetException { if (classLoader == null) { classLoader = new MyClassLoader(); } if (klass == null) { klass = classLoader.loadClass(CLASS_NAME); } Class\u0026lt;?\u0026gt; child = klass; System.out.println(child.getClassLoader()); Object instance = child.newInstance(); System.out.println(instance); Method[] methods = child.getDeclaredMethods(); for (Method method : methods) { Method m = child.getMethod(method.getName()); m.invoke(instance); } } private void listen() throws InterruptedException, ClassNotFoundException, NoSuchMethodException, InvocationTargetException, InstantiationException, IllegalAccessException { while (true) { Path classPath = Paths.get(CLASS_PATH); Path classFullPath = classPath.resolve(Paths.get(CLASS_NAME.replace(\u0026#34;.\u0026#34;, \u0026#34;/\u0026#34;) + \u0026#34;.class\u0026#34;)); System.out.println(\u0026#34;class path \u0026#34; + classFullPath.toFile().getAbsolutePath()); long newLastModified = classFullPath.toFile().lastModified(); System.out.println(\u0026#34;exists \u0026#34; + classFullPath.toFile().exists()); System.out.println(\u0026#34;new last modified: \u0026#34; + newLastModified); if (newLastModified \u0026gt; lastModified) { reload(); run(); } lastModified = newLastModified; TimeUnit.SECONDS.sleep(CHECK_INTERVAL); } } private void reload() { classLoader = null; klass = null; } } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 package a.b.c; public class Child { public void walk() { System.out.println(\u0026#34;I can walk\u0026#34;); } public void speak() { System.out.println(\u0026#34;I can talk\u0026#34;); } public void write() { System.out.println(\u0026#34;I can write\u0026#34;); } } 在/tmp/classloader/a/b/c中修改Child.java，修改之后执行javac Child.java会引起Child.class文件的变化，从而触发系统自动重新加载新的class文件，执行新的方法。\n# 总结 对于Tomcat会更复杂一些，但也就是把加载一个单独的class文件升级成加载一个war包，原理是一样的。\n不过我不太理解的是，为什么Tomcat不能实现热更新，也就是为什么不能像Nginx那样有一段时间是新老服务共存，等已经连接到老的服务上的请求完成之后再停掉老服务呢？这个答案只能从Tomcat的源码中寻找了。\n","date":"2020-10-27T22:04:17+08:00","permalink":"https://blog.happyhacker.fun/p/jvm%E7%B1%BB%E5%8A%A0%E8%BD%BD%E5%99%A8/","title":"JVM类加载器"},{"content":"Springboot的这个profiles的问题真是让人头疼。\n这个问题在小版本之间来瞎改，又没有明确的说明，不知道浪费了多少人的时间。\n首先明确一点，通过\n1 export SPRING_PROFILES_ACTIVE=prod,web 这种方式从始至终都是可行的。而改变的是\n1 mvn spring-boot:run -Dspring.profiles.active=prod,web 这种方式。亲测在springboot 2.3.1已经完全不起作用了。所以保险的方法就是前面提到的第一种。可以这么做\n1 export SPRING_PROFILES_ACTIVE=prod,web \u0026amp;\u0026amp; mvn spring-boot:run ","date":"2020-10-22T16:32:44+08:00","permalink":"https://blog.happyhacker.fun/p/springboot-profiles-active/","title":"Springboot Profiles Active"},{"content":"服务器端没有可视化界面，监控这种事情看CLI界面还是差点意思。\n# 方法 需要看Tomcat的监控，需要远程连接之，只需要在tomcat启动前在$TOMCAT_HOME/bin目录下添加setenv.sh文件，加上以下内容即可\n1 2 3 4 5 JAVA_OPTS=\u0026#34;-Dcom.sun.management.jmxremote=true \\ -Dcom.sun.management.jmxremote.port=9090 \\ -Dcom.sun.management.jmxremote.ssl=false \\ -Dcom.sun.management.jmxremote.authenticate=false \\ -Djava.rmi.server.hostname=current ip\u0026#34; 这样就可以在本地看到这种监控图了 # 扩展 既然Tomcat是这么做，其实我们自己的应用当然也可以，只需要在启动的时候加上这些参数即可。\n1 2 3 4 5 6 7 ➜ tomcat vim Hello.java ➜ tomcat javac Hello.java ➜ tomcat java -Dcom.sun.management.jmxremote=true \\ -Dcom.sun.management.jmxremote.port=9090 \\ -Dcom.sun.management.jmxremote.ssl=false \\ -Dcom.sun.management.jmxremote.authenticate=false \\ -Djava.rmi.server.hostname=10.75.1.42 Hello 果然是可以的。\n# 总结 其实很多开源的Java软件，都有类似这种做法，比如Apache Flink用$FLINK_HOME/bin/config.sh等，通过这种方式可以很容易的设置一些环境变量。\n","date":"2020-08-22T17:25:53+08:00","permalink":"https://blog.happyhacker.fun/p/%E4%BD%BF%E7%94%A8visualvm%E7%9B%91%E6%8E%A7%E8%BF%9C%E7%A8%8Bjvm%E8%BF%9B%E7%A8%8B/","title":"使用VisualVM监控远程JVM进程"},{"content":"本文使用docker环境。\n# 修改软件源 1 2 sed -i \u0026#39;s/deb.debian.org/mirrors.aliyun.com/g\u0026#39; /etc/apt/sources.list sed -i \u0026#39;s/security.debian.org/mirrors.aliyun.com/g\u0026#39; /etc/apt/sources.list # 安装JDK和maven 1 2 apt update apt install openjdk-11-jdk-headless maven vim git curl wget # 配置阿里云maven加速 参考使用阿里云加速依赖管理\n# 创建一个Java应用 # 使用maven创建一个应用骨架 1 mvn archetype:generate -DgroupId=fun.happyhacker -DartifactId=spring-demo -DarchetypeArtifactId=maven-archetype-quickstart -DarchetypeVersion=1.4 -DinteractiveMode=false # 总结 在一个全新的机器上搭建Java开发和运行环境，主要还是网络问题，使用阿里云的加速服务能极大的提高使用体验。\n","date":"2020-08-22T11:57:03+08:00","permalink":"https://blog.happyhacker.fun/p/debian-linux%E5%BF%AB%E9%80%9F%E6%90%AD%E5%BB%BAjava%E8%BF%90%E8%A1%8C%E7%8E%AF%E5%A2%83/","title":"Debian Linux快速搭建Java运行环境"},{"content":"最终还是得解决Springboot应用热部署的问题。\n# 背景 基于Springboot应用以war包的形式运行在tomcat容器中，当更新war包时会有一段时间服务返回404，这对于线上服务是不可接受的。4层的负载均衡可以自动将80端口关闭的节点下线，但由于内网服务器位于堡垒机后方，根据公司规定不能自行配置SSH服务，所以无法执行远程脚本。所以只能通过别的方式实现。\n# 实验素材 nginx 作为web server和7层负载均衡 tomcat * 2 作为应用后端 gitlab-ce 代码版本控制 jenkins 发布平台 # 基本原理 基本的原理就是让Nginx后方有2个Tomcat容器，其中1个是active，1个是backup，正常情况下不会访问到backup的容器，但可以通过额外的手段保证backup的容器是可以提供服务的，在发布前先更新所有的backup节点，验证没问题之后更新active的容器，来保证服务不会中断。\n# 实际操作 # 创建springboot项目 参考Springboot使用内置和独立tomcat以及其他思考。\n# 编写同一个接口的不同版本 1 2 3 4 5 6 7 8 // tag v1 @RestController public class HelloController { @GetMapping(\u0026#34;/hello\u0026#34;) public String hello() { return \u0026#34;V1\u0026#34;; } } 1 2 3 4 5 6 7 8 // tag v2 @RestController public class HelloController { @GetMapping(\u0026#34;/hello\u0026#34;) public String hello() { return \u0026#34;V2\u0026#34;; } } # 打包 1 mvn clean package -Dmaven.test.skip=true # 创建两个tomcat容器 1 2 docker run -itd --name tomcat-active -v /tmp/tomcat/active:/usr/local/tomcat/webapps -p 32771:8080 tomcat docker run -itd --name tomcat-backup -v /tmp/tomcat/backup:/usr/local/tomcat/webapps -p 32772:8080 tomcat # 将war包拷贝到容器中 可能是docker toolbox的问题，无法挂载目录，所以只好把war包手动拷贝进去。\n1 2 docker cp ~/workspace/spring-demo/target/spring-demo-0.0.1-SNAPSHOT.war tomcat-active:/usr/local/tomcat/webapps/ docker cp ~/workspace/spring-demo/target/spring-demo-0.0.1-SNAPSHOT.war tomcat-backup:/usr/local/tomcat/webapps/ # 访问两个容器中的服务 稍等片刻两个容器中的服务会自动部署，就可以分别通过相应的端口访问了，简单压测一下QPS可以达到2000+且没有报错。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 $ wrk -c 20 -d 10 -t 4 http://192.168.99.100:32771/spring-demo-0.0.1-SNAPSHOT/hello Running 10s test @ http://192.168.99.100:32771/spring-demo-0.0.1-SNAPSHOT/hello 4 threads and 20 connections Thread Stats Avg Stdev Max +/- Stdev Latency 10.20ms 8.70ms 122.66ms 81.20% Req/Sec 554.18 167.66 1.04k 63.25% 22088 requests in 10.02s, 2.43MB read Requests/sec: 2203.76 Transfer/sec: 247.89KB $ wrk -c 20 -d 10 -t 4 http://192.168.99.100:32772/spring-demo-0.0.1-SNAPSHOT/hello Running 10s test @ http://192.168.99.100:32772/spring-demo-0.0.1-SNAPSHOT/hello 4 threads and 20 connections Thread Stats Avg Stdev Max +/- Stdev Latency 11.30ms 14.24ms 186.52ms 92.95% Req/Sec 557.54 207.91 1.24k 67.17% 22025 requests in 10.03s, 2.42MB read Requests/sec: 2196.36 Transfer/sec: 247.05KB # 配置Nginx 1 2 3 4 5 6 7 8 9 10 11 12 13 upstream ha { server 192.168.99.100:32771; server 192.168.99.100:32772 backup; } server { listen 80; server_name _; location / { proxy_next_upstream http_502 http_504 http_404 error timeout invalid_header; proxy_pass http://ha/spring-demo-0.0.1-SNAPSHOT/; } } 注意：默认情况下只会转发GET/HEAD/PUT/DELETE/OPTIONS这种幂等的请求，而不会转发POST请求，如果需要对POST请求也做转发，就需要加上non_idempotent配置，整体配置如下\n1 2 3 4 5 6 7 8 9 10 11 12 13 upstream ha { server 192.168.99.100:32771; server 192.168.99.100:32772 backup; } server { listen 80; server_name _; location / { proxy_next_upstream http_502 http_504 http_404 error timeout invalid_header non_idempotent; proxy_pass http://ha/spring-demo-0.0.1-SNAPSHOT/; } } 注意proxy_next_upstream http_502 http_504 http_404 error timeout invalid_header;这行，这里就是表示把访问当前的upstream返回了这些状态码的请求转发到upstream中的下一台机器，在我们现在的应用场景下，当war包发布时，正在更新war包的tomcat会返回404，也就是对应http_404，如果不配置这行，是不会做转发的。 但这样简单的配置还会有一个问题，那就是Nginx不会把出问题的后端从upstream中摘除，也就是说请求还会访问到这个正在更新中的realserver，只是Nginx会再把请求转发到下一台好的realserver上，这样会增加一些耗时。目前有三种方式可以实现对Nginx负载均衡的后端节点服务器进行健康检查，具体参考Nginx负载均衡\n# 通过Nginx压测 # 基本测试 两个tomcat节点均正常的情况下压测 1 2 3 4 5 6 7 8 9 $ wrk -c 20 -d 10 -t 4 http://192.168.99.100:32778/hello Running 10s test @ http://192.168.99.100:32778/hello 4 threads and 20 connections Thread Stats Avg Stdev Max +/- Stdev Latency 57.36ms 32.06ms 335.36ms 71.29% Req/Sec 89.29 48.20 390.00 85.25% 3577 requests in 10.05s, 562.30KB read Requests/sec: 355.77 Transfer/sec: 55.93KB 和上面没有经过Nginx的压测相比，最明显的变化就是QPS下降了84%，平均响应时间增加了5倍，猜测可能是因为Nginx使用的默认配置中worker_processes 1;的问题。\n在开始压测后立即删除tomcat-active容器中的war包和目录，结果如下 1 2 3 4 5 6 7 8 9 $ wrk -c 20 -d 10 -t 4 http://192.168.99.100:32778/hello Running 10s test @ http://192.168.99.100:32778/hello 4 threads and 20 connections Thread Stats Avg Stdev Max +/- Stdev Latency 57.29ms 28.69ms 181.88ms 67.38% Req/Sec 87.93 39.51 240.00 75.25% 3521 requests in 10.05s, 553.50KB read Requests/sec: 350.22 Transfer/sec: 55.05KB 同样没有非200的响应，而且整体和正常情况相当。\n只有backup节点工作的情况下压测 1 2 3 4 5 6 7 8 9 $ wrk -c 20 -d 10 -t 4 http://192.168.99.100:32778/hello Running 10s test @ http://192.168.99.100:32778/hello 4 threads and 20 connections Thread Stats Avg Stdev Max +/- Stdev Latency 72.12ms 35.99ms 240.89ms 68.34% Req/Sec 70.04 31.84 180.00 76.50% 2810 requests in 10.05s, 441.71KB read Requests/sec: 279.48 Transfer/sec: 43.93KB 可以看到，响应时间有明显的增加，QPS也有明显的下降，也验证了上面说的响应是404的请求会被转发到正常工作的节点，但有问题的节点不会被摘除导致的响应时间变长的问题。\n# 进一步测试 为了消除上面测试中可能存在war包删除后对服务的影响还没有生效，压测就已经结束的可能，将压测时间调长，增加至60s。\n两个节点都正常的情况 1 2 3 4 5 6 7 8 9 $ wrk -c 20 -d 60 -t 4 http://192.168.99.100:32778/hello Running 1m test @ http://192.168.99.100:32778/hello 4 threads and 20 connections Thread Stats Avg Stdev Max +/- Stdev Latency 55.53ms 28.10ms 306.58ms 70.07% Req/Sec 91.52 39.35 300.00 69.23% 21906 requests in 1.00m, 3.36MB read Requests/sec: 364.66 Transfer/sec: 57.32KB 整体情况和上面10s的测试相同。查看日志发现backup节点没有接收到任何请求。为了验证是否是worker_processes配置导致的，把这个值改成4之后重新测试，结果如下\n1 2 3 4 5 6 7 8 9 $ wrk -c 20 -d 60 -t 4 http://192.168.99.100:32778/hello Running 1m test @ http://192.168.99.100:32778/hello 4 threads and 20 connections Thread Stats Avg Stdev Max +/- Stdev Latency 41.55ms 24.92ms 227.15ms 72.21% Req/Sec 125.06 46.88 373.00 71.76% 29922 requests in 1.00m, 4.59MB read Requests/sec: 498.11 Transfer/sec: 78.29KB 可以看到，有了将近20%的提升，但还是不太符合预期。\n开始测试后立即更新active节点的war包 1 2 3 4 5 6 7 8 9 $ wrk -c 20 -d 60 -t 4 http://192.168.99.100:32778/hello Running 1m test @ http://192.168.99.100:32778/hello 4 threads and 20 connections Thread Stats Avg Stdev Max +/- Stdev Latency 54.40ms 33.76ms 329.73ms 70.53% Req/Sec 95.85 56.28 420.00 81.60% 22914 requests in 1.00m, 3.52MB read Requests/sec: 381.42 Transfer/sec: 59.95KB 没有明显变化，测试开始后有一段时间backup节点收到请求，后面请求又全部指向了active节点。可能是因为服务太简单，重新加载的太快，只有很少量（5750）的请求转发到了backup节点，所以对整体结果影响不大。 3. 开始测试后立即删除active节点的war包\n1 2 3 4 5 6 7 8 9 $ wrk -c 20 -d 60 -t 4 http://192.168.99.100:32778/hello Running 1m test @ http://192.168.99.100:32778/hello 4 threads and 20 connections Thread Stats Avg Stdev Max +/- Stdev Latency 72.11ms 34.33ms 346.24ms 69.54% Req/Sec 70.16 29.78 191.00 67.23% 16813 requests in 1.00m, 2.58MB read Requests/sec: 279.84 Transfer/sec: 43.99KB 删除节点后，所有的请求都会先请求active，然后被Nginx转发至backup，所以吞吐量有明显下降，延迟也有明显的提升。\n# 效果测试 直接访问active 1 2 3 4 5 6 7 8 9 $ wrk -c 20 -d 60 -t 4 http://10.75.1.42:28080/web-0.0.1-SNAPSHOT/hello Running 1m test @ http://10.75.1.42:28080/web-0.0.1-SNAPSHOT/hello 4 threads and 20 connections Thread Stats Avg Stdev Max +/- Stdev Latency 5.56ms 25.16ms 203.83ms 95.82% Req/Sec 7.54k 0.91k 8.31k 84.44% 1803421 requests in 1.00m, 217.03MB read Requests/sec: 30006.18 Transfer/sec: 3.61MB 服务器的性能果然还是比本地强太多。\n在进行性能压测期间发布新版本 1 2 3 4 5 6 7 8 9 10 $ wrk -c 20 -d 60 -t 4 http://10.75.1.42:28080/web-0.0.1-SNAPSHOT/hello Running 1m test @ http://10.75.1.42:28080/web-0.0.1-SNAPSHOT/hello 4 threads and 20 connections Thread Stats Avg Stdev Max +/- Stdev Latency 4.47ms 22.31ms 401.95ms 96.67% Req/Sec 7.58k 0.88k 8.26k 87.12% 1811240 requests in 1.00m, 285.84MB read Non-2xx or 3xx responses: 72742 Requests/sec: 30181.93 Transfer/sec: 4.76MB 发布新版本导致4%的请求失败。\n通过Nginx访问服务 1 2 3 4 5 6 7 8 9 $ wrk -c 20 -d 60 -t 4 http://10.75.1.42:28010/web/hello Running 1m test @ http://10.75.1.42:28010/web/hello 4 threads and 20 connections Thread Stats Avg Stdev Max +/- Stdev Latency 2.94ms 16.21ms 248.18ms 98.01% Req/Sec 6.02k 551.52 6.92k 83.38% 1437098 requests in 1.00m, 260.33MB read Requests/sec: 23948.20 Transfer/sec: 4.34MB 虽然服务器配置的worker_processes auto，实际上开了40个进程，但仍然达不到直接访问Java服务的吞吐量。\n通过Nginx压测期间发布新版本 1 2 3 4 5 6 7 8 9 $ wrk -c 20 -d 60 -t 4 http://10.75.1.42:28010/web/hello Running 1m test @ http://10.75.1.42:28010/web/hello 4 threads and 20 connections Thread Stats Avg Stdev Max +/- Stdev Latency 4.09ms 20.50ms 402.11ms 97.12% Req/Sec 5.89k 733.62 6.86k 84.85% 1404463 requests in 1.00m, 253.67MB read Requests/sec: 23401.54 Transfer/sec: 4.23MB 可以看到，延迟明显变大了，但总体的QPS没有明显下降，还是因为存在一些转发。\n# 思考 原来是一台机器上运行一个tomcat容器，现在要运行两个，那么会对机器的负载造成多大的影响呢？可以通过visualvm连接上远程tomcat来观察对内存和CPU的占用\n可以看到正常情况下，backup容器对服务器的负载基本可以忽略不计。即便是在发布期间，backup容器也只是在active容器重新载入期间承担职责，之后马上就恢复了。 新版本在线上正式运行之后为保证下一次发布新版本时backup版本是最新的，需要再发布一下backup版本，当然这时流量都在active节点上，对backup节点的发布更新操作不会对负载有什么影响。\n# 总结 可以通过Nginx的backup机制可以保证服务不中断的情况下发布新版本。总体的发布流程如下：\n发布新版本到active容器 确认发布的新版本稳定后发布新版本到backup容器 # 优势 任意一台机器上在任意时刻都保证有一个tomcat容器是可用的，保证服务不中断 从直观上的分机器上线改为直接全量上线，并且保证如果上线的新版本有问题时也不会影响线上服务 # 劣势 需要上线两次 需要在tomcat容器所在的机器上安装Nginx和作为backup的tomcat的容器 backup容器在“待机”时的消耗 ","date":"2020-08-22T10:56:59+08:00","permalink":"https://blog.happyhacker.fun/p/nginx%E5%92%8Ctomcat%E9%85%8D%E5%90%88%E5%AE%9E%E7%8E%B0java-web%E6%9C%8D%E5%8A%A1%E7%83%AD%E9%83%A8%E7%BD%B2/","title":"Nginx和Tomcat配合实现Java Web服务热部署"},{"content":"有时候需要批量处理一些文件，又不方便打开文件，所以sed还是很有用的。简单记录一下常用的使用方法。\n本文介绍的用法仅仅针对GNU/sed，BSD的版本（macOS）会有不同，这里不涉及\n# 简介 先看一下手册的介绍\n1 sed [OPTION]... {script-only-if-no-other-script} [input-file]... # 选项 选项 解释 -f (file)将sed命令保存到文件来执行 -i (in-place)就地修改，默认是将修改后的目标文件内容输出到标准输出，而不会对目标文件进行修改 -n 取消默认输出，在将目标文件的内容输出到标准输出时，只输出处理过的行 -e 接下一个sed指令，只有在指定多个sed指令时需要用到 # 编辑命令 命令 解释 a (append)追加 c (change)更改匹配行的内容 i (insert)向匹配行前插入内容 d (delete)删除匹配的行 s 替换掉匹配的内容 p (print)打印匹配的行，通常和-n选项一起使用 = 用来打印被匹配的行的行号 n (next)读取下一行，遇到n时会跳入下一行 r (read)将内容读入文件 w (write)将匹配内容写入文件 # 示例 原始文本\n1 2 3 4 5 sponge bob square patrick star # 追加 # 在指定行后面追加 1 2 3 4 5 6 7 $ sed \u0026#39;3apants\u0026#39; test.txt sponge bob square pants patrick star 其中3是行号，a是追加，后面是要追加的内容\n# 在匹配行后面追加 1 2 3 4 5 6 7 $ sed \u0026#39;/square/apants\u0026#39; test.txt sponge bob square pants patrick star 其中用square来匹配行，第二个/后面的a表示追加，后面是追加的内容。如果有多个匹配行，则会在每个匹配行后面都追加\n# 在最后一行追加 1 2 3 4 5 6 7 $ sed \u0026#39;$abikini bottom\u0026#39; test.txt sponge bob square patrick star bikini bottom $表示最后一行，a表示追加，后面是追加的内容\n# 插入 # 在指定行插入 1 2 3 4 5 6 7 $ sed \u0026#39;4ipants\u0026#39; test.txt sponge bob square pants patrick star 表示在第4行前面插入一行\n# 在匹配行前插入 1 2 3 4 5 6 7 $ sed \u0026#39;/patrick/ipants\u0026#39; test.txt sponge bob square pants patrick star 如果有多个匹配行，则每个匹配行前面都会插入指定的内容\n# 在最后一行前插入 1 2 3 4 5 6 7 $ sed \u0026#39;$iis a pink\u0026#39; test.txt sponge bob square patrick is a pink star # 更改 # 更改指定的行 1 2 3 4 5 6 $ sed \u0026#39;1c海绵\u0026#39; test.txt 海绵 bob square patrick star # 更改匹配的行 1 2 3 4 5 6 $ sed \u0026#39;/sponge/c海绵\u0026#39; test.txt 海绵 bob square patrick star # 更改最后一行 1 2 3 4 5 6 $ sed \u0026#39;$c派大星\u0026#39; test.txt sponge bob square patrick 派大星 # 删除 # 删除指定的一行 1 2 3 4 5 $ sed \u0026#39;3d\u0026#39; test.txt sponge bob patrick star # 删除指定范围行号的多行 1 2 3 $ sed \u0026#39;1,3d\u0026#39; test.txt patrick star # 从第n行开始，每隔m行删除一行 1 2 3 4 $ sed \u0026#39;1~3d\u0026#39; test.txt bob square star 即从第一行开始，每隔3行删除一行，所以最终是删除了第1行和第4行。如果是1~2d这样会更有意义，也就是会删除奇数行，而2~2d则会删除所有的偶数行\n# 删除除满足条件的行之外的所有行 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 frost@master:~/workspace/sed $ cat test.txt sponge bob square patrick star frost@master:~/workspace/sed $ sed \u0026#39;1~3d\u0026#39; test.txt bob square star frost@master:~/workspace/sed $ sed \u0026#39;1~3!d\u0026#39; test.txt sponge patrick 这个比较复杂，把对应的几种情况放在一起看。\n# 删除某范围之外的所有行 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 frost@master:~/workspace/sed $ cat test.txt sponge bob square patrick star frost@master:~/workspace/sed $ sed \u0026#39;1,2d\u0026#39; test.txt square patrick star frost@master:~/workspace/sed $ sed \u0026#39;1,2!d\u0026#39; test.txt sponge bob # 删除匹配的行 1 2 3 4 5 $ sed \u0026#39;/patrick/d\u0026#39; test.txt sponge bob square star # 删除匹配行及其后的n行 1 2 3 4 $ sed \u0026#39;/square/,+1d\u0026#39; test.txt sponge bob star # 删除匹配的行之后的所有行 1 2 3 4 $ sed \u0026#39;/patrick/,$d\u0026#39; test.txt sponge bob square 可以和上面直接指定行号的对比一下，其实/patrick/这部分就是为了定位到这一行的行号，假如这行在第2行，那么这就等价于2,$d\n# 删除最后一行 1 2 3 4 5 $ sed \u0026#39;$d\u0026#39; test.txt sponge bob square patrick # 删除所有的空行 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 frost@master:~/workspace/sed $ cat test.txt sponge bob square patrick star frost@master:~/workspace/sed $ sed \u0026#39;/^$/d\u0026#39; test.txt sponge bob square patrick star # 删除匹配多种模式的行 1 2 3 4 $ sed \u0026#39;/sp\\|bob/d\u0026#39; test.txt square patrick star # 删除除匹配多种模式以外的所有行 1 2 3 $ sed \u0026#39;/sp\\|bob/!d\u0026#39; test.txt sponge bob # 删除指定行范围内满足条件的行 1 2 3 4 $ sed \u0026#39;1,4{/e/d}\u0026#39; test.txt bob patrick star 删除了从第1行到第4行中包含e的行\n# 替换指定字符 终于到了用的最多的场景了。\n# 替换匹配的字符 1 2 3 4 5 6 7 8 9 10 11 12 13 14 frost@master:~/workspace/sed $ cat test.txt sponge bob bob bob square patrick star frost@master:~/workspace/sed $ sed \u0026#39;s/bob/宝宝/\u0026#39; test.txt sponge 宝宝 bob bob square patrick star 默认只替换第一个，如果要替换匹配行中所有满足条件的字符，需要加上g选项（global)\n# 修改匹配行中第n个匹配的字符 1 2 3 4 5 6 $ sed \u0026#39;s/bob/宝宝/2\u0026#39; test.txt sponge bob 宝宝 bob square patrick star 替换了该行第2个匹配的字符\n# 全局替换匹配的字符 1 2 3 4 5 6 7 8 9 10 11 12 13 14 frost@master:~/workspace/sed $ cat test.txt sponge bob bob bob square patrick star frost@master:~/workspace/sed $ sed \u0026#39;s/bob/宝宝/g\u0026#39; test.txt sponge 宝宝 宝宝 宝宝 square patrick star # 将修改后的内容输出到文件 1 2 3 4 5 6 7 8 9 $ sed \u0026#39;s/bob/宝宝/gw 2.txt\u0026#39; test.txt sponge 宝宝 宝宝 宝宝 square patrick star frost@master:~/workspace/sed $ cat 2.txt 宝宝 宝宝 宝宝 # 将每行//后的内容删除 1 2 3 4 5 6 7 8 9 10 frost@master:~/workspace/sed $ cat a.php \u0026lt;?php $a = 20; //这是一个行内注释 frost@master:~/workspace/sed $ sed \u0026#39;s#//.*##g\u0026#39; a.php \u0026lt;?php $a = 20; 这是全文里找到符合条件的，还可以指定符合某个前置条件的才执行\n# 符合前置条件的行中，将//后面的内容删除 1 2 3 4 5 6 7 8 9 10 11 12 frost@master:~/workspace/sed $ cat a.php \u0026lt;?php $a = 20; //这是一个行内注释 define(\u0026#39;I_AM_A_CONST\u0026#39;, 20); // 这是一个常量 frost@master:~/workspace/sed $ sed \u0026#39;/\\$/s/\\/\\/.*//g\u0026#39; a.php \u0026lt;?php $a = 20; define(\u0026#39;I_AM_A_CONST\u0026#39;, 20); // 这是一个常量 这个例子把有变量定义的行内注释给删除了\n# 替换匹配行倒数n个字符 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 frost@master:~/workspace/sed $ sed \u0026#39;s/..$//g\u0026#39; a.php \u0026lt;?p $a = 20; //这是一个行内注� define(\u0026#39;I_AM_A_CONST\u0026#39;, 20); // 这是一个常� frost@master:~/workspace/sed $ sed \u0026#39;s/....$//g\u0026#39; a.php \u0026lt; $a = 20; //这是一个行内� define(\u0026#39;I_AM_A_CONST\u0026#39;, 20); // 这是一个� frost@master:~/workspace/sed $ sed \u0026#39;s/...$//g\u0026#39; a.php \u0026lt;? $a = 20; //这是一个行内注 define(\u0026#39;I_AM_A_CONST\u0026#39;, 20); // 这是一个常 最终目的是达成了，但是要注意一个汉字其实是占了3个字符。\n# 将匹配的行替换为空行 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 frost@master:~/workspace/sed $ cat a.php \u0026lt;?php $a = 20; //这是一个行内注释 define(\u0026#39;I_AM_A_CONST\u0026#39;, 20); // 这是一个常量 # 这是一个不符合规范的注释 frost@master:~/workspace/sed $ sed \u0026#39;s/^#.*//g\u0026#39; a.php \u0026lt;?php $a = 20; //这是一个行内注释 define(\u0026#39;I_AM_A_CONST\u0026#39;, 20); // 这是一个常量 frost@master:~/workspace/sed $ sed \u0026#39;/^#/d\u0026#39; a.php \u0026lt;?php $a = 20; //这是一个行内注释 define(\u0026#39;I_AM_A_CONST\u0026#39;, 20); // 这是一个常量 frost@master:~/workspace/sed 这里有意和前面讲过的d命令做一个比较，可以看到s命令做的是替换，将这行的字符替换为空（并没有替换换行符），但这行还在，但d是删除整行。\n# 总结 上面这些已经能满足大部分的需求了，后面遇到偏门的场景再补充。\n","date":"2020-08-01T16:38:23+08:00","permalink":"https://blog.happyhacker.fun/p/sed%E7%AE%80%E5%8D%95%E7%94%A8%E6%B3%95/","title":"Sed简单用法"},{"content":"本来选择AMD的黑苹果之前就已经做好了无法使用docker的准备了，没想到那天偶然发现说docker-toolbox是可以用的。\n国内下载docker官方的东西实在是太慢了，还是从阿里云下载的。这里多说一句，阿里云在开源软件镜像架设这方面的贡献远远超过国内其他公司了（BT？不存在的）\n# 下载Docker Toolbox 访问阿里云的Docker Toolbox下载地址，下载最新的安装包。\n这里同步是有点问题的，官方已经有19.xx版本了，但阿里云的镜像这里还是18.03，不过无所谓了，快才是王道。\n# 安装 安装过程就不说了，既然你找到这里了说明你肯定会安装软件了。\n# 配置 注意红色部分的提示，乍一看是需要在BIOS里打开AMD-v技术（对标Intel的VT-x），一般是叫SVM，默认通常是不开启的。但我这里已经开启过了，还报这个错只能说是检测脚本并没有适配【AMD黑苹果】这个可能性。\n所以直接忽略这个提示。\n然后需要创建一个默认的虚拟机，用virtualbox的驱动即可。这里要注意，由于众所周知的原因，我们下载docker的官方镜像非常慢，所以要配置一下国内的代理，建议使用阿里云。注册登陆之后访问这个镜像加速器，按文档说明执行你的命令即可\n注意：执行docker-machine create命令的时候还是会检查上面提到的有问题的虚拟化技术检查，所以需要加上一个--virtualbox-no-vtx-check选项\n1 docker-machine create default --engine-registry-mirror=https://yourcode.mirror.aliyuncs.com -d virtualbox --virtualbox-no-vtx-check --virtualbox-memory \u0026#34;8096\u0026#34; --virtualbox-cpu-count \u0026#34;6\u0026#34; 2020年11月添加：后来使用过程中发现内存会不够用，因为默认是1G内存和1个CPU核心，这明显是不能满足正常的使用需求的，所以就需要加一些资源了。\n# 打开Kitematic开始体验docker 哇！原来docker还能这么用？！当你走到这一步了，你一定知道我为什么会感叹。\n","date":"2020-07-25T23:35:01+08:00","permalink":"https://blog.happyhacker.fun/p/amd%E9%BB%91%E8%8B%B9%E6%9E%9C%E4%B8%8A%E7%9A%84docker/","title":"AMD黑苹果上的docker"},{"content":"用newman搞了个自动化测试脚本，测试出错的时候需要发邮件提醒，但之前尝试配置了sendmail有各种神奇的问题，这次换了思路，尝试用mailx。\n# 安装软件包 1 yum install -y mailx # 配置发件人信息 安装完成后会生成一个/etc/mail.rc配置文件，前面的那些都不用看，直接无脑在最后添加以下内容\n1 2 3 4 5 set from=\u0026#34;myname@happyhacker.fun\u0026#34; set smtp=\u0026#34;mail.happyhacker.fun\u0026#34; set smtp-auth-user=\u0026#34;myname@happyhacker.fun\u0026#34; set smtp-auth-password=\u0026#34;this is a strong password\u0026#34; set smtp-auth=login # 测试几种不同方式 # 不带附件 # 管道 1 echo \u0026#34;这是正文\u0026#34; | mail -s \u0026#34;这是标题\u0026#34; myname@happyhacker.fun # 重定向 1 mail -s \u0026#34;这是标题\u0026#34; myname@happyhacker.fun \u0026lt; /path/to/a/text/file 这样会把指定的文件中的内容当作邮件的正文发出\n# 带附件 1 mail -a 附件.docx -s \u0026#34;这是标题\u0026#34; myname@happyhacker.fun \u0026lt; /path/to/a/text/file 这些就可以应付大部分场景了。\n","date":"2020-07-20T17:10:43+08:00","permalink":"https://blog.happyhacker.fun/p/%E6%9C%80%E7%AE%80%E5%8D%95%E7%9A%84%E5%91%BD%E4%BB%A4%E8%A1%8C%E5%8F%91%E9%82%AE%E4%BB%B6%E6%96%B9%E5%BC%8F/","title":"最简单的命令行发邮件方式"},{"content":"更新完黑苹果之后发现进BIOS的时候多了一个选项，看起来很奇怪。\n其实是因为在安装Windows或者Windows进行系统更新时，会覆盖/EFI/BOOT/BOOTx64.efi文件，这个配置就是为了保护这个文件不被修改的，具体的原因我没有研究，因为暂时还没有安装Windows。（其实是因为安装不上，在Mac上创建的WindowsInstaller总是无法启动，不是刚需也就没有再继续研究了）\n多了个OpenCore，经过在Reddit上发帖求助，发现原因可能是更新到0.5.9的时候复制了一个配置来 Misc -\u0026gt; Security -\u0026gt; BootProtect，现在的配置是Bootstrap，改成none即可，但如果安装了Windows系统，每当windows系统更新时就会破坏OpenCore的启动顺序。\n所以这其实是OpenCore的保护机制，也就不难理解它是一个【启动安全】选项的原因了。不过我暂时没有安装Windows，所以也用不到这个选项。不过明白了，也就不再纠结了，暂时就不改它了，免得将来装了Windows但忘了这个事儿，净是给自己挖坑。\n","date":"2020-06-14T15:49:21+08:00","permalink":"https://blog.happyhacker.fun/p/%E9%BB%91%E8%8B%B9%E6%9E%9Cbootmenu/","title":"黑苹果bootmenu"},{"content":"装黑苹果的时候是10.15.3，后来经历了两次官方的更新，但我又不太清楚需要的更新过程，现在更新成功之后记录一下。\n# 1. 更新EFI 下载最新的OpenCorePkg，参考这里把主要的几个文件更新一下，把EFI更新到系统硬盘之后重启看看能不能正常启动，能正常启动后再进行第二步。\n我首次安装的时候是OpenCore0.5.6，更新到0.5.7的时候有一个不兼容的配置，如下图所示\n而正好重命名的这个是必须项，所以就需要自己把FwRuntimeServices.efi删除，把新的OpenRuntime.efi放进来，打开ProperTree，重新加载config.plist\n# 2. 更新macOS 这就是上述这里没有说清楚的地方，因为实在是没什么好说的。就按正常的系统更新流程直接点更新即可！\n# 3. 更新成功 ","date":"2020-06-14T10:12:22+08:00","permalink":"https://blog.happyhacker.fun/p/%E9%BB%91%E8%8B%B9%E6%9E%9C%E6%9B%B4%E6%96%B0%E5%8D%87%E7%BA%A7/","title":"黑苹果更新升级"},{"content":"作为黑苹果用户，不知道直接更新系统会发生什么不可预知的问题，所以还是尽量避免升级。\n# 设置方法 # 1. 在系统偏好设置中关闭自动更新 # 2. 在终端执行以下命令 1 2 3 sudo softwareupdate --ignore \u0026#34;macOS Catalina\u0026#34; defaults write com.apple.systempreferences AttentionPrefBundleIDs 0 killall Dock # 3. 补充 最近的不知道哪次更新又带来了一个问题，执行上面的命令的时候会报\n1 2 3 4 5 6 7 8 9 10 11 Password: Ignored updates: ( ) Software Update can only ignore updates that are eligible for installation. If the label provided to ignore is not in the above list, it is not eligible to be ignored. Ignoring software updates is deprecated. The ability to ignore individual updates will be removed in a future release of macOS. 也就是说苹果以后要把这个选项去掉了，可以参考一下这个文章Apple’s has brought back the nagging — you can no longer ignore major macOS updates，看起来不光是我反感这个事儿，全世界都觉得苹果一点都不在乎用户的感受啊，和微信有点像。\n# 总结 macOS和iPhone的升级速度快（指最新版本的更新率高）的原因就是这么不停提醒吧，太讨厌了。这种方法亲测有效。\n","date":"2020-06-10T10:52:37+08:00","permalink":"https://blog.happyhacker.fun/p/%E5%BD%BB%E5%BA%95%E5%85%B3%E9%97%ADmacos%E7%B3%BB%E7%BB%9F%E5%8D%87%E7%BA%A7%E6%8F%90%E7%A4%BA/","title":"彻底关闭macOS系统升级提示"},{"content":"在macOS上开发Springboot应用时发现应用启动前总是等待5秒钟，体现在应用启动的很慢。\n具体的提示信息因为改完之后找不到了，就是一句提示，说使用了5000 milliseconds，建议macOS用户修改/etc/hosts。但具体改什么就没有提到。\n其实是因为应用启动时会查询域名${hostname}，而macOS上默认是没有配置这个域名的，所以就要等到超时（5秒）才能继续了。知道了问题的原因也就清楚如何解决了，在/etc/hosts中添加以下两行\n1 2 127.0.0.1 ${hostname} ::1 ${hostname} 其中${hostname}要替换成你机器的hostname，要得到它只需要执行hostname命令即可，一般是一个以.local结尾的字符串。\n","date":"2020-05-21T10:41:26+08:00","permalink":"https://blog.happyhacker.fun/p/%E9%81%BF%E5%85%8Dspringboot%E5%BA%94%E7%94%A8%E5%90%AF%E5%8A%A8%E5%89%8D5%E7%A7%92%E7%9A%84%E7%AD%89%E5%BE%85/","title":"避免Springboot应用启动前5秒的等待"},{"content":"jar包应该是java应用使用最多的分发形式了，jar包中包含什么东西，怎么创建和执行它呢？\n# jar包是什么 Jar包全称Java Archive File，是以zip格式打包的一个压缩包，和普通的压缩包最本质的区别是——后缀不一样，一个是.jar，一个是.zip。但区分jar包和其他zip包的本质区别是jar包包含一个META-INF/MANIFEST.MF文件，这个清单文件是包含了以下内容\njar包的版本 创建人 Class-Path类搜索路径 Main-Class属性（表示Main方法的入口） # 为什么要用jar包 通常我们写一个HelloWord类之后会做以下操作\n1 2 3 4 5 6 // 保存为HelloWorld.java public class HelloWorld { public static void main(String[] args) { System.out.println(\u0026#34;Hello World!\u0026#34;); } } 1 2 3 4 javac HelloWorld.java # 会生成一个HelloWorld.class文件 java HelloWorld # output Hello World! 这里HelloWorld.class其实才是我们需要的东西，也就是Java所谓的“字节码”，Java的最大卖点Write Once, Run Anywhere的特点就来自这里了。需要注意的是，所谓跨平台，并不是指Java（jdk）本身跨平台，而是由jdk编译而来的class文件跨平台。所以也就需要针对不同平台的jdk了。\n当我们的应用只有一个文件时当然可以这样发布，但这肯定是不可能的。如果应用有其他的依赖，用这种原始的方式将会很难维护，因此，jar包就应运而生了。\n下面简单写一个带有依赖的例子，入口类App，小狗类Dog。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 package entity; public class Dog { private String name; private int age; public Dog(String name, int age) { this.name = name; this.age = age; } public String getName() { return name; } public int getAge() { return age; } } 1 2 3 4 5 6 7 8 import entity.Dog; public class App { public static void main(String[] args) { Dog aDog = new Dog(\u0026#34;Bailey\u0026#34;, 11); System.out.println(\u0026#34;I am \u0026#34; + aDog.getName() + \u0026#34; and I am \u0026#34; + aDog.getAge() + \u0026#34; years old\u0026#34;); } } 目录结构如下：\n1 2 3 4 . ├── App.java └── entity └── Dog.java 如果没有jar包，就需要执行如下命令\n1 2 3 4 5 6 7 8 9 10 11 12 ➜ demo javac App.java entity/Dog.java ➜ demo tree . ├── App.class ├── App.java └── entity ├── Dog.class └── Dog.java 1 directory, 4 files ➜ demo java App I am Bailey and I am 11 years old 这时候源码(.java)已经不需要了，删了它们一样可以运行。为了更方便的分发，可以把这些.class文件打包成jar包，并指定运行的入口类\n1 2 3 4 5 6 7 8 9 ➜ demo jar cvfe App.jar App entity/*.class App.class added manifest adding: entity/Dog.class(in = 442) (out= 286)(deflated 35%) adding: App.class(in = 784) (out= 476)(deflated 39%) ➜ demo jar -tf App.jar META-INF/ META-INF/MANIFEST.MF entity/Dog.class App.class 这时候执行java -jar App.jar，输出如下\n1 2 ➜ demo java -jar App.jar I am Bailey and I am 11 years old 这时候把这个压缩包发给别人，别人就可以直接这样执行了。\n# 添加和修改清单属性 如果第一次打包的时候没有添加MANIFEST文件，jar会自动添加一个META-INF/MANIFEST.MF文件，但其中只包含Manifest-Version和Created-By两个属性，在执行的时候还要在命令行传入入口才可以执行。所以这时如果要给MANIFEST文件添加属性，可以这么做。\n创建一个新的清单文件，比如MANIFEST-ADDITION.MF 将需要添加的属性假如新的清单文件 执行jar ufvm App.jar MANIFEST-ADDITION.MF 1 2 Main-Class: App Author: happyhakcer 然后通过vim打开这个jar包就可以看到其中的清单文件已经更新了。\n另外，如果是打包前就已经手动写好了完整的清单文件，也可以通过jar cfvm的方式直接把清单文件打包进jar包。\n需要注意一点，在执行jar vcfme这种命令时，f m e这三个选项的顺序也决定了后面传入的参数表示的含义，f表示目标文件（jar包的名称），m表示清单文件，e(entry)则表示入口点。 比如可以是\n1 jar vcfme app.jar MANIFEST.MF App App.class entity/*.class 这里是为了说明问题，如果MANIFEST.MF中已经指定了Main-Class属性，则在执行上面的命令时会因为又通过e选项指定了入口点而执行失败。\n# jar包的官方标准 现在我们已经有了一个标准的jar包了，下面打开它看看这个清单文件中究竟包含了什么信息\n1 2 3 Manifest-Version: 1.0 Created-By: 1.8.0_252 (AdoptOpenJDK) Main-Class: App 由于entity目录位于App.class所在目录的子目录中，所以无需指定Class-Path\n# jar包的其他标准 本节其实是为了说明标准的jar包和Springboot打包的FAT JAR的区别。\n通常一个jar包只包含了应用代码（区别于依赖），但Springboot的jar包则动辄几百MB，其实就是因为它把所有的依赖全部都打到jar包里了。一个典型的Springboot应用的jar包的清单文件内容如下 可以看到，下面有波浪线的其实就是标准的清单文件中不包含的部分。而Springboot能从jar包启动，核心就在于Main-Class配置。关于org.springframework.boot.loader.JarLauncher的原理，详见Springboot启动\n# jar包和war包的区别 最本质的区别就是war包是一个典型的web包，所谓典型也就是早年间把接口和页面等其他静态资源打包到一起的包，tomcat就是用来运行war包的。由于现在war包用的不多了，这里不再赘述。\n# 总结 本文简单介绍了jar包的结构和一些简单的应用，主要帮助初学者理解一些概念。\n","date":"2020-05-18T22:18:51+08:00","permalink":"https://blog.happyhacker.fun/p/%E8%81%8A%E8%81%8Ajar%E5%8C%85/","title":"聊聊jar包"},{"content":"火狐浏览器有个很奇怪的设定，在Windows下按住Ctrl（macOS下按住Cmd）加上鼠标滚轮会缩放网页。\n有些人可能觉得是个很有用的功能，但我觉得这个功能让我很烦恼，有时候有不自觉的放在Cmd上，鼠标滚动一下网页就变成了200%，原来是可以通过配置修改关闭这项功能的\n在地址栏输入about:config 搜索mousewheel.with_meta.action，把3改成0 如果是windows，则搜索mousewheel.with_control.action，同样把3改成0 改完立即生效，不需要重启浏览器 ","date":"2020-05-16T00:49:00+08:00","permalink":"https://blog.happyhacker.fun/p/%E5%85%B3%E9%97%ADfirefox%E6%B5%8F%E8%A7%88%E5%99%A8%E4%B8%AD%E4%BD%BF%E7%94%A8%E9%BC%A0%E6%A0%87%E7%BC%A9%E6%94%BE%E7%BD%91%E9%A1%B5%E7%9A%84%E5%8A%9F%E8%83%BD/","title":"关闭Firefox浏览器中使用鼠标缩放网页的功能"},{"content":"不知道为什么好像家里的网络对github很不友好，网页能打开，但通过curl访问就有问题，所以安装homebrew时如果使用官方提供的花里胡哨的curl或者wget的方式就安装不上。\n姑且说安装成功了吧，后面安装其他软件的时候又会各种慢。国内中科大(个人认为国内做的最好的开源镜像站点)和清华大学都提供了homebrew的镜像，但比较吊诡的是他们提供的都是post-installation，也就是前提是你已经安装好了brew。殊不知很多人（网络环境）这一步都过不了关了。\n所以我看了一下官方的安装脚本，参考了中科大的文档，把安装脚本中的github地址给改成中科大的，然后加上了brew-cask和brew-core，并把其中的brew update --force命令后面加了个-v，方便在安装的时候可以看到具体的进度。\n脚本正文直接贴出来，有想看github的在这里。\n脚本默认你使用的是zsh，有偏好使用bash的同学可以自行修改\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 #!/bin/bash set -u # First check if the OS is Linux. if [[ \u0026#34;$(uname)\u0026#34; = \u0026#34;Linux\u0026#34; ]]; then HOMEBREW_ON_LINUX=1 fi # On macOS, this script installs to /usr/local only. # On Linux, it installs to /home/linuxbrew/.linuxbrew if you have sudo access # and ~/.linuxbrew otherwise. # To install elsewhere (which is unsupported) # you can untar https://github.com/Homebrew/brew/tarball/master # anywhere you like. if [[ -z \u0026#34;${HOMEBREW_ON_LINUX-}\u0026#34; ]]; then HOMEBREW_PREFIX=\u0026#34;/usr/local\u0026#34; HOMEBREW_REPOSITORY=\u0026#34;/usr/local/Homebrew\u0026#34; HOMEBREW_CACHE=\u0026#34;${HOME}/Library/Caches/Homebrew\u0026#34; STAT=\u0026#34;stat -f\u0026#34; CHOWN=\u0026#34;/usr/sbin/chown\u0026#34; CHGRP=\u0026#34;/usr/bin/chgrp\u0026#34; GROUP=\u0026#34;admin\u0026#34; TOUCH=\u0026#34;/usr/bin/touch\u0026#34; else HOMEBREW_PREFIX_DEFAULT=\u0026#34;/home/linuxbrew/.linuxbrew\u0026#34; HOMEBREW_CACHE=\u0026#34;${HOME}/.cache/Homebrew\u0026#34; STAT=\u0026#34;stat --printf\u0026#34; CHOWN=\u0026#34;/bin/chown\u0026#34; CHGRP=\u0026#34;/bin/chgrp\u0026#34; GROUP=\u0026#34;$(id -gn)\u0026#34; TOUCH=\u0026#34;/bin/touch\u0026#34; fi BREW_REPO=\u0026#34;http://mirrors.ustc.edu.cn/brew.git\u0026#34; # TODO: bump version when new macOS is released MACOS_LATEST_SUPPORTED=\u0026#34;10.15\u0026#34; # TODO: bump version when new macOS is released MACOS_OLDEST_SUPPORTED=\u0026#34;10.13\u0026#34; # no analytics during installation export HOMEBREW_NO_ANALYTICS_THIS_RUN=1 export HOMEBREW_NO_ANALYTICS_MESSAGE_OUTPUT=1 # string formatters if [[ -t 1 ]]; then tty_escape() { printf \u0026#34;\\033[%sm\u0026#34; \u0026#34;$1\u0026#34;; } else tty_escape() { :; } fi tty_mkbold() { tty_escape \u0026#34;1;$1\u0026#34;; } tty_underline=\u0026#34;$(tty_escape \u0026#34;4;39\u0026#34;)\u0026#34; tty_blue=\u0026#34;$(tty_mkbold 34)\u0026#34; tty_red=\u0026#34;$(tty_mkbold 31)\u0026#34; tty_bold=\u0026#34;$(tty_mkbold 39)\u0026#34; tty_reset=\u0026#34;$(tty_escape 0)\u0026#34; have_sudo_access() { if [[ -z \u0026#34;${HAVE_SUDO_ACCESS-}\u0026#34; ]]; then /usr/bin/sudo -l mkdir \u0026amp;\u0026gt;/dev/null HAVE_SUDO_ACCESS=\u0026#34;$?\u0026#34; fi if [[ -z \u0026#34;${HOMEBREW_ON_LINUX-}\u0026#34; ]] \u0026amp;\u0026amp; [[ \u0026#34;$HAVE_SUDO_ACCESS\u0026#34; -ne 0 ]]; then abort \u0026#34;Need sudo access on macOS!\u0026#34; fi return \u0026#34;$HAVE_SUDO_ACCESS\u0026#34; } shell_join() { local arg printf \u0026#34;%s\u0026#34; \u0026#34;$1\u0026#34; shift for arg in \u0026#34;$@\u0026#34;; do printf \u0026#34; \u0026#34; printf \u0026#34;%s\u0026#34; \u0026#34;${arg// /\\ }\u0026#34; done } chomp() { printf \u0026#34;%s\u0026#34; \u0026#34;${1/\u0026#34;$\u0026#39;\\n\u0026#39;\u0026#34;/}\u0026#34; } ohai() { printf \u0026#34;${tty_blue}==\u0026gt;${tty_bold} %s${tty_reset}\\n\u0026#34; \u0026#34;$(shell_join \u0026#34;$@\u0026#34;)\u0026#34; } warn() { printf \u0026#34;${tty_red}Warning${tty_reset}: %s\\n\u0026#34; \u0026#34;$(chomp \u0026#34;$1\u0026#34;)\u0026#34; } abort() { printf \u0026#34;%s\\n\u0026#34; \u0026#34;$1\u0026#34; exit 1 } execute() { if ! \u0026#34;$@\u0026#34;; then abort \u0026#34;$(printf \u0026#34;Failed during: %s\u0026#34; \u0026#34;$(shell_join \u0026#34;$@\u0026#34;)\u0026#34;)\u0026#34; fi } execute_sudo() { local -a args=(\u0026#34;$@\u0026#34;) if [[ -n \u0026#34;${SUDO_ASKPASS-}\u0026#34; ]]; then args=(\u0026#34;-A\u0026#34; \u0026#34;${args[@]}\u0026#34;) fi if have_sudo_access; then ohai \u0026#34;/usr/bin/sudo\u0026#34; \u0026#34;${args[@]}\u0026#34; execute \u0026#34;/usr/bin/sudo\u0026#34; \u0026#34;${args[@]}\u0026#34; else ohai \u0026#34;${args[@]}\u0026#34; execute \u0026#34;${args[@]}\u0026#34; fi } getc() { local save_state save_state=$(/bin/stty -g) /bin/stty raw -echo IFS= read -r -n 1 -d \u0026#39;\u0026#39; \u0026#34;$@\u0026#34; /bin/stty \u0026#34;$save_state\u0026#34; } wait_for_user() { local c echo echo \u0026#34;Press RETURN to continue or any other key to abort\u0026#34; getc c # we test for \\r and \\n because some stuff does \\r instead if ! [[ \u0026#34;$c\u0026#34; == $\u0026#39;\\r\u0026#39; || \u0026#34;$c\u0026#34; == $\u0026#39;\\n\u0026#39; ]]; then exit 1 fi } major_minor() { echo \u0026#34;${1%%.*}.$(x=\u0026#34;${1#*.}\u0026#34;; echo \u0026#34;${x%%.*}\u0026#34;)\u0026#34; } if [[ -z \u0026#34;${HOMEBREW_ON_LINUX-}\u0026#34; ]]; then macos_version=\u0026#34;$(major_minor \u0026#34;$(/usr/bin/sw_vers -productVersion)\u0026#34;)\u0026#34; fi version_gt() { [[ \u0026#34;${1%.*}\u0026#34; -gt \u0026#34;${2%.*}\u0026#34; ]] || [[ \u0026#34;${1%.*}\u0026#34; -eq \u0026#34;${2%.*}\u0026#34; \u0026amp;\u0026amp; \u0026#34;${1#*.}\u0026#34; -gt \u0026#34;${2#*.}\u0026#34; ]] } version_ge() { [[ \u0026#34;${1%.*}\u0026#34; -gt \u0026#34;${2%.*}\u0026#34; ]] || [[ \u0026#34;${1%.*}\u0026#34; -eq \u0026#34;${2%.*}\u0026#34; \u0026amp;\u0026amp; \u0026#34;${1#*.}\u0026#34; -ge \u0026#34;${2#*.}\u0026#34; ]] } version_lt() { [[ \u0026#34;${1%.*}\u0026#34; -lt \u0026#34;${2%.*}\u0026#34; ]] || [[ \u0026#34;${1%.*}\u0026#34; -eq \u0026#34;${2%.*}\u0026#34; \u0026amp;\u0026amp; \u0026#34;${1#*.}\u0026#34; -lt \u0026#34;${2#*.}\u0026#34; ]] } should_install_git() { if [[ $(command -v git) ]]; then return 1 fi } should_install_command_line_tools() { if [[ -n \u0026#34;${HOMEBREW_ON_LINUX-}\u0026#34; ]]; then return 1 fi if version_gt \u0026#34;$macos_version\u0026#34; \u0026#34;10.13\u0026#34;; then ! [[ -e \u0026#34;/Library/Developer/CommandLineTools/usr/bin/git\u0026#34; ]] else ! [[ -e \u0026#34;/Library/Developer/CommandLineTools/usr/bin/git\u0026#34; ]] || ! [[ -e \u0026#34;/usr/include/iconv.h\u0026#34; ]] fi } get_permission() { $STAT \u0026#34;%A\u0026#34; \u0026#34;$1\u0026#34; } user_only_chmod() { [[ -d \u0026#34;$1\u0026#34; ]] \u0026amp;\u0026amp; [[ \u0026#34;$(get_permission \u0026#34;$1\u0026#34;)\u0026#34; != \u0026#34;755\u0026#34; ]] } exists_but_not_writable() { [[ -e \u0026#34;$1\u0026#34; ]] \u0026amp;\u0026amp; ! [[ -r \u0026#34;$1\u0026#34; \u0026amp;\u0026amp; -w \u0026#34;$1\u0026#34; \u0026amp;\u0026amp; -x \u0026#34;$1\u0026#34; ]] } get_owner() { $STAT \u0026#34;%u\u0026#34; \u0026#34;$1\u0026#34; } file_not_owned() { [[ \u0026#34;$(get_owner \u0026#34;$1\u0026#34;)\u0026#34; != \u0026#34;$(id -u)\u0026#34; ]] } get_group() { $STAT \u0026#34;%g\u0026#34; \u0026#34;$1\u0026#34; } file_not_grpowned() { [[ \u0026#34; $(id -G \u0026#34;$USER\u0026#34;) \u0026#34; != *\u0026#34; $(get_group \u0026#34;$1\u0026#34;) \u0026#34;* ]] } # USER isn\u0026#39;t always set so provide a fall back for the installer and subprocesses. if [[ -z \u0026#34;${USER-}\u0026#34; ]]; then USER=\u0026#34;$(chomp \u0026#34;$(id -un)\u0026#34;)\u0026#34; export USER fi # Invalidate sudo timestamp before exiting (if it wasn\u0026#39;t active before). if ! /usr/bin/sudo -n -v 2\u0026gt;/dev/null; then trap \u0026#39;/usr/bin/sudo -k\u0026#39; EXIT fi # Things can fail later if `pwd` doesn\u0026#39;t exist. # Also sudo prints a warning message for no good reason cd \u0026#34;/usr\u0026#34; || exit 1 ####################################################################### script if should_install_git; then abort \u0026#34;$(cat \u0026lt;\u0026lt;EOABORT You must install Git before installing Homebrew. See: ${tty_underline}https://docs.brew.sh/Installation${tty_reset} EOABORT )\u0026#34; fi if [[ -n \u0026#34;${HOMEBREW_ON_LINUX-}\u0026#34; ]]; then if [[ -n \u0026#34;${CI-}\u0026#34; ]] || [[ -w \u0026#34;$HOMEBREW_PREFIX_DEFAULT\u0026#34; ]] || [[ -w \u0026#34;/home/linuxbrew\u0026#34; ]] || [[ -w \u0026#34;/home\u0026#34; ]]; then HOMEBREW_PREFIX=\u0026#34;$HOMEBREW_PREFIX_DEFAULT\u0026#34; else trap exit SIGINT sudo_output=\u0026#34;$(/usr/bin/sudo -n -l mkdir 2\u0026gt;\u0026amp;1)\u0026#34; sudo_exit_code=\u0026#34;$?\u0026#34; if [[ \u0026#34;$sudo_exit_code\u0026#34; -ne 0 ]] \u0026amp;\u0026amp; [[ \u0026#34;$sudo_output\u0026#34; = \u0026#34;sudo: a password is required\u0026#34; ]]; then ohai \u0026#34;Select the Homebrew installation directory\u0026#34; echo \u0026#34;- ${tty_bold}Enter your password${tty_reset} to install to ${tty_underline}${HOMEBREW_PREFIX_DEFAULT}${tty_reset} (${tty_bold}recommended${tty_reset})\u0026#34; echo \u0026#34;- ${tty_bold}Press Control-D${tty_reset} to install to ${tty_underline}$HOME/.linuxbrew${tty_reset}\u0026#34; echo \u0026#34;- ${tty_bold}Press Control-C${tty_reset} to cancel installation\u0026#34; fi if have_sudo_access; then HOMEBREW_PREFIX=\u0026#34;$HOMEBREW_PREFIX_DEFAULT\u0026#34; else HOMEBREW_PREFIX=\u0026#34;$HOME/.linuxbrew\u0026#34; fi trap - SIGINT fi HOMEBREW_REPOSITORY=\u0026#34;${HOMEBREW_PREFIX}/Homebrew\u0026#34; fi if [[ \u0026#34;$UID\u0026#34; == \u0026#34;0\u0026#34; ]]; then abort \u0026#34;Don\u0026#39;t run this as root!\u0026#34; elif [[ -d \u0026#34;$HOMEBREW_PREFIX\u0026#34; \u0026amp;\u0026amp; ! -x \u0026#34;$HOMEBREW_PREFIX\u0026#34; ]]; then abort \u0026#34;$(cat \u0026lt;\u0026lt;EOABORT The Homebrew prefix, ${HOMEBREW_PREFIX}, exists but is not searchable. If this is not intentional, please restore the default permissions and try running the installer again: sudo chmod 775 ${HOMEBREW_PREFIX} EOABORT )\u0026#34; fi if [[ -z \u0026#34;${HOMEBREW_ON_LINUX-}\u0026#34; ]]; then if version_lt \u0026#34;$macos_version\u0026#34; \u0026#34;10.7\u0026#34;; then abort \u0026#34;$(cat \u0026lt;\u0026lt;EOABORT Your Mac OS X version is too old. See: ${tty_underline}https://github.com/mistydemeo/tigerbrew${tty_reset} EOABORT )\u0026#34; elif version_lt \u0026#34;$macos_version\u0026#34; \u0026#34;10.9\u0026#34;; then abort \u0026#34;Your OS X version is too old\u0026#34; elif ! [[ \u0026#34;$(dsmemberutil checkmembership -U \u0026#34;$USER\u0026#34; -G \u0026#34;$GROUP\u0026#34;)\u0026#34; = *\u0026#34;user is a member\u0026#34;* ]]; then abort \u0026#34;This script requires the user $USER to be an Administrator.\u0026#34; elif version_gt \u0026#34;$macos_version\u0026#34; \u0026#34;$MACOS_LATEST_SUPPORTED\u0026#34; || \\ version_lt \u0026#34;$macos_version\u0026#34; \u0026#34;$MACOS_OLDEST_SUPPORTED\u0026#34;; then who=\u0026#34;We\u0026#34; what=\u0026#34;\u0026#34; if version_gt \u0026#34;$macos_version\u0026#34; \u0026#34;$MACOS_LATEST_SUPPORTED\u0026#34;; then what=\u0026#34;pre-release version\u0026#34; else who+=\u0026#34; (and Apple)\u0026#34; what=\u0026#34;old version\u0026#34; fi ohai \u0026#34;You are using macOS ${macos_version}.\u0026#34; ohai \u0026#34;${who} do not provide support for this ${what}.\u0026#34; echo \u0026#34;$(cat \u0026lt;\u0026lt;EOS This installation may not succeed. After installation, you will encounter build failures with some formulae. Please create pull requests instead of asking for help on Homebrew\\\u0026#39;s GitHub, Discourse, Twitter or IRC. You are responsible for resolving any issues you experience while you are running this ${what}. EOS ) \u0026#34; fi fi ohai \u0026#34;This script will install:\u0026#34; echo \u0026#34;${HOMEBREW_PREFIX}/bin/brew\u0026#34; echo \u0026#34;${HOMEBREW_PREFIX}/share/doc/homebrew\u0026#34; echo \u0026#34;${HOMEBREW_PREFIX}/share/man/man1/brew.1\u0026#34; echo \u0026#34;${HOMEBREW_PREFIX}/share/zsh/site-functions/_brew\u0026#34; echo \u0026#34;${HOMEBREW_PREFIX}/etc/bash_completion.d/brew\u0026#34; echo \u0026#34;${HOMEBREW_REPOSITORY}\u0026#34; # Keep relatively in sync with # https://github.com/Homebrew/brew/blob/master/Library/Homebrew/keg.rb directories=(bin etc include lib sbin share opt var Frameworks etc/bash_completion.d lib/pkgconfig share/aclocal share/doc share/info share/locale share/man share/man/man1 share/man/man2 share/man/man3 share/man/man4 share/man/man5 share/man/man6 share/man/man7 share/man/man8 var/log var/homebrew var/homebrew/linked bin/brew) group_chmods=() for dir in \u0026#34;${directories[@]}\u0026#34;; do if exists_but_not_writable \u0026#34;${HOMEBREW_PREFIX}/${dir}\u0026#34;; then group_chmods+=(\u0026#34;${HOMEBREW_PREFIX}/${dir}\u0026#34;) fi done # zsh refuses to read from these directories if group writable directories=(share/zsh share/zsh/site-functions) zsh_dirs=() for dir in \u0026#34;${directories[@]}\u0026#34;; do zsh_dirs+=(\u0026#34;${HOMEBREW_PREFIX}/${dir}\u0026#34;) done directories=(bin etc include lib sbin share var opt share/zsh share/zsh/site-functions var/homebrew var/homebrew/linked Cellar Caskroom Homebrew Frameworks) mkdirs=() for dir in \u0026#34;${directories[@]}\u0026#34;; do if ! [[ -d \u0026#34;${HOMEBREW_PREFIX}/${dir}\u0026#34; ]]; then mkdirs+=(\u0026#34;${HOMEBREW_PREFIX}/${dir}\u0026#34;) fi done user_chmods=() if [[ \u0026#34;${#zsh_dirs[@]}\u0026#34; -gt 0 ]]; then for dir in \u0026#34;${zsh_dirs[@]}\u0026#34;; do if user_only_chmod \u0026#34;${dir}\u0026#34;; then user_chmods+=(\u0026#34;${dir}\u0026#34;) fi done fi chmods=() if [[ \u0026#34;${#group_chmods[@]}\u0026#34; -gt 0 ]]; then chmods+=(\u0026#34;${group_chmods[@]}\u0026#34;) fi if [[ \u0026#34;${#user_chmods[@]}\u0026#34; -gt 0 ]]; then chmods+=(\u0026#34;${user_chmods[@]}\u0026#34;) fi chowns=() chgrps=() if [[ \u0026#34;${#chmods[@]}\u0026#34; -gt 0 ]]; then for dir in \u0026#34;${chmods[@]}\u0026#34;; do if file_not_owned \u0026#34;${dir}\u0026#34;; then chowns+=(\u0026#34;${dir}\u0026#34;) fi if file_not_grpowned \u0026#34;${dir}\u0026#34;; then chgrps+=(\u0026#34;${dir}\u0026#34;) fi done fi if [[ \u0026#34;${#group_chmods[@]}\u0026#34; -gt 0 ]]; then ohai \u0026#34;The following existing directories will be made group writable:\u0026#34; printf \u0026#34;%s\\n\u0026#34; \u0026#34;${group_chmods[@]}\u0026#34; fi if [[ \u0026#34;${#user_chmods[@]}\u0026#34; -gt 0 ]]; then ohai \u0026#34;The following existing directories will be made writable by user only:\u0026#34; printf \u0026#34;%s\\n\u0026#34; \u0026#34;${user_chmods[@]}\u0026#34; fi if [[ \u0026#34;${#chowns[@]}\u0026#34; -gt 0 ]]; then ohai \u0026#34;The following existing directories will have their owner set to ${tty_underline}${USER}${tty_reset}:\u0026#34; printf \u0026#34;%s\\n\u0026#34; \u0026#34;${chowns[@]}\u0026#34; fi if [[ \u0026#34;${#chgrps[@]}\u0026#34; -gt 0 ]]; then ohai \u0026#34;The following existing directories will have their group set to ${tty_underline}${GROUP}${tty_reset}:\u0026#34; printf \u0026#34;%s\\n\u0026#34; \u0026#34;${chgrps[@]}\u0026#34; fi if [[ \u0026#34;${#mkdirs[@]}\u0026#34; -gt 0 ]]; then ohai \u0026#34;The following new directories will be created:\u0026#34; printf \u0026#34;%s\\n\u0026#34; \u0026#34;${mkdirs[@]}\u0026#34; fi if should_install_command_line_tools; then ohai \u0026#34;The Xcode Command Line Tools will be installed.\u0026#34; fi if [[ -t 0 \u0026amp;\u0026amp; -z \u0026#34;${CI-}\u0026#34; ]]; then wait_for_user fi if [[ -d \u0026#34;${HOMEBREW_PREFIX}\u0026#34; ]]; then if [[ \u0026#34;${#chmods[@]}\u0026#34; -gt 0 ]]; then execute_sudo \u0026#34;/bin/chmod\u0026#34; \u0026#34;u+rwx\u0026#34; \u0026#34;${chmods[@]}\u0026#34; fi if [[ \u0026#34;${#group_chmods[@]}\u0026#34; -gt 0 ]]; then execute_sudo \u0026#34;/bin/chmod\u0026#34; \u0026#34;g+rwx\u0026#34; \u0026#34;${group_chmods[@]}\u0026#34; fi if [[ \u0026#34;${#user_chmods[@]}\u0026#34; -gt 0 ]]; then execute_sudo \u0026#34;/bin/chmod\u0026#34; \u0026#34;755\u0026#34; \u0026#34;${user_chmods[@]}\u0026#34; fi if [[ \u0026#34;${#chowns[@]}\u0026#34; -gt 0 ]]; then execute_sudo \u0026#34;$CHOWN\u0026#34; \u0026#34;$USER\u0026#34; \u0026#34;${chowns[@]}\u0026#34; fi if [[ \u0026#34;${#chgrps[@]}\u0026#34; -gt 0 ]]; then execute_sudo \u0026#34;$CHGRP\u0026#34; \u0026#34;$GROUP\u0026#34; \u0026#34;${chgrps[@]}\u0026#34; fi else execute_sudo \u0026#34;/bin/mkdir\u0026#34; \u0026#34;-p\u0026#34; \u0026#34;${HOMEBREW_PREFIX}\u0026#34; if [[ -z \u0026#34;${HOMEBREW_ON_LINUX-}\u0026#34; ]]; then execute_sudo \u0026#34;$CHOWN\u0026#34; \u0026#34;root:wheel\u0026#34; \u0026#34;${HOMEBREW_PREFIX}\u0026#34; else execute_sudo \u0026#34;$CHOWN\u0026#34; \u0026#34;$USER:$GROUP\u0026#34; \u0026#34;${HOMEBREW_PREFIX}\u0026#34; fi fi if [[ \u0026#34;${#mkdirs[@]}\u0026#34; -gt 0 ]]; then execute_sudo \u0026#34;/bin/mkdir\u0026#34; \u0026#34;-p\u0026#34; \u0026#34;${mkdirs[@]}\u0026#34; execute_sudo \u0026#34;/bin/chmod\u0026#34; \u0026#34;g+rwx\u0026#34; \u0026#34;${mkdirs[@]}\u0026#34; execute_sudo \u0026#34;$CHOWN\u0026#34; \u0026#34;$USER\u0026#34; \u0026#34;${mkdirs[@]}\u0026#34; execute_sudo \u0026#34;$CHGRP\u0026#34; \u0026#34;$GROUP\u0026#34; \u0026#34;${mkdirs[@]}\u0026#34; fi if ! [[ -d \u0026#34;${HOMEBREW_CACHE}\u0026#34; ]]; then if [[ -z \u0026#34;${HOMEBREW_ON_LINUX-}\u0026#34; ]]; then execute_sudo \u0026#34;/bin/mkdir\u0026#34; \u0026#34;-p\u0026#34; \u0026#34;${HOMEBREW_CACHE}\u0026#34; else execute \u0026#34;/bin/mkdir\u0026#34; \u0026#34;-p\u0026#34; \u0026#34;${HOMEBREW_CACHE}\u0026#34; fi fi if exists_but_not_writable \u0026#34;${HOMEBREW_CACHE}\u0026#34;; then execute_sudo \u0026#34;/bin/chmod\u0026#34; \u0026#34;g+rwx\u0026#34; \u0026#34;${HOMEBREW_CACHE}\u0026#34; fi if file_not_owned \u0026#34;${HOMEBREW_CACHE}\u0026#34;; then execute_sudo \u0026#34;$CHOWN\u0026#34; \u0026#34;$USER\u0026#34; \u0026#34;${HOMEBREW_CACHE}\u0026#34; fi if file_not_grpowned \u0026#34;${HOMEBREW_CACHE}\u0026#34;; then execute_sudo \u0026#34;$CHGRP\u0026#34; \u0026#34;$GROUP\u0026#34; \u0026#34;${HOMEBREW_CACHE}\u0026#34; fi if [[ -d \u0026#34;${HOMEBREW_CACHE}\u0026#34; ]]; then execute \u0026#34;$TOUCH\u0026#34; \u0026#34;${HOMEBREW_CACHE}/.cleaned\u0026#34; fi if should_install_command_line_tools \u0026amp;\u0026amp; version_ge \u0026#34;$macos_version\u0026#34; \u0026#34;10.13\u0026#34;; then ohai \u0026#34;Searching online for the Command Line Tools\u0026#34; # This temporary file prompts the \u0026#39;softwareupdate\u0026#39; utility to list the Command Line Tools clt_placeholder=\u0026#34;/tmp/.com.apple.dt.CommandLineTools.installondemand.in-progress\u0026#34; execute_sudo \u0026#34;$TOUCH\u0026#34; \u0026#34;$clt_placeholder\u0026#34; clt_label_command=\u0026#34;/usr/sbin/softwareupdate -l | grep -B 1 -E \u0026#39;Command Line Tools\u0026#39; | awk -F\u0026#39;*\u0026#39; \u0026#39;/^ *\\\\*/ {print \\$2}\u0026#39; | sed -e \u0026#39;s/^ *Label: //\u0026#39; -e \u0026#39;s/^ *//\u0026#39; | sort -V | tail -n1\u0026#34; clt_label=\u0026#34;$(chomp \u0026#34;$(/bin/bash -c \u0026#34;$clt_label_command\u0026#34;)\u0026#34;)\u0026#34; if [[ -n \u0026#34;$clt_label\u0026#34; ]]; then ohai \u0026#34;Installing $clt_label\u0026#34; execute_sudo \u0026#34;/usr/sbin/softwareupdate\u0026#34; \u0026#34;-i\u0026#34; \u0026#34;$clt_label\u0026#34; execute_sudo \u0026#34;/bin/rm\u0026#34; \u0026#34;-f\u0026#34; \u0026#34;$clt_placeholder\u0026#34; execute_sudo \u0026#34;/usr/bin/xcode-select\u0026#34; \u0026#34;--switch\u0026#34; \u0026#34;/Library/Developer/CommandLineTools\u0026#34; fi fi # Headless install may have failed, so fallback to original \u0026#39;xcode-select\u0026#39; method if should_install_command_line_tools \u0026amp;\u0026amp; test -t 0; then ohai \u0026#34;Installing the Command Line Tools (expect a GUI popup):\u0026#34; execute_sudo \u0026#34;/usr/bin/xcode-select\u0026#34; \u0026#34;--install\u0026#34; echo \u0026#34;Press any key when the installation has completed.\u0026#34; getc execute_sudo \u0026#34;/usr/bin/xcode-select\u0026#34; \u0026#34;--switch\u0026#34; \u0026#34;/Library/Developer/CommandLineTools\u0026#34; fi if [[ -z \u0026#34;${HOMEBREW_ON_LINUX-}\u0026#34; ]] \u0026amp;\u0026amp; ! output=\u0026#34;$(/usr/bin/xcrun clang 2\u0026gt;\u0026amp;1)\u0026#34; \u0026amp;\u0026amp; [[ \u0026#34;$output\u0026#34; == *\u0026#34;license\u0026#34;* ]]; then abort \u0026#34;$(cat \u0026lt;\u0026lt;EOABORT You have not agreed to the Xcode license. Before running the installer again please agree to the license by opening Xcode.app or running: sudo xcodebuild -license EOABORT )\u0026#34; fi ohai \u0026#34;Downloading and installing Homebrew...\u0026#34; ( cd \u0026#34;${HOMEBREW_REPOSITORY}\u0026#34; \u0026gt;/dev/null || return # we do it in four steps to avoid merge errors when reinstalling execute \u0026#34;git\u0026#34; \u0026#34;init\u0026#34; \u0026#34;-q\u0026#34; # \u0026#34;git remote add\u0026#34; will fail if the remote is defined in the global config execute \u0026#34;git\u0026#34; \u0026#34;config\u0026#34; \u0026#34;remote.origin.url\u0026#34; \u0026#34;${BREW_REPO}\u0026#34; execute \u0026#34;git\u0026#34; \u0026#34;config\u0026#34; \u0026#34;remote.origin.fetch\u0026#34; \u0026#34;+refs/heads/*:refs/remotes/origin/*\u0026#34; # ensure we don\u0026#39;t munge line endings on checkout execute \u0026#34;git\u0026#34; \u0026#34;config\u0026#34; \u0026#34;core.autocrlf\u0026#34; \u0026#34;false\u0026#34; execute \u0026#34;git\u0026#34; \u0026#34;fetch\u0026#34; \u0026#34;origin\u0026#34; \u0026#34;--force\u0026#34; execute \u0026#34;git\u0026#34; \u0026#34;fetch\u0026#34; \u0026#34;origin\u0026#34; \u0026#34;--tags\u0026#34; \u0026#34;--force\u0026#34; execute \u0026#34;git\u0026#34; \u0026#34;reset\u0026#34; \u0026#34;--hard\u0026#34; \u0026#34;origin/master\u0026#34; execute \u0026#34;ln\u0026#34; \u0026#34;-sf\u0026#34; \u0026#34;${HOMEBREW_REPOSITORY}/bin/brew\u0026#34; \u0026#34;${HOMEBREW_PREFIX}/bin/brew\u0026#34; execute \u0026#34;cd\u0026#34; \u0026#34;${HOMEBREW_PREFIX}/Homebrew/Library/Taps/homebrew\u0026#34; execute \u0026#34;git\u0026#34; \u0026#34;clone\u0026#34; \u0026#34;https://mirrors.ustc.edu.cn/homebrew-cask.git\u0026#34; execute \u0026#34;git\u0026#34; \u0026#34;clone\u0026#34; \u0026#34;https://mirrors.ustc.edu.cn/homebrew-core.git\u0026#34; execute \u0026#34;${HOMEBREW_PREFIX}/bin/brew\u0026#34; \u0026#34;update\u0026#34; \u0026#34;--force\u0026#34; \u0026#34;-v\u0026#34; execute \u0026#34;echo\u0026#34; \u0026#34;export\u0026#34; \u0026#34;HOMEBREW_BOTTLE_DOMAIN=https://mirrors.ustc.edu.cn/homebrew-bottles\u0026#34; \u0026#34;\u0026gt;\u0026gt;\u0026#34; \u0026#34;~/.zshrc\u0026#34; execute \u0026#34;source\u0026#34; \u0026#34;~/.zshrc\u0026#34; ) if [[ \u0026#34;:${PATH}:\u0026#34; != *\u0026#34;:${HOMEBREW_PREFIX}/bin:\u0026#34;* ]]; then warn \u0026#34;${HOMEBREW_PREFIX}/bin is not in your PATH.\u0026#34; fi ohai \u0026#34;Installation successful!\u0026#34; echo # Use the shell\u0026#39;s audible bell. if [[ -t 1 ]]; then printf \u0026#34;\\a\u0026#34; fi # Use an extra newline and bold to avoid this being missed. ohai \u0026#34;Homebrew has enabled anonymous aggregate formulae and cask analytics.\u0026#34; echo \u0026#34;$(cat \u0026lt;\u0026lt;EOS ${tty_bold}Read the analytics documentation (and how to opt-out) here: ${tty_underline}https://docs.brew.sh/Analytics${tty_reset} No analytics data has been sent yet (or will be during this \\`install\\` run). EOS ) \u0026#34; ohai \u0026#34;Homebrew is run entirely by unpaid volunteers. Please consider donating:\u0026#34; echo \u0026#34;$(cat \u0026lt;\u0026lt;EOS ${tty_underline}https://github.com/Homebrew/brew#donations${tty_reset} EOS ) \u0026#34; ( cd \u0026#34;${HOMEBREW_REPOSITORY}\u0026#34; \u0026gt;/dev/null || return execute \u0026#34;git\u0026#34; \u0026#34;config\u0026#34; \u0026#34;--replace-all\u0026#34; \u0026#34;homebrew.analyticsmessage\u0026#34; \u0026#34;true\u0026#34; execute \u0026#34;git\u0026#34; \u0026#34;config\u0026#34; \u0026#34;--replace-all\u0026#34; \u0026#34;homebrew.caskanalyticsmessage\u0026#34; \u0026#34;true\u0026#34; ) ohai \u0026#34;Next steps:\u0026#34; echo \u0026#34;- Run \\`brew help\\` to get started\u0026#34; echo \u0026#34;- Further documentation: \u0026#34; echo \u0026#34; ${tty_underline}https://docs.brew.sh${tty_reset}\u0026#34; if [[ -n \u0026#34;${HOMEBREW_ON_LINUX-}\u0026#34; ]]; then case \u0026#34;$SHELL\u0026#34; in */bash*) if [[ -r \u0026#34;$HOME/.bash_profile\u0026#34; ]]; then shell_profile=\u0026#34;$HOME/.bash_profile\u0026#34; else shell_profile=\u0026#34;$HOME/.profile\u0026#34; fi ;; */zsh*) shell_profile=\u0026#34;$HOME/.zprofile\u0026#34; ;; *) shell_profile=\u0026#34;$HOME/.profile\u0026#34; ;; esac cat \u0026lt;\u0026lt;EOS - Install the Homebrew dependencies if you have sudo access: ${tty_bold}Debian, Ubuntu, etc.${tty_reset} sudo apt-get install build-essential ${tty_bold}Fedora, Red Hat, CentOS, etc.${tty_reset} sudo yum groupinstall \u0026#39;Development Tools\u0026#39; See ${tty_underline}https://docs.brew.sh/linux${tty_reset} for more information. - Configure Homebrew in your ${tty_underline}${shell_profile}${tty_reset} by running echo \u0026#39;eval \\$(${HOMEBREW_PREFIX}/bin/brew shellenv)\u0026#39; \u0026gt;\u0026gt; ${shell_profile} - Add Homebrew to your ${tty_bold}PATH${tty_reset} eval \\$(${HOMEBREW_PREFIX}/bin/brew shellenv) - We recommend that you install GCC by running: brew install gcc EOS fi ","date":"2020-05-16T00:20:48+08:00","permalink":"https://blog.happyhacker.fun/p/%E4%B8%AD%E5%9B%BD%E5%A4%A7%E9%99%86%E5%AE%89%E8%A3%85homebrew%E7%9A%84%E6%9C%80%E4%BC%98%E6%96%B9%E6%B3%95/","title":"中国大陆安装homebrew的最优方法"},{"content":"Java的stream api真是功能强大，但写的时候总是忘，这里简单记录以下。\n# 概念 经常会搞混淆的应该就是Stream和Collection的区别。从定义上讲，Collection是一个内存数据结构，它包含了这个数据结构拥有的所有元素，每个元素都必须是确定的。也就是说，一个元素在加入到一个集合（Collection）中之前一定是计算好了的、确定的。而Stream从概念上固定的数据结构，它里面的元素可以按需计算。\n差异如下：\n流并不存储其元素。这些元素可能存储在底层的集合中，或者是按需生成的。 流的操作不会修改其数据源。filter方法不会从流中移除元素，而是会生成一个新的流。 流的操作是尽可能惰性的，这意味着直至需要结果时，操作才会执行。 # 创建Stream的几种方式 # 1. Stream.Of(val1, val2, val3) 1 2 3 4 5 6 7 public class StreamBuilders { public static void main(String[] args) { Stream\u0026lt;Integer\u0026gt; stream = Stream.of(1,2,3,4,5,6,7,8,9); stream.forEach(p -\u0026gt; System.out.println(p)); // stream.forEach(System.out::println); } } # 2. Stream.of(arrayOfElements) 1 2 3 4 5 6 public class StreamBuilders { public static void main(String[] args) { Stream\u0026lt;Integer\u0026gt; stream = Stream.of(new Integer[]{1,2,3,4,5,6,7,8,9}); stream.forEach(System.out::println); } } # 3. List.stream() 1 2 3 4 5 6 7 8 9 10 11 public class StreamBuilders { public static void main(String[] args) { List\u0026lt;Integer\u0026gt; list = new ArrayList\u0026lt;\u0026gt;(); for (int i = 0; i \u0026lt; 10; i++) { list.add(i); } Stream\u0026lt;Integer\u0026gt; stream = list.stream(); stream.forEach(System.out::println); } } # 4. Stream.generate()或Stream.iterate() 1 2 3 4 5 6 public class StreamBuilders { public static void main(String[] args) { Stream\u0026lt;Date\u0026gt; stream = Stream.generate(Date::new); stream.forEach(System.out::println); } } # 5. String chars或String tokens 1 2 3 4 5 6 public class StreamBuilders { public static void main(String[] args) { IntStream stream = \u0026#34;12345_abcde\u0026#34;.chars(); stream.forEach(System.out::println); } } # 6. Map通过entrySet().stream() 1 2 3 4 5 6 7 8 9 10 public class StreamBuilders { public static void main(String[] args) { Map\u0026lt;Integer, String\u0026gt; map = new HashMap\u0026lt;\u0026gt;(); for (int i = 0; i \u0026lt; 10; i++) { map.put(i, Character.toString(i+96)); } Stream\u0026lt;Map.Entry\u0026lt;Integer, String\u0026gt;\u0026gt; stream = map.entrySet().stream(); stream.forEach(System.out::println); } } # 中间操作和终止操作 1 2 3 4 5 6 7 8 9 10 11 12 13 public class StreamBuilders { private static final List\u0026lt;String\u0026gt; memberNames = new ArrayList\u0026lt;\u0026gt;(); static { memberNames.add(\u0026#34;Amitabh\u0026#34;); memberNames.add(\u0026#34;Shekhar\u0026#34;); memberNames.add(\u0026#34;Aman\u0026#34;); memberNames.add(\u0026#34;Rahul\u0026#34;); memberNames.add(\u0026#34;Shahrukh\u0026#34;); memberNames.add(\u0026#34;Salman\u0026#34;); memberNames.add(\u0026#34;Yana\u0026#34;); memberNames.add(\u0026#34;Lokesh\u0026#34;); } } # 中间操作 1 2 3 4 5 6 7 8 9 10 11 public static void main(String[] args) { memberNames.stream().filter(s -\u0026gt; s.startsWith(\u0026#34;A\u0026#34;)) .forEach(System.out::println); memberNames.stream().filter(s -\u0026gt; s.startsWith(\u0026#34;S\u0026#34;)) .map(String::toUpperCase) .forEach(System.out::println); memberNames.stream().sorted() .map(String::toUpperCase) .forEach(System.out::println); } # 终止操作 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 memberNames.forEach(System.out::println); System.out.println(memberNames.stream().map(String::toLowerCase) .collect(Collectors.toList())); boolean b1 = memberNames.stream().anyMatch(s -\u0026gt; s.startsWith(\u0026#34;A\u0026#34;)); System.out.println(b1); boolean b2 = memberNames.stream().allMatch(s -\u0026gt; s.startsWith(\u0026#34;A\u0026#34;)); System.out.println(b2); boolean b3 = memberNames.stream().noneMatch(s -\u0026gt; s.startsWith(\u0026#34;A\u0026#34;)); System.out.println(b3); long count = memberNames.stream().filter(s -\u0026gt; s.startsWith(\u0026#34;S\u0026#34;)).count(); System.out.println(count); Optional\u0026lt;String\u0026gt; reduced = memberNames.stream() .reduce((s1, s2) -\u0026gt; s1 + \u0026#34;#\u0026#34; + s2); reduced.ifPresent(System.out::println); # map和flatMap 这两个真是最容易搞混的，不过如果要搞清楚他们的区别也很简单：map就是一个转换，把原来是a的转换成b，原来是List\u0026lt;String\u0026gt;，map之后还是List\u0026lt;String\u0026gt;（其实是stream，是类型没有变），而flatMap会生成一个新的stream把原先的多个stream合并在一起。举例说明下\n1 2 3 4 5 // 例1 List\u0026lt;String\u0026gt; lower = Arrays.asList(\u0026#34;a\u0026#34;, \u0026#34;b\u0026#34;, \u0026#34;c\u0026#34;, \u0026#34;d\u0026#34;); List\u0026lt;String\u0026gt; upper = lower.stream().map(String::toUpperCase).collect(Collectors.toList()); System.out.println(lower); System.out.println(upper); 从这个例子可以看出，通过map，让这个List\u0026lt;String\u0026gt;中的每个元素都执行了e.toUpperCase()方法，输出的结果就是\n1 2 [a, b, c, d] [A, B, C, D] 下面看flatMap可以实现什么功能\n1 2 3 4 5 6 7 8 // 例2 List\u0026lt;List\u0026lt;String\u0026gt;\u0026gt; packed = new ArrayList\u0026lt;\u0026gt;(); packed.add(lower); packed.add(upper); System.out.println(packed); List\u0026lt;String\u0026gt; flat = packed.stream().flatMap(s -\u0026gt; s.stream()).map(String::toUpperCase).collect(Collectors.toList()); System.out.println(flat); 这里必须要注意一点，map()的输出是stream中的一级元素，像例1中的String::toUpperCase显然是输入a，输出A。而对于flatMap而言，它的输入是一个Collection，而输出是一个stream，那怎么形成一个stream呢，像例2中的packed，它就是由两个List\u0026lt;String\u0026gt;组成的，对他们调用stream()方法就让它返回一个stream到flatMap的输出了。\n举个不太恰当的例子，这里有3包牛奶，map方法只能把3包牛奶倒到3个杯子里，而flatMap可以把它们倒到1个杯子里。如何实现呢？当然就是在flatMap中把每个牛奶的袋子撕开，然后倒出来。\n例2的输出如下\n1 2 [[a, b, c, d], [A, B, C, D]] [A, B, C, D, A, B, C, D] 理解到了这一步，基本上就搞清楚了二者的区别了，总之就是记住一定要让flatMap()输出一个stream。\n# Map的stream() List输出到stream的方法很容易理解，因为它本身就是一个一个的元素，但Map是分了key和value的，要怎么才能把它转换成stream呢？答案是Map.Entry。可能第一门语言就是Java的同学觉得很理所应当，但熟悉PHP的同学再来理解这个概念就有点对应不上了（因为PHP里基本上不区分List和Map，一切皆为数组）。先看一个PHP的例子\n1 2 3 4 5 6 7 8 $a = [ \u0026#39;a\u0026#39; =\u0026gt; \u0026#39;b\u0026#39;, \u0026#39;c\u0026#39; =\u0026gt; \u0026#39;d\u0026#39;, ]; foreach ($a as $key =\u0026gt; $value) { echo $key . \u0026#39;=\u0026gt;\u0026#39; . $value, \u0026#34;\\n\u0026#34;; } 在这个例子中$key和$value的组合就是Java中Map.Entry的概念了，只不过继续遵循封闭的原则，给二者都配备了对应的方法getKey()和getValue()，也就是说，可以从一个Map的Entry中同时获取当前这个元素的key和value。那么问题又来了，怎么拿到它的Entry呢？通过entrySet()方法。\n对一个Map调用entrySet()方法，相当于新建了一个List，它的元素是这个Map的Entry，这就又回到了List，当然也就可以用stream api了。比如要把一个Map的key和value倒过来（PHP的array_flip方法）\n1 2 3 4 5 6 7 8 9 10 Map\u0026lt;String, String\u0026gt; g = new HashMap\u0026lt;\u0026gt;(); g.put(\u0026#34;k1\u0026#34;, \u0026#34;v1\u0026#34;); g.put(\u0026#34;k2\u0026#34;, \u0026#34;v2\u0026#34;); g.put(\u0026#34;k3\u0026#34;, \u0026#34;v3\u0026#34;); g.put(\u0026#34;k4\u0026#34;, \u0026#34;v4\u0026#34;); g.put(\u0026#34;k5\u0026#34;, \u0026#34;v5\u0026#34;); val h = g.entrySet().stream().collect(Collectors.toMap(Map.Entry::getValue, Map.Entry::getKey)); System.out.println(g); System.out.println(h); 结果如下\n1 2 {k1=v1, k2=v2, k3=v3, k4=v4, k5=v5} {v1=k1, v2=k2, v3=k3, v4=k4, v5=k5} 注意这个方法可能会有异常，因为如果原始输入中不同的key对应了相同的value，就无法生成新map了\n1 2 3 4 5 6 7 8 9 10 11 Exception in thread \u0026#34;main\u0026#34; java.lang.IllegalStateException: Duplicate key v3 (attempted merging values k3 and k4) at java.base/java.util.stream.Collectors.duplicateKeyException(Collectors.java:133) at java.base/java.util.stream.Collectors.lambda$uniqKeysMapAccumulator$1(Collectors.java:180) at java.base/java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169) at java.base/java.util.HashMap$EntrySpliterator.forEachRemaining(HashMap.java:1746) at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:484) at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:474) at java.base/java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:913) at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234) at java.base/java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:578) at fun.happyhacker.java.stream.MapDemo.main(MapDemo.java:56) Map还有一个方法，可以把它的所有key输出到一个List，PHPer肯定又想到了array_keys()，没错，就是keySet()\n# collect 很多时候前面的一堆操作都是要把结果收集起来，这个话题太大了，collect的方式多种多样，这里也说不完，简单说几个最常用的。\n# collect(Collectors.toList()) 这是最简单的，把一个stream中的所有元素按前面的输出收集到一个List中。而这个List的类型，当然就取决于这个stream中的元素了，参考例1的代码即可。\n# collect(Collectors.toMap()) 这个就相对复杂一点了，既然把结果收集成map，那就肯定得设置key和value，比如我们要把一个小写字母的列表和它对应的大写字母分别对应起来，\n1 2 3 4 // 例3 List\u0026lt;String\u0026gt; lower = Arrays.asList(\u0026#34;a\u0026#34;, \u0026#34;b\u0026#34;, \u0026#34;c\u0026#34;, \u0026#34;d\u0026#34;); val f = lower.stream().collect(Collectors.toMap(e -\u0026gt; e, String::toUpperCase)); System.out.println(f); 输出如下\n1 {a=A, b=B, c=C, d=D} ","date":"2020-04-29T23:23:57+08:00","permalink":"https://blog.happyhacker.fun/p/java-stream-api/","title":"Java Stream Api"},{"content":"前段时间写Java一直忙于做工程上的工作，还是得停下来看看理论的东西。\n# 基本类型 类型 存储需求 取值范围 备注 int 4 Bytes $-2^{31}$ ~ $2^{31}-1$ short 2 Bytes $-2^{15}$ ~ $2^{15}-1$ long 8 Bytes $-2^{63}$ ~ $2^{63}-1$ byte 1 Byte $-2^7$ ~ $2^7-1$ float 4 Bytes 大约 $3.40282347E+38F$（有效位数6～7位） 只有很少场景可以用到float double 8 Bytes 大约 $1.79769313486231570E+308$（有效位数15位） 带小数点的默认是double char - char描述了UTF-16编码中的一个代码单元，尽量不要使用 boolean 1 bit true 和 false # 位运算 \u0026amp; and | or ^ xor ~ not 以(n \u0026amp; 0b1000) / 0b1000为例，如果整数n的二进制表示从左到右第4位是1，结果就是1，其余情况则为0。利用\u0026amp;并结合适当的2的幂，可以把其他位mask掉，而只留下其中一位。\n\u0026gt;\u0026gt;和\u0026lt;\u0026lt;运算符可以将位模式左移或者右移，需要建立位模式来完成掩码时，这两个运算符很方便。\n1 \u0026lt;\u0026lt; 3 == 0b1000 == 8\n\u0026gt;\u0026gt;\u0026gt;运算符会用0填充高位，这与\u0026gt;\u0026gt;不同，后者用符号填充高位。不存在\u0026lt;\u0026lt;\u0026lt;运算符。\n# 输入输出 Scanner就不多说了，只说比较有意思也最可能出现问题的Console.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 package fun.happyhacker.java.basics; import java.io.Console; import java.util.Scanner; public class IO { public static void main(String[] args) { IO io = new IO(); io.console(); } private void scanner() { Scanner in = new Scanner(System.in); System.out.println(\u0026#34;What is your name?\u0026#34;); String name = in.nextLine(); System.out.println(\u0026#34;How old are you?\u0026#34;); int age = in.nextInt(); System.out.println(\u0026#34;Hello \u0026#34; + name + \u0026#34;, you\u0026#39;ll be \u0026#34; + (age + 1) + \u0026#34; next year!\u0026#34;); } private void console() { Console console = System.console(); String username = console.readLine(\u0026#34;User name: \u0026#34;); char[] password = console.readPassword(\u0026#34;Password: \u0026#34;); System.out.println(username); System.out.println(password); } } 然后很开心的执行一下，结果发现报错了\n1 2 3 4 5 Exception in thread \u0026#34;main\u0026#34; java.lang.NullPointerException at fun.happyhacker.java.basics.IO.console(IO.java:25) at fun.happyhacker.java.basics.IO.main(IO.java:9) Process finished with exit code 1 这个原因是\n\u0026ldquo;If the virtual machine is started from an interactive command line without redirecting the standard input and output streams then its console will exist and will typically be connected to the keyboard and display from which the virtual machine was launched. If the virtual machine is started automatically, for example by a background job scheduler, then it will typically not have a console.\u0026rdquo;\n简单说就是如果它是从命令行直接启动的就没问题，而如果是从一个【后台工作调度器】，其实也就是IDEA的工作线程启动的，就没有console了。所以要执行带有console的应用，就需要javac App.java \u0026amp;\u0026amp; java App了。\n# 数组 Arrays和List在Java中是不同的数据类型（神奇的是为什么还有一个ArrayList）。。。 Array数组表示的是【定长的数组，一旦确定了长度就不能再改变了】，而List列表则表示可以改变长度的列表。 Array的创建方式有很多，如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 package fun.happyhacker.java.basics; import java.util.Arrays; public class Array { public static void main(String[] args) { // 总之数组就是定长的，要么给定长度，要么给定元素让它自己计算长度 int[] a = new int[10]; int n = 100; int[] b = new int[n]; int[] c = {1,2,3,4,}; int[] d = new int[]{1,2,3,4,}; // 还有一点需要注意，下面的e和f两个变量会共用同一个数组，所以改变其中一个也会改变另外一个 int[] e = {1,2,3,4,}; int[] f = e; f[2] = 5; System.out.println(Arrays.toString(e)); System.out.println(Arrays.toString(f)); // 如果要拷贝数组，则需要用Arrays.copy方法 int[] g = {1,2,3,4,}; int[] h = Arrays.copyOf(g, g.length); h[2] = 8; System.out.println(Arrays.toString(g)); System.out.println(Arrays.toString(h)); } } ","date":"2020-04-21T23:28:34+08:00","permalink":"https://blog.happyhacker.fun/p/java%E5%9F%BA%E7%A1%80/","title":"Java基础"},{"content":"CentOS 8又搞出这么幺蛾子，不过这个接口还挺好用，我喜欢。\nAMD的黑苹果没办法安装Docker，只好虚拟机搞起了，还要硬件性能够强，这点性能损耗不算什么。装的服务器版本的当然要ssh登录，所以需要用桥接的网络接口 默认是dhcp的配置，所以每次开机IP都会变，这样很不方便，于是要给它一个固定的地址。然后就发现CentOS 8又搞了一个nmcli，虽然又是一个轮子，但不得不说，这个接口设计的还挺好。\n# 预期配置 下面是要给虚拟机的配置 IP: 192.168.0.108 Mask: 255.255.255.0 网关: 192.168.0.1 DNS: 114.114.114.114\n# 命令 不出意外的话，第一个网络接口应该是enp0s3，所以下面的命令一气呵成就可以配置成功了\n1 2 3 4 5 6 7 8 [root@localhost network-scripts]# nmcli con NAME UUID TYPE DEVICE enp0s3 f131fbc3-12a2-4d96-bb3a-623aad5c4fda ethernet enp0s3 [root@localhost network-scripts]# nmcli con mod enp0s3 ipv4.addresses 192.168.0.108/24 # 配置IP和Mask [root@localhost network-scripts]# nmcli con mod enp0s3 ipv4.gateway 192.168.0.1 # 配置网关 [root@localhost network-scripts]# nmcli conn mod enp0s3 ipv4.dns \u0026#34;114.114.114.114\u0026#34; # 配置DNS [root@localhost network-scripts]# nmcli con mod enp0s3 ipv4.method manual # 取消dhcp，使用静态地址 [root@localhost network-scripts]# nmcli con up enp0s3 # 重启网络接口 # 配置key登录 然后就可以愉快的在macOS上通过以下命令生成配置使用key登录虚拟机了\n1 2 3 4 5 6 7 8 9 10 $ ssh-copy-id frost@192.168.0.108 /usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: \u0026#34;/Users/frost/.ssh/id_rsa.pub\u0026#34; /usr/bin/ssh-copy-id: INFO: attempting to log in with the new key(s), to filter out any that are already installed /usr/bin/ssh-copy-id: INFO: 1 key(s) remain to be installed -- if you are prompted now it is to install the new keys frost@192.168.0.108\u0026#39;s password: Number of key(s) added: 1 Now try logging into the machine, with: \u0026#34;ssh \u0026#39;frost@192.168.0.108\u0026#39;\u0026#34; and check to make sure that only the key(s) you wanted were added. # 配置快捷登录方式 接着在macOS的$HOME/.ssh/config中添加以下内容\n1 2 3 4 5 6 7 Host * ControlMaster auto ControlPath /tmp/ssh-%r@%h Host dev HostName 192.168.0.108 User frost 这样就可以通过ssh dev这条简短的命令登录了。虽然用key做认证倒也无所谓，不过通常还是配置上上述内容中的前三行，让登录到同一个主机的ssh共享session，也就是说在使用密码登录时，只有第一次需要输入密码，在保留第一个连接时，后续的连接都可以免密。\n# 补充 后来因为买了块新的硬盘重新安装了macOS，把之前安装的CentOS8虚拟机拷贝过来之后无法运行，调整了网络配置（从BridgeNetwork调整到NAT）之后可以开机了但是无法联网，不知道是什么原因。由于之前是配置了静态地址，在NAT模式下想着还是配置成DHCP吧，但发现nmcli并没有提供类似nmcli connection reset这种操作，所以只能删了重新配置。\n1 2 3 nmcli connection delete enp0s3 nmcli connection add type ethernet con-name home ifname enp3s0 nmcli connection up home 即可联网了。关于nmcli还有很多细节的用法，可以参考RedHat官方文档。\n# 总结 每次配置ssh都想起来几年前写的这个小工具ic，当时是为了更简单的配置本地端口转发，实现sftp上传文件，后来公司不让用这种方式了也就作罢了。不过当时还研究了下go语言和ssh的配置，还有homebrew的打包方式，虽然现在都不记得了。。。。\n","date":"2020-04-17T23:02:34+08:00","permalink":"https://blog.happyhacker.fun/p/centos8%E9%85%8D%E7%BD%AE%E9%9D%99%E6%80%81%E5%9C%B0%E5%9D%80/","title":"CentOS8配置静态地址"},{"content":"如果你遇到了crontab不能按预期执行的问题，可以参考本文的内容。\ncrontab是运维工作中经常需要做的，多数时候只需要执行crontab -e来编辑即可，当面对更复杂的场景时，这种方式就显得不够用了。\n关于crontab更多的内容可以通过查看man 8 cron和man 5 crontab来获取。\n# crontab的分类 一般来说，crontab可以分为三种，分别位于以下三个目录中\n/var/spool/cron/ /etc/cron.d /etc/cron.{hourly,daily,weekly,monthly} # 不同的crontab的作用 # /var/spool/cron/ 这里就是存储我们最常用的crontab -e编辑的文件的地方了，这里保存着和执行这个命令的用户名相同的文件，所以当需要用某一个用户名执行cron时，有两种方式，以用户名frost为例\n执行crontab -e vim /var/spool/cron/frost 这两种方式本质上是一样的。值得注意的是，它的文件内容格式如下\n1 * * * * * /path/to/command 先记住这个格式，后面会做对比。\n# /etc/cron.d 这里存储的是更通用的配置，每一行可以由不同的用户执行，没有了用户名做区分应该怎么做呢？这就是它的文件内容和前面的不同的地方了，放在这个目录种的cron文件想要被执行，需要满足下面的格式\n1 * * * * * ${username} /path/to/command 也就是说，在时间格式后面需要指定执行命令所用的用户名，这是非常重要的，因为如果没有这个用户名，这行命令就不会被执行。\n个人认为，这种方式其实更适合批量执行的环境，从中心节点下发crontab相关的配置时，都放在一个文件会导致该文件功能众多，难以维护，相反如果把不同功能的配置文件以不同的名字下发到/etc/cron.d中，则更为清晰，可维护性更好。\n比如在/etc/cron.d中可以有log, check_db等，分别用于执行压缩/删除日志、检查数据库是否可访问等任务，职责清晰明了。\n# /etc/cron.{hourly,daily,weekly,monthly} 这里和前面两种都不同，因为它里面存放的并不是任何类型的crontab的配置，而是脚本，从目录名也可以知道，对应的目录中的脚本会以相应的时间间隔执行。\n","date":"2020-04-14T22:40:01+08:00","permalink":"https://blog.happyhacker.fun/p/crontab%E9%85%8D%E7%BD%AE%E7%9A%84%E4%B8%80%E4%B8%AA%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF/","title":"crontab配置的一个常见错误"},{"content":"Java报错真是多，一不小心单元测试也报错。\n代码是这样的\n1 2 3 4 5 6 7 8 9 @SpringBootTest @Slf4j @RunWith(SpringJUnit4ClassRunner.class) public class MyTestClass { @Test void testShowBatch() { } } 执行mvn test -Dtest=MyTestClass#testShowBatch报错\n1 2 3 4 5 6 7 8 [INFO] Running MyTestClass [ERROR] Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 0.001 s \u0026lt;\u0026lt;\u0026lt; FAILURE! - in MyTestClass [ERROR] initializationError Time elapsed: 0.001 s \u0026lt;\u0026lt;\u0026lt; ERROR! org.junit.runners.model.InvalidTestClassError: Invalid test class \u0026#39;MyTestClass\u0026#39;: 1. No runnable methods ... 事实上这个并不是问题的关键，当时这个问题的原因在于本地代码上传到远端的目录和我执行的目录不在一个地方，所以报错了。。。但总体上在执行单元测试时的用法和下文说的差不多，junit5不需要@RunWith注解了。\n明显是有runnable methods的啊，原来是因为@RunWith(SpringJUnit4ClassRunner.class)这个注解，它是junit4的用法，加上它，就会查找带有@org.junit.Test注解的方法，也就是所谓的runnable methods。而我这里的看起来是@Test的方法，其实是@org.junit.jupiter.api.Test，是junit5的runnable method，二者不能兼容，所以就出现了上面的错误。\n# 总结 在测试Springboot应用时，如果你需要Springboot加载才能执行单元测试代码，可以选择使用junit4或者junit5。\n当使用junit4\n需要的类注解是\n1 2 3 @SpringBootTest @Slf4j @RunWith(SpringJUnit4ClassRunner.class) 需要的方法注解是\n1 @org.junit.Test 当使用junit5 需要的类注解是\n1 2 @SpringBootTest @Slf4j 需要的方法注解是\n1 @org.junit.jupiter.api.Test ","date":"2020-04-08T00:30:39+08:00","permalink":"https://blog.happyhacker.fun/p/%E6%89%A7%E8%A1%8C%E5%8D%95%E5%85%83%E6%B5%8B%E8%AF%95%E6%97%B6%E6%8A%A5%E9%94%99%E5%88%86%E6%9E%90%E5%8F%8A%E8%A7%A3%E5%86%B3/","title":"执行单元测试时报错分析及解决"},{"content":"多年不安装虚拟机，竟然卡在了第一步。\n拣重点提一下容易出问题的地方。\n# 配置联网 # 选择安装源 因为DVD的安装iso太大了，就选了个最小化安装iso。这里配置的地方也没有可以参考的文档，照着这个写就可以了。国内有很多镜像站，可以选一个你自己访问速度最快的。\n后面的步骤就不用说了。\n","date":"2020-04-06T00:10:43+08:00","permalink":"https://blog.happyhacker.fun/p/install-centos8/","title":"Install Centos8"},{"content":"我们用Java是为了让它自动管理内存，然后我的第一个Java项目上线就面临排查线程数过多导致无法创建新线程、内存泄漏导致机器重启各种个样的问题。这和我原本想象的可不一样。\n最近几天比较忙，排查的过程没有有效的记录下来，但其实也不影响最终的结果，因为并不是代码的问题导致的内存泄漏，而是docker的版本问题。\n# 现象描述 我理解的内存泄漏应该是内存的缓慢增长，但实际上从监控曲线看，它总是每隔几分钟就突增一次，每次增加的量大概是物理机内存总量的15%，所以能在30分钟左右就耗尽物理机的内存，最终docker进程挂掉，内存释放。\n注意，这里的现象是docker服务挂掉，而不是运行tomcat的容器挂掉。这是一个很重要的信号，也是在最早期被忽略的线索。\n同时还发现，当内存占用量不停升高时，重启运行tomcat的容器并不能释放内存。这一点很反常规，因为按我们的理解，tomcat是一个jvm进程，当jvm进程重启时，它对应的内存泄漏应该也被释放才对。这时候我们又被绕进了另外一个误区，去排查native memory了，这里按下不表。\n其实从上面的描述看，最可能的问题是在docker上。使用的docker版本是非常老的1.6.2，java版本是adoptopenjdk 1.8.242，按照官方的说法，在jdk 1.8.131之后，jvm已经可以识别cgroup，即可以感知到自己是在容器中运行，因而不会将物理机的总内存认为是jvm可用的总内存。但还是无法解释我们遇到但问题。一筹莫展之际，我尝试了在宿主机上执行运行tomcat，持续运行了几个小时，内存占用率一直在15%左右，非常稳定，内存泄漏的问题不见了。\n那么基本上就锁定是docker的问题了。升级docker版本到1.13.1，问题解决。\n# 排查用到的工具 虽然排查的过程走了不少弯路，但基本上能用到的排查工具也都用到了。这里只记录一下大概，用到的时候还是要查对应的文档。\njps 首先是最基础的jps -lvVm，可以看到整个jvm进程的启动参数，也可以用于验证指定的各种命令行参数是否生效了。\nvisualvm 这个就是比较高端的分析工具了，最重要的是它能连接远程jvm进程，实时查看堆内存和gc的情况，甚至还有很多插件用于查看更多信息。但对于我们的这个情况来说，只能看到堆内存的占用一直维持在一个正常水平，ygc也很正常，整个内存的使用情况呈锯齿状，能说明堆内存没有泄漏。\nperf 这是linux内核支持的工具，可以监听指定进程在一段时间内的所有系统调用。\nmat 全称Memory Analyzer Tool, 可以用于分析生成的hprof文件，这个hprof文件可以是系统崩溃时自动生成的堆内存快照，也可以是由jmap生成。\njmap 生成可供mat分析的内存快照。\n需要注意的是.hprof文件可能会非常大，最好监控起来，一方面是因为除去人为通过jmap生成的情况之外，都是系统崩溃了才会生成，另一方面是文件太大，很可能会快速占满磁盘空间。\n基本上常用的内存检查工具就这些了，关于这些工具的详细使用方法，最好还是参照官方文档，后面如果再遇到需要查内存泄漏的例子，将会在这里补充一些具体的案例。\n","date":"2020-04-05T23:11:28+08:00","permalink":"https://blog.happyhacker.fun/p/java%E5%86%85%E5%AD%98%E6%B3%84%E6%BC%8F%E6%8E%92%E6%9F%A5%E8%AE%B0%E5%BD%95/","title":"Java内存泄漏排查记录"},{"content":"简单的工具最好用。\nlftp是我非常喜欢的一个lftp工具，命令简单。为了避免每次输入密码，可以通过在$HOME/.netrc中添加一行记录来实现\n1 machine 192.168.1.108 login ${my_user_name} password ${my_password} 非常简单。而且这个.netrc并不是lftp专用的，而是一个用在很多开源软件上的公用配置。\n","date":"2020-04-05T11:32:45+08:00","permalink":"https://blog.happyhacker.fun/p/lftp%E8%AE%B0%E5%BD%95%E5%AF%86%E7%A0%81/","title":"lftp记录密码"},{"content":"以下操作基于CentOS8, 理论上应该适用于其他版本的操作系统.\n现在终端默认的编码都已经设置成了\n1 export LANG=en_US.UTF-8 所以, 这里应该是不需要改动的, 只需要安装一下glibc-langpack-*包就可以了, 对于我们的实际使用来说, 其实就是中文包\n1 yum install -y glibc-langpack-zh 即可.\n","date":"2020-04-03T22:14:19+08:00","permalink":"https://blog.happyhacker.fun/p/centos8%E4%B9%B1%E7%A0%81%E5%8F%8A%E8%A7%A3%E5%86%B3/","title":"Centos8乱码及解决"},{"content":"用Java开发新项目，遇到了很多之前没见过的问题，时区算是第二头疼的一个。\n# 现象 数据库中的时间是2020-03-20 00:00:00，但查询时需要2020-03-21 08:00:00才能查到。这明显是差了8个时区。\n# 排查过程 # JDBC连接 搜索该问题，提到的最多的就是在jdbc url中加上时区的配置，应该是Asia/Shanghai和GMT+8(注意encode)都行，我测试的时候两种都没有解决我的问题。 serverTimezone=GMT%2B8\n# 数据库服务器的时间 到数据库服务器执行以下命令，得到的输出如下\n1 2 3 4 5 6 7 mysql\u0026gt; show variables like \u0026#39;%time_zone%\u0026#39;; +------------------+--------+ | Variable_name | Value | +------------------+--------+ | system_time_zone | CST | | time_zone | SYSTEM | +------------------+--------+ 测试环境和线上环境都是这样的配置，看起来问题应该不是出在这里。但还是要提以下这个容易产生混淆的地方，CST这个缩写是有歧义的，起码在指时区这一件事情时，就有多种不同的意思\n但其实在mysql服务的时区这件事上其实是没有歧义的，就是指UTC-6:00的中央标准时间。\n# 服务器本地时间和容器的时间 由于我的代码是在docker中执行的，所以其实更应该关注的是容器中的时间。这时候发现了容器中的时间和宿主机相差了8小时，猜测问题可能是由此引起的。\n# Java的LocalDateTime 因为LocalDateTime是和时区无关的时间，jvm默认它就是UTC时间，所以传过来的2020-03-20 00:00:00会被转换成2020-03-19 16:00:00，这就是最上面的问题的答案了。\n可以通过以下方式验证\n1 2 3 4 5 6 7 8 import java.time.LocalDateTime; public class TimeZoneTest { public static void main(String[] args) { System.out.println(LocalDateTime.now()); } } 执行javac TimeZoneTest.java \u0026amp;\u0026amp; java -cp TimeZoneTest，会发现和预期不符合，准确的说是比当前时间慢了8小时。\n1 2 3 4 5 6 7 8 9 10 11 import java.time.LocalDateTime; import java.util.TimeZone; public class TimeZoneTest { public static void main(String[] args) { TimeZone tz = TimeZone.getTimeZone(\u0026#34;GMT+8\u0026#34;); TimeZone.setDefault(tz); System.out.println(LocalDateTime.now()); } } 同样执行javac TimeZoneTest.java \u0026amp;\u0026amp; java -cp TimeZoneTest，就会发现和当前时间一致了。\n# 解决方案 找到了问题的根本原因，就容易解决了。\n# Springboot 1 2 3 4 5 6 7 8 9 10 11 @SpringBootApplication public class WebApplication { public static void main(String[] args) { SpringApplication.run(WebApplication.class, args); } @PostConstruct void setDefaultTimeZone() { TimeZone.setDefault(TimeZone.getTimeZone(\u0026#34;GMT+8\u0026#34;)); } } # 设定Java命令行参数 java -Duser.timezone=GMT+8 TimeZoneTest\n# 总结 我最终选择的是在jdbc url中添加时区的同时，在SpringBootApplication中设置默认时区，完美的解决了问题。\n","date":"2020-04-01T12:48:50+08:00","permalink":"https://blog.happyhacker.fun/p/java%E6%97%B6%E5%8C%BA%E9%97%AE%E9%A2%98%E7%9A%84%E6%8E%92%E6%9F%A5%E5%92%8C%E5%88%86%E6%9E%90/","title":"Java时区问题的排查和分析"},{"content":"自认为从易用性、自由度、速度和资源占用方面，Firefox（Quantom）已经超越了Chrome，但现在居然出现了兼容性问题。\n虽然火狐优点很多，但不稳定这一点就足以让我放弃了，不知道是最近几次更新的不稳定还是怎样，打开一会儿就假死，在两台电脑上都频繁出现，忍受不了，还是用回Chrome了。\n# 易用性 # 多标签场景优化 我的使用习惯是会打开非常多的标签页，在使用Chrome的时候且不说资源占用会有多大，更大的问题是Chrome的标签页过多时只保留非常非常小窄的tab，甚至都不能看清favico，这样的标签页留着还有什么用呢？\n而Firefox则有一个相对更合适的“标签页最小宽度”，超过这个宽度之后会出现两个人性化的组件：\n横向的左右箭头，解决横向简单查找的问题 向下展开的标签页列表带搜索功能，解决精确查找的问题 而Chrome则是一直挤下去\n# 书签栏位置 这个就完全时个人习惯了，个人一直不喜欢在书签栏放书签，觉得影响了整体的页面高度。但又很多时候需要用到书签栏，所以在Chrome里设置了只有在新打开标签页时才在显示书签栏。而Firefox的书签栏可以放在上下左右，随你喜欢放在什么地方。我喜欢把它放在左侧，需要用的时候Cmd+B打开，不需要的时候关闭。主要是放在左侧不会像放在上方一样打开时遮挡大面积的网页。\n而且还有一个杀手锏应用Firefox Multi-Account Containers，可以在不通的标签页之间设置Cookie隔离，并在标签页有明显的提示，本质上相当于手机的多开应用，同一个浏览器登录多个不通的账号，非常好用。\n","date":"2020-03-31T10:02:07+08:00","permalink":"https://blog.happyhacker.fun/p/%E7%AE%80%E5%8D%95%E8%AF%B4%E8%AF%B4firefox%E5%92%8Cchrome/","title":"简单说说Firefox和Chrome"},{"content":"mac本身安装了ssh服务，默认情况下不会开机自启。本文记录了开启和停止sshd服务的方法。\n# 1. 启动sshd服务： sudo launchctl load -w /System/Library/LaunchDaemons/ssh.plist\n# 2. 停止sshd服务： sudo launchctl unload -w /System/Library/LaunchDaemons/ssh.plist\n# 3. 查看是否启动： sudo launchctl list | grep ssh\n如果看到下面的输出表示成功启动了：\n1 2 $ sudo launchctl list | grep ssh -\t0\tcom.openssh.sshd 为什么需要开启macOS上的sshd服务呢？是因为在本地部署flink或者其他某些集群服务时，默认是要通过ssh协议发送文件的。对，传输到本机也是用ssh服务，所以如果没有开通服务就无法正确部署。\n","date":"2020-03-29T10:57:10+08:00","permalink":"https://blog.happyhacker.fun/p/macos%E4%B8%8A%E7%AE%A1%E7%90%86ssh%E6%9C%8D%E5%8A%A1/","title":"macOS上管理SSH服务"},{"content":"用了5年macOS也一直没有用过苹果原生的服务管理、AppleSript等等，总感觉不够直观，好在还有Homebrew这个神器，帮我解决了很多问题。\n对于通过homebrew安装的服务，可以通过其提供的brew services或者服务自带的命令进行管理。这里只记录了两种，其他需要查看启动方法的可以通过brew info命令查看。\n# MySQL 1 2 3 4 brew install mysql mysql.server start mysql.server stop # ZooKeeper 1 2 3 brew install zookeeper brew services start zookeeper # 后台启动 zkServer start # 前台启动 ","date":"2020-03-29T10:53:04+08:00","permalink":"https://blog.happyhacker.fun/p/%E4%BD%BF%E7%94%A8homebrew%E7%AE%A1%E7%90%86macos%E4%B8%8A%E7%9A%84%E6%9C%8D%E5%8A%A1/","title":"使用Homebrew管理macOS上的服务"},{"content":"实在是受不了MBP13孱弱的性能和糟糕的发热，终于下定决心组装一台台式机。最终还是真香。\n本文操作过程基于白苹果，Windows环境的小伙伴可参考其他教程\n本次操作的大部分流程都基于opencore-vanilla-desktop-guide，感谢国内外友人对黑苹果作出的卓越贡献。\n用过两个MacBookPro，最大的感觉就是用起来很方便（我是一个软件开发者），但性能实在是太弱鸡了，严重影响了效率。以至于黑苹果安装完成之后我发现原来的过渡动画竟然不见了。。。对，就是这么明显。\n# 准备阶段 为了这个黑苹果，我全新配置了一台台式机，配置也是改了又改，最终性能需求战胜了便携性，选择了一台MATX机箱。\n选择配置考虑的问题如下：\n# 1. CPU 我就是需要一个性能强劲的CPU和足够的内存。本来想着上i7 9700KF，但如果上了这个CPU，就还需要配一块Z390主板，成本太高了就失去了黑苹果的意义了。\n当然AMD YES!!!\n我知道AMD的黑苹果可能遇到更多的问题，本着遇到问题解决问题的态度，还是硬着头皮上了。\n# 2. 主板 主板也是在华硕的Tuf B450 Pro Gaming和微星的迫击炮MAX之间纠结了很久，最终还是随大流选择了迫击炮。这里提个醒，3600原装的CPU散热器和内存插槽会有冲突，如果不让它和内存插槽有冲突，它就会和朝后的IO接口散热片有冲突。我只在A2和B2两个位置插了内存，所以还好。如果你用塔式散热器可能也不存在这个问题，但我不确定会不会出现其他的问题。\n# 3. 显卡 推荐的最多的就是RX 580 2304SP，但现在几乎已经买不到全新的满血版本了，一天在京东看到了盈通的RX 580 游戏高手，惊喜的发现竟然是满血版本，而且还是三风扇设计。由于我对显卡的性能要求不高，所以一般也不会满载，功耗什么的应该也没有太大的区别。虽然1119不是什么好价格（毕竟听说之前有蓝宝石999的RX 590 OC），但对比价格高了300块但性能差不多的RX 5500XT还是更合适一点。\n# 4. 内存 16G应该足够了，尤其是现在内存涨价那么厉害的时候，芝奇Snipper X 3200 8Gx2，迷彩马甲，看起来还可以。\n# 5. 其他 机箱和硬盘就没啥说的了，硬盘只要不选择三星PM981就行了。而且我这次也没有买硬盘。\n# 6. 机箱风扇 本来以为机箱风扇并不是必须用品，而且我也不玩游戏，想必机器的负载不会很高，但实际使用中发现玻璃侧板还是有点温热的。想来也是，因为CPU的风扇把热风吹到玻璃侧板上，GPU的风扇往下吹，不管怎么样都没有让热空气往外走的路径。于是在机箱背部和上部各安装了一把12厘米的风扇，效果立竿见影，玻璃侧板一直冷冰冰了。\n# 配置单 配件 型号 价格 CPU+主板 AMD 3600 + MSI Motar MAX 1919 显卡 盈通RX580游戏高手 1119 内存 芝奇 Snipper X 3200 8GB x 2 598 机箱 先马平头哥M1 159 硬盘 浦科特M6M（2014年的存货）+一块HGST的500G机械盘 0 优盘 闪迪Cruzer Glide 3.0 32GB 29.9 机箱风扇 追风者工包RGB 2把，型号未知 39.8 # 安装过程 # 1. 制作安装盘 格式化优盘\n虽然这个看起来是最简单的，但我偏偏是在这里出了问题。\n注意这两个选项，如果你默认选中的是第一个，则在抹掉优盘的时候就找不到分区表的方式，如下图所示\n如果选择了Show All Devices，就如下图所示\n# 2. 下载必备软件包 # 1. 下载操作系统 从Mac AppStore下载最新的macOS Catalina 10.15.3，然后执行\n1 sudo /Applications/Install\\ macOS\\ Catalina.app/Contents/Resources/createinstallmedia --volume /Volumes/MyVolume # 2. 下载MountEFI 执行./MountEFI.command，会弹出\n正常制作完USB启动盘之后会有一个上图标红的挂载点，选它就行了。然后输入密码，这时在Finder里会看到有一个EFI挂载点，正常是空的。后面配置完OpenCore之后把EFI目录放在里面就行了。\n# 3. 配置config.plist 这是整个过程最复杂的一步了，但只要按照我在文章开头附上的链接一步步操作，肯定不会有问题。不得不说，那个教程真是严谨。甚至还有一个配置检查器，如果有问题可以提前发现，避免直接上机执行反复重启浪费时间。\n不过这里我就不详细描述了，只把最终我配置好的EFI目录放在这里，供大家直接使用。\n# 使用优盘启动 优盘插到台式机的USB上，启动时按F11选择启动项，选择那个很明显是优盘的选项。然后就开始跑代码了，之后就一步一步的进行安装过程。\n进入系统之后先不要做任何配置，因为你可能还没有安装成功。这时候系统还只能从优盘启动，如果拔了优盘，就看不到黑苹果的启动项了。\n# 免优盘启动 其实仔细想下就知道是因为你安装了黑苹果的磁盘没有优盘里面的那个EFI目录（当然不只是目录）。所以问题也就迎刃而解了。\n在新安装的黑苹果执行MountEFI，选择优盘，把优盘里的EFI目录整个复制到桌面上。然后把优盘卸载掉，再挂载你安装了黑苹果的系统盘。这时就看到了系统盘里面的EFI挂载点，然后把刚才放在桌面上的EFI目录放在系统盘的EFI挂载点里。\n这时你的系统就可以不用优盘启动了。\n你可能会问，为什么不能同时挂载两个，直接拖过去呢？放在桌面上这一步看起来很多余。其实上述的文档里专门提到了这一点，因为如果系统同时有两个EFI被挂载了，它可能就懵逼了，为了避免不必要的麻烦，还是事先麻烦一点好。\n不过这时候启动还是会跑代码，我觉得无所谓，就还没有花时间研究。\n# 总结 现在基本上能正常使用了，几个不太重要的问题还没有解决。\niService不能用，因为没有找到合适的Serial Number，也就是序列号。教程中的序列号生成器生成的序列号，去苹果官网上总是验证不通过，验证不通过也不影响使用AppleID，只是无法使用iMessage等我不用的服务，所以无所谓了。 启动时跑代码，这个应该就是一个DebugVerbose模式，我还没有仔细研究，无关紧要了。实际上只要在config.plist中找到boot-args，把后面的值里面的-v删掉就可以了。 我用的是有线网络，所以蓝牙、无线还未配置，也没有计划研究了。 # 补充 后来发现一些比较头疼的问题\n无法睡眠、休眠：点击睡眠后总是黑屏1秒后重新唤醒，不知道是不是什么驱动没有装好的问题，而且我也不知道台式机睡眠会是什么样子。。。 机箱前面板的USB2.0接口无法使用：根本不通电，检查了线缆应该是插错，由于没有Windows系统，也就无法验证。不过好在我也用不到这两个插口，所以无所谓了。 ","date":"2020-03-29T10:49:38+08:00","permalink":"https://blog.happyhacker.fun/p/amd%E9%BB%91%E8%8B%B9%E6%9E%9C%E7%9A%84%E8%A3%85%E6%9C%BA%E7%BB%8F%E9%AA%8C/","title":"AMD黑苹果的装机经验"},{"content":"开发Java应用的过程中通常需要依赖大量的第三方包，而由于众所周知的原因，我们访问这些资源的速度非常慢，感谢阿里云给我们提供了一个选项可以快速访问这些资源。\n下面分别是使用maven和gradle时的配置。\n# maven 将以下内容写入$HOME/.m2/settings.xml中。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 \u0026lt;settings xmlns=\u0026#34;http://maven.apache.org/SETTINGS/1.0.0\u0026#34; xmlns:xsi=\u0026#34;http://www.w3.org/2001/XMLSchema-instance\u0026#34; xsi:schemaLocation=\u0026#34;http://maven.apache.org/SETTINGS/1.0.0 https://maven.apache.org/xsd/settings-1.0.0.xsd\u0026#34;\u0026gt; \u0026lt;!-- \u0026lt;localRepository/\u0026gt; --\u0026gt; \u0026lt;!-- \u0026lt;interactiveMode/\u0026gt; --\u0026gt; \u0026lt;!-- \u0026lt;offline/\u0026gt; --\u0026gt; \u0026lt;!-- \u0026lt;pluginGroups/\u0026gt; --\u0026gt; \u0026lt;!-- \u0026lt;servers/\u0026gt; --\u0026gt; \u0026lt;mirrors\u0026gt; \u0026lt;mirror\u0026gt; \u0026lt;id\u0026gt;aliyunmaven\u0026lt;/id\u0026gt; \u0026lt;mirrorOf\u0026gt;*\u0026lt;/mirrorOf\u0026gt; \u0026lt;name\u0026gt;阿里云公共仓库\u0026lt;/name\u0026gt; \u0026lt;url\u0026gt;https://maven.aliyun.com/repository/public\u0026lt;/url\u0026gt; \u0026lt;/mirror\u0026gt; \u0026lt;/mirrors\u0026gt; \u0026lt;!-- \u0026lt;proxies/\u0026gt; --\u0026gt; \u0026lt;!-- \u0026lt;profiles/\u0026gt; --\u0026gt; \u0026lt;profiles\u0026gt; \u0026lt;profile\u0026gt; \u0026lt;id\u0026gt;jdk-1.8\u0026lt;/id\u0026gt; \u0026lt;activation\u0026gt; \u0026lt;activeByDefault\u0026gt;true\u0026lt;/activeByDefault\u0026gt; \u0026lt;jdk\u0026gt;1.8\u0026lt;/jdk\u0026gt; \u0026lt;/activation\u0026gt; \u0026lt;properties\u0026gt; \u0026lt;maven.compiler.source\u0026gt;1.8\u0026lt;/maven.compiler.source\u0026gt; \u0026lt;maven.compiler.target\u0026gt;1.8\u0026lt;/maven.compiler.target\u0026gt; \u0026lt;maven.compiler.compilerVersion\u0026gt;1.8\u0026lt;/maven.compiler.compilerVersion\u0026gt; \u0026lt;/properties\u0026gt; \u0026lt;/profile\u0026gt; \u0026lt;/profiles\u0026gt; \u0026lt;!-- \u0026lt;activeProfiles/\u0026gt; --\u0026gt; \u0026lt;/settings\u0026gt; # gradle # 单个项目 在buile.gradle中添加以下配置\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 buildscript { repositories { maven { url \u0026#39;https://maven.aliyun.com/repository/google/\u0026#39; } maven { url \u0026#39;https://maven.aliyun.com/repository/jcenter/\u0026#39;} } dependencies { classpath \u0026#39;com.android.tools.build:gradle:2.2.3\u0026#39; // NOTE: Do not place your application dependencies here; they belong // in the individual module build.gradle files } } allprojects { repositories { maven { url \u0026#39;https://maven.aliyun.com/repository/google/\u0026#39; } maven { url \u0026#39;https://maven.aliyun.com/repository/jcenter/\u0026#39;} } } # 全局生效 将以下内容写入$HOME/.gradle/init.gradle中。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 allprojects{ repositories { def ALIYUN_REPOSITORY_URL = \u0026#39;https://maven.aliyun.com/repository/public/\u0026#39; def ALIYUN_JCENTER_URL = \u0026#39;https://maven.aliyun.com/repository/jcenter/\u0026#39; def ALIYUN_GOOGLE_URL = \u0026#39;https://maven.aliyun.com/repository/google/\u0026#39; def ALIYUN_GRADLE_PLUGIN_URL = \u0026#39;https://maven.aliyun.com/repository/gradle-plugin/\u0026#39; all { ArtifactRepository repo -\u0026gt; if(repo instanceof MavenArtifactRepository){ def url = repo.url.toString() if (url.startsWith(\u0026#39;https://repo1.maven.org/maven2/\u0026#39;)) { project.logger.lifecycle \u0026#34;Repository ${repo.url} replaced by $ALIYUN_REPOSITORY_URL.\u0026#34; remove repo } if (url.startsWith(\u0026#39;https://jcenter.bintray.com/\u0026#39;)) { project.logger.lifecycle \u0026#34;Repository ${repo.url} replaced by $ALIYUN_JCENTER_URL.\u0026#34; remove repo } if (url.startsWith(\u0026#39;https://dl.google.com/dl/android/maven2/\u0026#39;)) { project.logger.lifecycle \u0026#34;Repository ${repo.url} replaced by $ALIYUN_GOOGLE_URL.\u0026#34; remove repo } if (url.startsWith(\u0026#39;https://plugins.gradle.org/m2/\u0026#39;)) { project.logger.lifecycle \u0026#34;Repository ${repo.url} replaced by $ALIYUN_GRADLE_PLUGIN_URL.\u0026#34; remove repo } } } maven { url ALIYUN_REPOSITORY_URL } maven { url ALIYUN_JCENTER_URL } maven { url ALIYUN_GOOGLE_URL } maven { url ALIYUN_GRADLE_PLUGIN_URL } } } ","date":"2020-03-29T00:21:43+08:00","permalink":"https://blog.happyhacker.fun/p/%E4%BD%BF%E7%94%A8%E9%98%BF%E9%87%8C%E4%BA%91%E5%8A%A0%E9%80%9F%E4%BE%9D%E8%B5%96%E7%AE%A1%E7%90%86/","title":"使用阿里云加速依赖管理"},{"content":"不得不说Hugo的图片路径支持有些不友好，网上也有很多吐槽。简单说就是即便神级的Markdown编辑器Typora都无法适应Hugo的图片路径。由于Typora需要做日常的工作记录，所以就配置了一下VSCode来支持Hugo。\n# Hugo支持两种放置本地图片的方式 本地图片是相对网络图片而言，如果你有图床也就无所谓是否相对路径了\ncontent目录下\n例如图片content/a.png，在文章content/post/a.md中引用就需要是![](/../a.png) static目录下\n例如图片static/images/a.png，在文章content/post/a.md中引用就需要是![](/images/a.png) 这里还是想吐槽一下，主要是第一种方式，既然在文章中是这样的写法，其实就已经默认是从【当前文章所在目录】向前查找了，那为什么不能放在当前文章目录下？\n# 配置VSCode支持两种方式 我还是比较倾向于内容和图片分离，所以就使用上述的第二种方式，方法确定了其实配置方式差别不大。\n# 依赖工具 VSCode 扩展Paste Image (作者 mushan） # 配置步骤 配置图片文件存放路径 Paste Image: Path中配置 ${projectRoot}/static/images/ 配置粘贴到文章中的文本 Paste Image: Insert Pattern中配置 ${imageSyntaxPrefix}/images/${imageFileName}${imageSyntaxSuffix}\n这一点我没有仔细看文档，花费了一些时间。\n效果图如下\n# 存在的问题 当然这样配置还是解决不了【正常的Markdown】编辑器无法识别图片路径从而导致图片无法渲染的问题。但好在Hugo有一个不错的实时预览功能，弥补了这一点。\n","date":"2020-03-28T23:44:36+08:00","permalink":"https://blog.happyhacker.fun/p/%E9%85%8D%E7%BD%AEhugo%E7%9A%84%E5%9B%BE%E7%89%87%E8%B7%AF%E5%BE%84/","title":"配置Hugo的图片路径"},{"content":"本文探讨了Springboot应用使用jar包和war包的区别，以及使用中的一些思考。我刚开始研究Java，一些想法可能不准确，欢迎提出宝贵意见。\n在开发中我们会使用嵌入式的tomcat容器，但实际项目部署中一般不会这么做，事实证明大部分都是这么用的，独立的tomcat部署已经被淘汰了。下面在macOS环境下操作以下步骤：\n由于实验用的黑苹果不支持docker，以下所有操作需要的应用均使用macOS下的homebrew安装。\n# 创建一个简单的Springboot Web应用 使用Spring Initializr创建一个基础的Springboot应用，只选择Web组件。\n以上就是一个最简单的Springboot应用了。\n# 在嵌入式tomcat容器中运行Web应用 可以看到，这个应用已经可以在嵌入式tomcat容器中运行了。注意，这里访问的路径是http://localhost:8080/v1/hello/world。\n# 打包编写完成的war包 在Idea中执行mvn pacakge，然后在target目录中检查生成的war包。\n# 将war包部署到独立的tomcat服务中 这时候就可以关闭Idea中运行的嵌入式tomcat容器了，因为启动独立tomcat服务时默认端口也是8080，会有冲突导致无法启动。\n可以执行brew services start tomcat来启动web容器。这里为了观察服务的输出，使用前台运行的方式catalina run。\n可以看到tomcat服务已经成功启动，并监听了8080端口。\n# 访问独立tomcat服务中的应用 将前面target目录中的war包部署到tomcat的webapps目录中。\n可以看到，服务启动后，直接将war包复制到tomcat的工作目录中，服务就会检测到新war包的加入，并自动运行相应的服务。\n这时如果我们还像刚才那样访问http://localhost:8080/v1/hello/world会怎样呢？\n可以看到，是不存在这个路径的。\n问题出在哪儿呢？我们看一下webapps目录下都有哪些东西。\n可以看到，我们是把应用部署在了web容器中，但web容器中却是有多个应用的，所以，访问应用时需要带上应用的名字。那名字是什么呢？当然就是spring-in-tomcat-0.0.1-SNAPSHOT，试一下\n果然可以了。\n# 访问应用的不同版本 刚才是应用从不存在到存在，tomcat可以自动检测。我们再测试一下是否可以检测文件的变更。\n这里做了一个微小的变化。\n复制完成之后tomcat马上就检测到了文件的更新。\n可以看到，应用更新也无感的完成了。\n# 是否真的是无感？ 在war包替换期间发生了什么？服务有没有中断呢？再做一个测试\n首先启动30秒的并发请求，然后将重新编辑并打包的war包重新部署，结果发现有大量的非200的返回值。这就证明了并不是“软重启”，而是存在服务中断。那怎么证明不是wrk发起的请求太多，从而导致的服务繁忙呢？在正常情况下再跑一次测试就行了。\n所以，重新部署服务导致服务中断的结论无误。\n从这个结论萌生了另外一个想法，这个访问的路径是带版本号的，这里是0.0.1-SNAPSHOT，那如果我直接加一个0.0.2-SNAPSHOT的版本进来，不就两个都能访问了？然后配合Nginx的反向代理和负载均衡，步进式的切流量，也就同时实现了灰度发布。\n# 在tomcat前部署nginx反向代理 添加一个如图的配置文件，这时就可以通过nginx访问spring的服务了。不出意外的话，改变nginx的配置并重新reload nginx的过程，服务是不会中断的。\n多次实验结果表明，在并发请求期间reload nginx的server配置，对服务可用性的影响非常小。\n关于Nginx的负载均衡相关内容这里不再过多涉及。\n# 总结 tomcat会自动加载新加入的war包 tomcat更新同名的新war包时服务会中断 可以利用tomcat可同时运行多个war包的特性提供不同版本的服务 可以利用Nginx反向代理实现服务不中断 可以利用Nginx的负载均衡实现灰度发布 ","date":"2020-03-28T12:56:22+08:00","permalink":"https://blog.happyhacker.fun/p/springboot%E4%BD%BF%E7%94%A8%E5%86%85%E7%BD%AE%E5%92%8C%E7%8B%AC%E7%AB%8Btomcat%E4%BB%A5%E5%8F%8A%E5%85%B6%E4%BB%96%E6%80%9D%E8%80%83/","title":"Springboot使用内置和独立tomcat以及其他思考"}]